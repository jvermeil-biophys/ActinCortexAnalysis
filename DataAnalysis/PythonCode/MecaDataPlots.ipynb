{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from cycler import cycler\n",
    "from copy import copy\n",
    "import itertools\n",
    "from statannot import add_stat_annotation\n",
    "from datetime import date\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as st\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dateFormatExcel = re.compile('\\d{2}/\\d{2}/\\d{4}')\n",
    "dateFormatOk = re.compile('\\d{2}-\\d{2}-\\d{2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Range1d\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.layouts import gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# %matplotlib inline\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "# colors = prop_cycle.by_key()['color']\n",
    "new_color_cycle = cycler(color=['#ff7f0e', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "plt.rcParams['axes.prop_cycle'] = new_color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mainDir = \"C://Users//JosephVermeil//Desktop//ActinCortexAnalysis\"\n",
    "mainDir = \"C://Users//josep//Desktop//ActinCortexAnalysis\"\n",
    "experimentalDataDir = os.path.join(mainDir, \"ExperimentalData\")\n",
    "dataDir = os.path.join(mainDir, \"DataAnalysis\")\n",
    "figDir = os.path.join(dataDir, \"Figures\")\n",
    "todayFigDir = os.path.join(figDir, \"Historique//\" + str(date.today()))\n",
    "timeSeriesDataDir = os.path.join(dataDir, \"TimeSeriesData\")\n",
    "# os.mkdir(todayFigDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\"))]\n",
    "# allTimeSeriesDataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     26
    ]
   },
   "outputs": [],
   "source": [
    "def get_R2(Y1, Y2):\n",
    "    meanY = np.mean(Y1)\n",
    "    meanYarray = meanY*np.ones(len(Y1))\n",
    "    SST = np.sum((Y1-meanYarray)**2)\n",
    "    SSE = np.sum((Y2-meanYarray)**2)\n",
    "    R2 = SSE/SST\n",
    "    return(R2)\n",
    "\n",
    "def getDictAggMean(df):\n",
    "    dictAggMean = {}\n",
    "    for c in df.columns:\n",
    "#         t = df[c].dtype\n",
    "#         print(c, t)\n",
    "        try :\n",
    "            if np.array_equal(df[c], df[c].astype(bool)):\n",
    "                dictAggMean[c] = 'first'\n",
    "            else:\n",
    "                try:\n",
    "                    np.mean(df[c])\n",
    "                    dictAggMean[c] = 'mean'\n",
    "                except:\n",
    "                    dictAggMean[c] = 'first'\n",
    "        except:\n",
    "                dictAggMean[c] = 'first'\n",
    "    return(dictAggMean)\n",
    "\n",
    "def findFirst(x, A):\n",
    "    idx = (A==x).view(bool).argmax()\n",
    "    return(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16,
     35,
     56
    ]
   },
   "outputs": [],
   "source": [
    "def getCellTimeSeriesData(cellID):\n",
    "    allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\"))]\n",
    "    fileFound = False\n",
    "    nFile = len(allTimeSeriesDataFiles)\n",
    "    iFile = 0\n",
    "    while (not fileFound) and (iFile < nFile):\n",
    "        f = allTimeSeriesDataFiles[iFile]\n",
    "        if f.startswith(cellID):\n",
    "            timeSeriesDataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "            timeSeriesDataFrame = pd.read_csv(timeSeriesDataFilePath, sep=';')\n",
    "            fileFound = True\n",
    "        iFile += 1\n",
    "    if not fileFound:\n",
    "        timeSeriesDataFrame = pd.DataFrame([])\n",
    "    return(timeSeriesDataFrame)\n",
    "\n",
    "def plotCellTimeSeriesData(cellID):\n",
    "    X = 'T'\n",
    "    Y = np.array(['B', 'F', 'dx', 'dy', 'dz', 'D2', 'D3'])\n",
    "    units = np.array([' (mT)', ' (pN)', ' (µm)', ' (µm)', ' (µm)', ' (µm)', ' (µm)'])\n",
    "    timeSeriesDataFrame = getCellTimeSeriesData(cellID)\n",
    "    if not timeSeriesDataFrame.size == 0:\n",
    "#         plt.tight_layout()\n",
    "#         fig.show() # figsize=(20,20)\n",
    "        axes = timeSeriesDataFrame.plot(x=X, y=Y, kind='line', ax=None, subplots=True, sharex=True, sharey=False, layout=None, \\\n",
    "                       figsize=(8,10), use_index=True, title = cellID + '- Time dependant data', grid=None, legend=False, style=None, logx=False, logy=False, \\\n",
    "                       loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, \\\n",
    "                       table=False, yerr=None, xerr=None, secondary_y=False, sort_columns=False)\n",
    "        plt.gcf().tight_layout()\n",
    "        for i in range(len(Y)):\n",
    "            axes[i].set_ylabel(Y[i] + units[i])\n",
    "        \n",
    "    else:\n",
    "        print('cell not found')\n",
    "        \n",
    "def addExcludedCell(cellID, motive):\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    nLines = len(lines)\n",
    "    excludedCellsList = []\n",
    "    for iLine in range(nLines):\n",
    "        line = lines[iLine]\n",
    "        splitLine = line[:-1].split(',')\n",
    "        excludedCellsList.append(splitLine[0])\n",
    "    if cellID in excludedCellsList:\n",
    "        newlines = copy(lines)\n",
    "        iLineOfInterest = excludedCellsList.index(cellID)\n",
    "        if motive not in newlines[iLineOfInterest][:-1].split(','):\n",
    "            newlines[iLineOfInterest] = newlines[iLineOfInterest][:-1] + ',' + motive + '\\n'            \n",
    "    else:\n",
    "        newlines = copy(lines)\n",
    "        newlines.append('' + cellID + ',' + motive + '\\n')\n",
    "    f.close()\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'w')\n",
    "    f.writelines(newlines)\n",
    "    \n",
    "def getExcludedCells():\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    nLines = len(lines)\n",
    "    excludedCellsDict = {}\n",
    "    for iLine in range(nLines):\n",
    "        line = lines[iLine]\n",
    "        splitLine = line[:-1].split(',')\n",
    "        excludedCellsDict[splitLine[0]] = splitLine[1:]\n",
    "    return(excludedCellsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = getCellTimeSeriesData('21-02-10_M1_P1_C1')\n",
    "plotCellTimeSeriesData('21-01-21_M1_P1_C11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCellTimeSeriesData('21-02-10_M1_P1_C2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of the get R2 function.\n",
    "# T = df['T']\n",
    "# Y = df['D3']\n",
    "# plt.plot(T,Y)\n",
    "# p, residuals, rank, singular_values, rcond = np.polyfit(T, Y, deg=5, full=True)\n",
    "# plt.plot(T, Y)\n",
    "# Y2 = np.zeros(len(T))\n",
    "# for i in range(len(T)):\n",
    "#     deg = len(p)-1\n",
    "#     for k in range(deg+1):\n",
    "#         Y2[i] += p[k]*(T[i]**(deg-k))\n",
    "# plt.plot(T,Y2)\n",
    "# get_R2(Y, Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GlobalTables functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getExperimentalConditions(save = False):\n",
    "    # Getting the table\n",
    "    experimentalDataFile = 'ExperimentalConditions.csv'\n",
    "    experimentalDataFilePath = os.path.join(experimentalDataDir, experimentalDataFile)\n",
    "    expConditionsDF = pd.read_csv(experimentalDataFilePath, sep=';',header=0)\n",
    "    print('Extracted a table with ' + str(expConditionsDF.shape[0]) + ' lines and ' + str(expConditionsDF.shape[1]) + ' columns.')\n",
    "    \n",
    "    # Cleaning the table\n",
    "    try:\n",
    "        for c in expConditionsDF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                expConditionsDF = expConditionsDF.drop([c], axis=1)\n",
    "        expConditionsDF = expConditionsDF.convert_dtypes()\n",
    "\n",
    "        listTextColumns = []\n",
    "        for col in expConditionsDF.columns:\n",
    "            try:\n",
    "                if expConditionsDF[col].dtype == 'string':\n",
    "                    listTextColumns.append(col)\n",
    "            except:\n",
    "                aaaa=0\n",
    "                #Ok\n",
    "\n",
    "        expConditionsDF[listTextColumns] = expConditionsDF[listTextColumns].apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "        expConditionsDF['scale pixel per um'] = expConditionsDF['scale pixel per um'].astype(float)\n",
    "        try:\n",
    "            expConditionsDF['optical index correction'] = \\\n",
    "                      expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[0]).astype(float) \\\n",
    "                    / expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[1]).astype(float)\n",
    "        except:\n",
    "            print('optical index correction already in ' + str(expConditionsDF['optical index correction'].dtype) + ' type.')\n",
    "\n",
    "        expConditionsDF['magnetic field correction'] = expConditionsDF['magnetic field correction'].astype(float)\n",
    "        expConditionsDF['with fluo images'] = expConditionsDF['with fluo images'].astype(bool)\n",
    "\n",
    "        try:\n",
    "            expConditionsDF['ramp field'] = \\\n",
    "            expConditionsDF['ramp field'].apply(lambda x: [x.split(';')[0], x.split(';')[1]] if not pd.isnull(x) else [])\n",
    "        except:\n",
    "            aaaa=0\n",
    "            #Ok\n",
    "\n",
    "        dateExemple = expConditionsDF.loc[expConditionsDF.index[1],'date']\n",
    "\n",
    "        if re.match(dateFormatExcel, dateExemple):\n",
    "            print('dates corrected')\n",
    "            expConditionsDF.loc[1:,'date'] = expConditionsDF.loc[1:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])        \n",
    "        \n",
    "    except:\n",
    "        print('Unexpected bug with the cleaning step')\n",
    "\n",
    "    if save:\n",
    "        saveName = 'ExperimentalConditions.csv'\n",
    "        savePath = os.path.join(experimentalDataDir, saveName)\n",
    "        expConditionsDF.to_csv(savePath, sep=';')\n",
    "\n",
    "    expConditionsDF['manipID'] = expConditionsDF['date'] + '_' + expConditionsDF['manip']\n",
    "    \n",
    "    return(expConditionsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getExperimentalConditions(save=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     85
    ]
   },
   "outputs": [],
   "source": [
    "listColumnsCtField = ['date','cellName','cellID','manipID',\\\n",
    "                      'duration','medianRawB','medianThickness',\\\n",
    "                      '1stDThickness','9thDThickness','fluctuAmpli',\\\n",
    "                      'R2_polyFit','validated']\n",
    "\n",
    "def analyseTimeSeries_ctField(tsDf):\n",
    "    results = {}\n",
    "    results['duration'] = np.max(tsDf['T'])\n",
    "    results['medianRawB'] = np.median(tsDf.B)\n",
    "    results['medianThickness'] = np.median(tsDf.D3)\n",
    "    results['1stDThickness'] = np.percentile(tsDf.D3, 10)\n",
    "    results['9thDThickness'] = np.percentile(tsDf.D3, 90)\n",
    "    results['fluctuAmpli'] = results['9thDThickness'] - results['1stDThickness']\n",
    "    results['validated'] = (results['1stDThickness'] > 0)\n",
    "    X, Y = tsDf['T'], tsDf['D3']\n",
    "    p, residuals, rank, singular_values, rcond = np.polyfit(X, Y, deg=5, full=True)\n",
    "    Y2 = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        deg = len(p)-1\n",
    "        for k in range(deg+1):\n",
    "            Y2[i] += p[k]*(X[i]**(deg-k))\n",
    "    results['R2_polyFit'] = get_R2(Y, Y2)\n",
    "    return(results)\n",
    "\n",
    "def createCtFieldDataDict(list_ctFieldFiles):\n",
    "    tableDict = {}\n",
    "    tableDict['date'], tableDict['cellName'], tableDict['cellID'], tableDict['manipID'] = [], [], [], []\n",
    "    tableDict['duration'], tableDict['medianRawB'], tableDict['medianThickness'] = [], [], []\n",
    "    tableDict['1stDThickness'], tableDict['9thDThickness'], tableDict['fluctuAmpli'] = [], [], []\n",
    "    tableDict['R2_polyFit'], tableDict['validated'] = [], []\n",
    "    for f in list_ctFieldFiles:\n",
    "        split_f = f.split('_')\n",
    "        tableDict['date'].append(split_f[0])\n",
    "        tableDict['cellName'].append(split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['cellID'].append(split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['manipID'].append(split_f[0] + '_' + split_f[1])\n",
    "        tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "        current_tsDf = pd.read_csv(tS_DataFilePath, ';')\n",
    "        current_resultDict = analyseTimeSeries_ctField(current_tsDf)\n",
    "        for k in current_resultDict.keys():\n",
    "            tableDict[k].append(current_resultDict[k])\n",
    "    return(tableDict)\n",
    "\n",
    "def computeGlobalTable_ctField(task = 'fromScratch', fileName = 'Global_CtFieldData', save = False):\n",
    "    ctFieldTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                                  if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                                      and ('thickness' in f))]\n",
    "#     print(ctFieldTimeSeriesDataFiles)\n",
    "    if task == 'fromScratch':\n",
    "        # create a dict containing the data\n",
    "        tableDict = createCtFieldDataDict(ctFieldTimeSeriesDataFiles)\n",
    "        # create the table\n",
    "        CtField_DF = pd.DataFrame(tableDict)\n",
    "        \n",
    "    elif task == 'updateExisting':\n",
    "        # get existing table\n",
    "        try:\n",
    "            savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "            existing_CtField_DF = pd.read_csv(savePath, sep=';')\n",
    "            for c in existing_CtField_DF.columns:\n",
    "                if 'Unnamed' in c:\n",
    "                    existing_CtField_DF = existing_CtField_DF.drop([c], axis=1)\n",
    "        except:\n",
    "            print('No existing table found')\n",
    "        # find which of the time series files are new\n",
    "        new_ctFieldTimeSeriesDataFiles = []\n",
    "        for f in ctFieldTimeSeriesDataFiles:\n",
    "            split_f = f.split('_')\n",
    "            currentCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "            if currentCellID not in existing_CtField_DF.cellID.values:\n",
    "                new_ctFieldTimeSeriesDataFiles.append(f)\n",
    "        new_tableDict = createCtFieldDataDict(new_ctFieldTimeSeriesDataFiles)\n",
    "        # create the table with new data\n",
    "        new_CtField_DF = pd.DataFrame(new_tableDict)\n",
    "        # fuse the two\n",
    "        new_CtField_DF.index += existing_CtField_DF.shape[0]\n",
    "        CtField_DF = pd.concat([existing_CtField_DF, new_CtField_DF])\n",
    "    \n",
    "    dateExemple = CtField_DF.loc[CtField_DF.index[0],'date']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        CtField_DF.loc[:,'date'] = CtField_DF.loc[:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])\n",
    "    \n",
    "    if save:\n",
    "        saveName = fileName + '.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        CtField_DF.to_csv(savePath, sep=';')\n",
    "        \n",
    "    return(CtField_DF)\n",
    "\n",
    "def getGlobalTable_ctField(fileName = 'Global_CtFieldData'):\n",
    "    try:\n",
    "        savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "        CtField_DF = pd.read_csv(savePath, sep=';')\n",
    "        for c in CtField_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                CtField_DF = CtField_DF.drop([c], axis=1)\n",
    "        print('Extracted a table with ' + str(CtField_DF.shape[0]) + ' lines and ' + str(CtField_DF.shape[1]) + ' columns.')\n",
    "        \n",
    "    except:\n",
    "        print('No existing table found')\n",
    "        \n",
    "    dateExemple = CtField_DF.loc[CtField_DF.index[0],'date']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        print('dates corrected')\n",
    "        CtField_DF.loc[:,'date'] = CtField_DF.loc[:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])\n",
    "#         mecaDF['ManipID'] = mecaDF['ExpDay'] + '_' + mecaDF['CellName'].apply(lambda x: x.split('_')[0])\n",
    "    return(CtField_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "computeGlobalTable_ctField(task='updateExisting',save=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_ctField().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meca data\n",
    "\n",
    "Workflow\n",
    "* analyseTimeSeries_meca() analyse 1 file and return the dict (with the results of the analysis)\n",
    "* createMecaDataDict() call the previous function on the given list of files and concatenate the results\n",
    "* computeGlobalTable_meca() call the previous function and convert the dict to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sample data\n",
    "# data = [12, 12, 13, 13, 15, 16, 17, 22, 23, 25, 26, 27, 28, 28, 29]\n",
    "# create 95% confidence interval for population mean weight\n",
    "# st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) \n",
    "st.t.interval(alpha=0.95, df=1000, loc=0, scale=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     83
    ]
   },
   "outputs": [],
   "source": [
    "listColumnsMeca = ['date','cellName','cellID','manipID',\\\n",
    "                   'compNum','compDuration','compStartTime','maxIndent',\\\n",
    "#                    'minThickness','initialThickness','surroundingThickness'] #,\\\n",
    "                   'H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth',\\\n",
    "                   'hysteresis',\\\n",
    "                   'validated','comments'] # 'fitParams',\n",
    "\n",
    "def compressionFitChadwick(hCompr, fCompr, DIAMETER):\n",
    "    \n",
    "    error = False\n",
    "    \n",
    "    def chadwickModel(h, E, H0):\n",
    "        R = DIAMETER/2\n",
    "        f = (np.pi*E*(R/2)*((H0-h)**2))/(3*H0)\n",
    "        return(f)\n",
    "\n",
    "    def inversedChadwickModel(f, E, H0):\n",
    "        R = DIAMETER/2\n",
    "        h = H0 - ((3*H0*f)/(np.pi*E*R))**0.5\n",
    "        return(h)\n",
    "\n",
    "    # some initial parameter values - must be within bounds\n",
    "    initH0 = max(hCompr) # H0 ~ h_max\n",
    "    initE = (3*max(hCompr)*max(fCompr))/(np.pi*(DIAMETER/2)*(max(hCompr)-min(hCompr))**2) # E ~ 3*H0*F_max / pi*R*(H0-h_min)²\n",
    "    initH0, initE = initH0*(initH0>0), initE*(initE>0)\n",
    "    \n",
    "    initialParameters = [initE, initH0]\n",
    "#     print(initialParameters)\n",
    "\n",
    "    # bounds on parameters - initial parameters must be within these\n",
    "    lowerBounds = (0, 0)\n",
    "    upperBounds = (np.Inf, np.Inf)\n",
    "    parameterBounds = [lowerBounds, upperBounds]\n",
    "    \n",
    "    try:\n",
    "        # fittedParameters, pcov = curve_fit(chadwickModel, hCompr, fCompr, initialParameters, bounds = parameterBounds)\n",
    "        fittedParameters, pcov = curve_fit(inversedChadwickModel, fCompr, hCompr, initialParameters, bounds = parameterBounds)\n",
    "\n",
    "        E, H0 = fittedParameters\n",
    "        hPredict = inversedChadwickModel(fCompr, E, H0)\n",
    "\n",
    "        varE = pcov[0,0]\n",
    "        SSR = np.sum((hCompr-hPredict)**2)\n",
    "        seE = ((SSR/len(fCompr))*varE)**0.5\n",
    "        E, seE = E*1e6, seE*1e6\n",
    "        confIntE = st.t.interval(alpha=0.95, df=len(fCompr), loc=E, scale=seE)\n",
    "        confIntEWidth = confIntE[1] - confIntE[0]\n",
    "\n",
    "        varH0 = pcov[1,1]\n",
    "        seH0 = ((SSR/len(fCompr))*varH0)**0.5\n",
    "        confIntH0 = st.t.interval(alpha=0.95, df=len(fCompr), loc=H0, scale=seH0)\n",
    "        confIntH0Width = confIntH0[1] - confIntH0[0]\n",
    "        R2 = get_R2(hCompr,hPredict)\n",
    "        \n",
    "    except:\n",
    "        error = True\n",
    "        E, H0, hPredict, R2, confIntE, confIntH0 = 'NaN', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN'\n",
    "    \n",
    "    return(E, H0, hPredict, R2, confIntE, confIntH0, error)\n",
    "\n",
    "\n",
    "\n",
    "def analyseTimeSeries_meca(f, tsDF, expDf, listColumnsMeca, PLOT):\n",
    "    \n",
    "    split_f = f.split('_')\n",
    "    tsDF.dx, tsDF.dy, tsDF.dz, tsDF.D2, tsDF.D3 = tsDF.dx*1000, tsDF.dy*1000, tsDF.dz*1000, tsDF.D2*1000, tsDF.D3*1000\n",
    "    thisManipID = split_f[0] + '_' + split_f[1]\n",
    "    expDf['manipID'] = expDf['date'] + '_' + expDf['manip']\n",
    "    thisExpDf = expDf.loc[expDf['manipID'] == thisManipID]\n",
    "    DIAMETER = thisExpDf.at[thisExpDf.index.values[0], 'bead diameter']\n",
    "    \n",
    "    results = {}\n",
    "    for c in listColumnsMeca:\n",
    "        results[c] = []\n",
    "        \n",
    "    Ncomp = max(tsDF['idxCompression'])\n",
    "    NimgComp = np.sum((tsDF['idxCompression'] != 0))/Ncomp\n",
    "    NimgCompTh = round(0.49 + np.sum((tsDF['idxCompression'] != 0))/Ncomp)\n",
    "    NimgBtwComp = np.sum((tsDF['idxCompression'] == 0))/Ncomp\n",
    "    NimgBtwCompTh = round(0.49 + np.sum((tsDF['idxCompression'] == 0))/Ncomp)\n",
    "#     print('Ncomp : ' + str(Ncomp) + ' ; ' + 'NimgComp : ' + str(NimgComp) + '/' + str(NimgCompTh) + ' ; ' + 'NimgBtwComp : ' + str(NimgBtwComp) + '/' + str(NimgBtwCompTh))\n",
    "#     if not NimgBtwComp%2 == 0:\n",
    "#         print('Bug with the compressions sequence delimitation')\n",
    "    \n",
    "    for i in range(1, Ncomp+1):#Ncomp+1):\n",
    "        results['date'].append(split_f[0])\n",
    "        results['cellName'].append(split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        results['cellID'].append(split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        results['manipID'].append(split_f[0] + '_' + split_f[1])\n",
    "        \n",
    "        thisCompDf = tsDF.loc[tsDF['idxCompression'] == i,:]\n",
    "        iStart = (findFirst(tsDF['idxCompression'], i))\n",
    "        iStop = iStart+thisCompDf.shape[0]\n",
    "        \n",
    "        # First, easy-to-get parameters\n",
    "        results['compNum'].append(i)\n",
    "        results['compDuration'].append(thisExpDf.at[thisExpDf.index.values[0], 'compression duration'])\n",
    "        results['compStartTime'].append(thisCompDf['T'].values[0])\n",
    "        results['maxIndent'].append(max(thisCompDf.D3.values)-min(thisCompDf.D3.values))\n",
    "        \n",
    "        # Delimit the start of the increase of B (typically the moment when the field decrease from 5 to 3)\n",
    "        # and the end of its decrease (typically when it goes back from 3 to 5)\n",
    "        \n",
    "        listB = thisCompDf.B.values\n",
    "        offsetStart, offsetStop = 0, 0\n",
    "        minB, maxB = min(listB), max(listB)\n",
    "        thresholdB = (maxB-minB)/50\n",
    "        k = 0\n",
    "        while (listB[k] > minB+thresholdB) or (listB[-1-k] > minB+thresholdB):\n",
    "            offsetStart += int(listB[k] > minB+thresholdB)\n",
    "            offsetStop += int(listB[-1-k] > minB+thresholdB)\n",
    "            k += 1\n",
    "        jStart = offsetStart\n",
    "        jMax = np.argmax(thisCompDf.B)\n",
    "        jStop = thisCompDf.shape[0] - offsetStop\n",
    "        \n",
    "        hCompr = (thisCompDf.D3.values[jStart:jMax+1] - DIAMETER)\n",
    "        hRelax = (thisCompDf.D3.values[jMax+1:jStop] - DIAMETER)\n",
    "        fCompr = (thisCompDf.F.values[jStart:jMax+1])\n",
    "        fRelax = (thisCompDf.F.values[jMax+1:jStop])\n",
    "        \n",
    "        # Remove the 1-2 points at the begining where there is just the viscous relaxation of the cortex\n",
    "        # Because of the initial decrease of B and the cortex thickness increases.\n",
    "        offsetStart2 = 0\n",
    "        k = 0\n",
    "        while (hCompr[k] < hCompr[k+1]) and k<len(hCompr):\n",
    "            offsetStart2 += 1\n",
    "            k += 1\n",
    "        hCompr, fCompr = hCompr[offsetStart2:], fCompr[offsetStart2:]\n",
    "        \n",
    "        E, H0, hPredict, R2, confIntE, confIntH0, fitError = compressionFitChadwick(hCompr, fCompr, DIAMETER)\n",
    "        \n",
    "        if not fitError:\n",
    "            confIntEWidth = abs(confIntE[0] - confIntE[1])\n",
    "\n",
    "            results['H0Chadwick'].append(H0)\n",
    "            results['EChadwick'].append(E)\n",
    "            results['R2Chadwick'].append(R2)\n",
    "            results['EChadwick_CIWidth'].append(confIntEWidth)\n",
    "\n",
    "            if PLOT:\n",
    "                fig, ax = plt.subplots(1,1)\n",
    "                ax.plot(hCompr,fCompr,'b-')\n",
    "                ax.plot(hPredict,fCompr,'k--')\n",
    "                ax.plot(hRelax,fRelax,'r-')\n",
    "                fig.suptitle(results['cellID'][-1] + ' _ ' + str(i))\n",
    "                fig.show()\n",
    "                \n",
    "            R2CRITERION = 0.8\n",
    "            validated = (R2 > R2CRITERION)\n",
    "\n",
    "            results['validated'].append(validated)\n",
    "            if validated:\n",
    "                results['comments'].append('ok')\n",
    "            else:\n",
    "                results['comments'].append('not ok')\n",
    "                \n",
    "        if fitError:\n",
    "            results['H0Chadwick'].append('NaN')\n",
    "            results['EChadwick'].append('NaN')\n",
    "            results['R2Chadwick'].append('NaN')\n",
    "            results['EChadwick_CIWidth'].append('NaN')\n",
    "            validated = False\n",
    "            results['validated'].append(validated)\n",
    "            results['comments'].append('fitFailure')\n",
    "        \n",
    "        results['hysteresis'].append(hCompr[0] - hRelax[-1])\n",
    "        \n",
    "        \n",
    "    \n",
    "    return(results)\n",
    "\n",
    "\n",
    "\n",
    "def createDataDict_meca(list_mecaFiles, listColumnsMeca, PLOT):\n",
    "    expDf = getExperimentalConditions()\n",
    "    tableDict = {}\n",
    "    for c in listColumnsMeca:\n",
    "        tableDict[c] = []\n",
    "    for f in list_mecaFiles: #[41:45]:\n",
    "#         print(f)\n",
    "        tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "        current_tsDF = pd.read_csv(tS_DataFilePath, ';')\n",
    "        current_resultDict = analyseTimeSeries_meca(f, current_tsDF, expDf, listColumnsMeca, PLOT)\n",
    "        for k in current_resultDict.keys():\n",
    "            tableDict[k] += current_resultDict[k]\n",
    "    return(tableDict)\n",
    "\n",
    "\n",
    "\n",
    "def computeGlobalTable_meca(task = 'fromScratch', fileName = 'Global_MecaData', save = False, \\\n",
    "                            listColumnsMeca=listColumnsMeca, PLOT = True):\n",
    "    \n",
    "    list_mecaFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                      if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                      and ('R40' in f))] # Change to allow different formats in the future\n",
    "    \n",
    "    # print(mecaTimeSeriesDataFiles)\n",
    "    if task == 'fromScratch':\n",
    "        # create a dict containing the data\n",
    "        tableDict = createDataDict_meca(list_mecaFiles, listColumnsMeca, PLOT)\n",
    "        # create the table\n",
    "        meca_DF = pd.DataFrame(tableDict)\n",
    "        \n",
    "    elif task == 'updateExisting':\n",
    "        # get existing table\n",
    "        try:\n",
    "            savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "            existing_meca_DF = pd.read_csv(savePath, sep=';')\n",
    "        except:\n",
    "            print('No existing table found')\n",
    "        # find which of the time series files are new\n",
    "        new_mecaTimeSeriesDataFiles = []\n",
    "        for f in mecaTimeSeriesDataFiles:\n",
    "            split_f = f.split('_')\n",
    "            currentCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "            if currentCellID not in existing_meca_DF.cellID:\n",
    "                new_mecaTimeSeriesDataFiles.append(f)\n",
    "        new_tableDict = createMecaDataDict(new_mecaTimeSeriesDataFiles)\n",
    "        # create the table with new data\n",
    "        new_meca_DF = pd.dataframe(tableDict)\n",
    "        # fuse the two\n",
    "        meca_DF = pd.concat([existing_meca_DF, new_meca_DF])\n",
    "        \n",
    "    if save:\n",
    "        saveName = fileName + '.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        meca_DF.to_csv(savePath, sep=';')\n",
    "    \n",
    "    return(meca_DF)\n",
    "            \n",
    "\n",
    "    \n",
    "def getGlobalTable_meca(fileName = 'Global_mecaData'):\n",
    "    try:\n",
    "        savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "        meca_DF = pd.read_csv(savePath, sep=';')\n",
    "        print('Extracted a table with ' + str(meca_DF.shape[0]) + ' lines and ' + str(meca_DF.shape[1]) + ' columns.')\n",
    "    except:\n",
    "        print('No existing table found')\n",
    "    for c in meca_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                meca_DF = meca_DF.drop([c], axis=1)\n",
    "    if not ('manipID' in meca_DF.columns):\n",
    "        meca_DF['manipID'] = meca_DF['ExpDay'] + '_' + meca_DF['CellID'].apply(lambda x: x.split('_')[0])\n",
    "    dateExemple = meca_DF.loc[meca_DF.index[0],'ExpDay']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        print('bad date')\n",
    "    return(meca_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A = computeGlobalTable_meca(PLOT = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_meca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getFluoData(save = False):\n",
    "    # Getting the table\n",
    "    fluoDataFile = 'FluoQuantification.csv'\n",
    "    fluoDataFilePath = os.path.join(dataDir, fluoDataFile)\n",
    "    fluoDF = pd.read_csv(fluoDataFilePath, sep=';',header=0)\n",
    "    print('Extracted a table with ' + str(fluoDF.shape[0]) + ' lines and ' + str(fluoDF.shape[1]) + ' columns.')\n",
    "    # Cleaning the table\n",
    "    try:\n",
    "        for c in fluoDF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                fluoDF = fluoDF.drop([c], axis=1)\n",
    "        \n",
    "    except:\n",
    "        print('Unexpected bug with the cleaning step')\n",
    "\n",
    "    if save:\n",
    "        saveName = 'FluoQuantification.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        fluoDF.to_csv(savePath, sep=';')\n",
    "\n",
    "    \n",
    "    return(fluoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFluoData().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting and filtering\n",
    "\n",
    "TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecaDataFile = 'Global_MecaData.csv'\n",
    "mecaDataFilePath = os.path.join(dataDir, mecaDataFile)\n",
    "mecaDF = pd.read_csv(mecaDataFilePath, sep=';')\n",
    "print('Extracted a table with ' + str(mecaDF.shape[0]) + ' lines and ' + str(mecaDF.shape[1]) + ' columns.')\n",
    "\n",
    "mecaDF = mecaDF.rename(columns={\"CellID\": \"CellName\", \"CellName\": \"CellID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentalDataFile = 'ExperimentalConditions.csv'\n",
    "experimentalDataFilePath = os.path.join(experimentalDataDir, experimentalDataFile)\n",
    "expConditionsDF = pd.read_csv(experimentalDataFilePath, sep=';',header=0)\n",
    "print('Extracted a table with ' + str(expConditionsDF.shape[0]) + ' lines and ' + str(expConditionsDF.shape[1]) + ' columns.')\n",
    "\n",
    "# Cleaning the table\n",
    "try:\n",
    "    expConditionsDF = expConditionsDF.convert_dtypes()\n",
    "\n",
    "    listTextColumns = []\n",
    "    for col in expConditionsDF.columns:\n",
    "        if expConditionsDF[col].dtype == 'string':\n",
    "            listTextColumns.append(col)\n",
    "\n",
    "    expConditionsDF[listTextColumns] = expConditionsDF[listTextColumns].apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "    expConditionsDF['scale pixel per um'] = expConditionsDF['scale pixel per um'].astype(float)\n",
    "    expConditionsDF['optical index correction'] = \\\n",
    "              expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[0]).astype(float) \\\n",
    "            / expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[1]).astype(float)\n",
    "    expConditionsDF['magnetic field correction'] = expConditionsDF['magnetic field correction'].astype(float)\n",
    "    expConditionsDF['with fluo images'] = expConditionsDF['with fluo images'].astype(bool)\n",
    "\n",
    "    expConditionsDF['ramp field'] = \\\n",
    "    expConditionsDF['ramp field'].apply(lambda x: [x.split(';')[0], x.split(';')[1]] if not pd.isnull(x) else [])\n",
    "\n",
    "except:\n",
    "    print('Unexpected bug with the cleaning step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expConditionsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused for now\n",
    "cellDescriptionDataFile = 'CellDescription.csv'\n",
    "cellDescriptionDataFilePath = os.path.join(experimentalDataDir, cellDescriptionDataFile)\n",
    "cellDescriptionDF = pd.read_csv(cellDescriptionDataFilePath, ',')\n",
    "print('Extracted a table with ' + str(cellDescriptionDF.shape[0]) + ' lines and ' + str(cellDescriptionDF.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecaDF['ManipID'] = mecaDF['ExpDay'] + '_' + mecaDF['CellName'].apply(lambda x: x.split('_')[0])\n",
    "expConditionsDF['ManipID'] = expConditionsDF['date'] + '_' + expConditionsDF['manip']\n",
    "\n",
    "mainMecaDF = pd.merge(\n",
    "    expConditionsDF,\n",
    "    mecaDF,\n",
    "    how=\"inner\",\n",
    "    on='ManipID',\n",
    "#     left_on=None,\n",
    "#     right_on=None,\n",
    "#     left_index=False,\n",
    "#     right_index=False,\n",
    "#     sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),\n",
    "#     copy=True,\n",
    "#     indicator=False,\n",
    "#     validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mainMecaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mainMecaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.reset_option('max_columns')\n",
    "# mainMecaDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainMecaDF_f = mainMecaDF.loc[(mainMecaDF[\"Validated\"] == 1)]\n",
    "# mainMecaDF_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "listCells = mainMecaDF_f['CellID'].drop_duplicates().astype('string').values\n",
    "timeSeriesDict = {}\n",
    "for cell in listCells:\n",
    "    currentCell_TimeSeriesData = getCellTimeSeriesData(cell)\n",
    "    timeSeriesDict[cell] = currentCell_TimeSeriesData\n",
    "start, stop = 80, 100\n",
    "fig, axes = plt.subplots((stop-start),1, figsize = (7,4*(stop-start)))\n",
    "fig.tight_layout()\n",
    "for k in range(start, stop):\n",
    "    if k < len(listCells):\n",
    "        currentCell_TimeSeriesData = timeSeriesDict[listCells[k]]\n",
    "        T = currentCell_TimeSeriesData['T'].values\n",
    "        idxCompression = currentCell_TimeSeriesData['idxCompression'].values\n",
    "        D3 = currentCell_TimeSeriesData['D3'].values\n",
    "        maskConstant = (idxCompression == 0)\n",
    "        maskCompression = (idxCompression > 0)\n",
    "        axes[k - start].plot(T, D3*1000-4503, 'k-', linewidth = 0.5)\n",
    "        axes[k - start].plot(T[maskCompression], D3[maskCompression]*1000-4503, 'ro', markersize=2)\n",
    "        axes[k - start].plot(T[maskConstant], D3[maskConstant]*1000-4503, 'co', markersize=2)\n",
    "        axes[k - start].set_title(listCells[k])\n",
    "        axes[k - start].set_xlabel('T (s)')\n",
    "        axes[k - start].set_ylabel('D3 (µm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addExcludedCell('21-01-18_M1_P1_C2', 'passive')\n",
    "addExcludedCell('21-01-18_M1_P1_C3', 'passive')\n",
    "addExcludedCell('21-01-18_M1_P1_C4', 'passive')\n",
    "addExcludedCell('21-01-21_M3_P1_C4', 'passive')\n",
    "addExcludedCell('21-01-21_M3_P1_C5', 'passive')\n",
    "addExcludedCell('20-08-07_M1_P1_C6', 'too thick')\n",
    "addExcludedCell('20-08-07_M1_P1_C62', 'too thick')\n",
    "\n",
    "excludedCellsDict = getExcludedCells()\n",
    "# # excludedMask = (mainMecaDF_f[\"CellID\"].values not in excludedCellsDict.keys())\n",
    "# # mainMecaDF_f = mainMecaDF_f.loc[(mainMecaDF_f[\"CellID\"].values not in excludedCellsDict.keys())]\n",
    "# for i in range(len(excludedCellsDict)):\n",
    "#     print('a')\n",
    "# mainMecaDF_f[\"CellID\"].drop_duplicates().astype('string').values\n",
    "excludedCellsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currentCell_TimeSeriesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainMecaDF_GroupedPerCell = mainMecaDF_f.groupby('CellID')\n",
    "mainMecaDF_DataPerCell = mainMecaDF_GroupedPerCell.agg({\"EChadwick\": np.median, \"SurroundingThickness\": np.median, \"H0Chadwick\" : np.median})\n",
    "# mainMecaDF_GroupedPerCell.agg({\"EChadwick\": np.median, \"D\": lambda x: np.std(x, ddof=1)})\n",
    "cols = ['date', 'manip', 'experimentType', 'drug', 'substrate',\n",
    "       'objective magnification', 'scale pixel per um', 'objective immersion',\n",
    "       'optical index correction', 'magnetic field correction', 'cell type',\n",
    "       'cell subtype', 'bead type', 'bead diameter', 'normal field',\n",
    "       'ramp field', 'compression duration', 'with fluo images', 'comments',\n",
    "       'ManipID', 'ExpType', 'CellName', 'CellID']\n",
    "mainMecaDF_DataPerCell.dropna(inplace = True)\n",
    "mainMecaDF_DataPerCell = pd.merge(mainMecaDF_DataPerCell,\n",
    "                                  mainMecaDF_f[cols].drop_duplicates(subset=['CellID']),\n",
    "                                  how=\"inner\",\n",
    "                                  on='CellID',\n",
    "                                  #     left_on='CellID',\n",
    "                                  #     right_on='CellID',\n",
    "                                  #     left_index=False,\n",
    "                                  #     right_index=False,\n",
    "                                  #     sort=True,\n",
    "                                  #     suffixes=(\"_x\", \"_y\"),\n",
    "                                  #     copy=True,\n",
    "                                  #     indicator=False,\n",
    "                                  #     validate=None,\n",
    "                                  )\n",
    "# mainMecaDF_DataPerCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainMecaDF_DataPerCell_Count = mainMecaDF_DataPerCell.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "mainMecaDF_DataPerCell_Count.loc[:, ['CellID']].rename(columns={'CellID' : 'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Styles = {''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def D1Plot(data, CondCol=[],Parameters=[],Filters=[],Boxplot=True,AvgPerCell=False,cellID='cellID', co_order=[],\\\n",
    "           stats=True,statMethod='Mann-Whitney',statVerbose = 0,box_pairs=[],figSizeFactor = 1,markerSizeFactor=1):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "    NCond = len(CondCol)    \n",
    "    if NCond == 1:\n",
    "        CondCol = CondCol[0]\n",
    "    elif NCond > 1:\n",
    "        newColName = ''\n",
    "        for i in range(NCond):\n",
    "            newColName += CondCol[i]\n",
    "            newColName += ' & '\n",
    "        newColName = newColName[:-3]\n",
    "        data_filtered[newColName] = ''\n",
    "        for i in range(NCond):\n",
    "            data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "            data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "        CondCol = newColName\n",
    "    \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean)\n",
    "        \n",
    "    data_filtered.sort_values(CondCol, axis=0, ascending=True, inplace=True)\n",
    "    \n",
    "    NPlots = len(Parameters)\n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    if len(co_order) == 0:\n",
    "        co_order = Conditions\n",
    "        \n",
    "    fig, ax = plt.subplots(1, NPlots, figsize = (5*NPlots*NCond*figSizeFactor,5))\n",
    "    markerSize = 5*markerSizeFactor\n",
    "    if NPlots > 1:\n",
    "        for k in range(NPlots):\n",
    "#             vals = []\n",
    "#             for i in range(len(Conditions)):\n",
    "#                 vals.append(data_filtered[data_filtered[CondCol] == Conditions[i]][Parameters[k]])\n",
    "            sns.swarmplot(x=CondCol, y=Parameters[k], data=data_filtered, ax=ax[k], \n",
    "                          size=markerSize, edgecolor='k',linewidth = 1*markerSizeFactor, order= co_order)\n",
    "#             if Parameters[k] == 'EChadwick':\n",
    "#                 ax[k].set_yscale('log')\n",
    "            if Boxplot:\n",
    "                sns.boxplot(x=CondCol, y=Parameters[k], data=data_filtered, ax=ax[k], \n",
    "                            color='w', linewidth = 2, width = 0.5, showfliers = False, order= co_order)\n",
    "#                 data_filtered.boxplot(column=Parameters[k], by = CondCol, ax=ax[k],\n",
    "#                             showfliers = False) # linewidth = 2, width = 0.5\n",
    "            if stats:\n",
    "                if len(box_pairs) == 0:\n",
    "                    box_pairs = makeBoxPairs(co_order)\n",
    "                add_stat_annotation(ax[k], x=CondCol, y=Parameters[k], data=data_filtered,\n",
    "                                   box_pairs = box_pairs,\n",
    "                                   test=statMethod, text_format='star',\n",
    "                                   loc='inside', verbose=statVerbose)\n",
    "            ax[k].set_xlabel('')\n",
    "            ax[k].set_ylabel(Parameters[k])\n",
    "            ax[k].tick_params(axis='x', labelrotation = 10)\n",
    "            ax[k].yaxis.grid(True)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "#         vals = []\n",
    "#         for i in range(len(Conditions)):\n",
    "#             vals.append(data_filtered[data_filtered[CondCol] == Conditions[i]][Parameters[0]])\n",
    "        sns.swarmplot(x=CondCol, y=Parameters[0], data=data_filtered, ax=ax, \n",
    "                          size=markerSize, edgecolor='k',linewidth = 0.5, order= co_order)\n",
    "        if Boxplot:\n",
    "            sns.boxplot(x=CondCol, y=Parameters[0], data=data_filtered, ax=ax, \n",
    "                        color='w', linewidth = 2, width = 0.5, showfliers = False, order= co_order)\n",
    "        if stats:\n",
    "            if len(box_pairs) == 0:\n",
    "                box_pairs = makeBoxPairs(co_order)\n",
    "            add_stat_annotation(ax, x=CondCol, y=Parameters[k], data=data_filtered,\n",
    "                               box_pairs = box_pairs,\n",
    "                               test=statMethod, text_format='star',\n",
    "                               loc='inside', verbose=statVerbose)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(Parameters[0])\n",
    "        ax.tick_params(axis='x', labelrotation = 10)\n",
    "        ax.yaxis.grid(True)\n",
    "    return(fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D1PlotInteractive(data, CondCol='',Parameters=[],Filters=[],AvgPerCell=False,cellID='cellID'):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "#     print(data_filtered[cellID])\n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "    \n",
    "#     return(data_filtered)\n",
    "    \n",
    "    NPlots = len(Parameters)\n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    if NPlots > 1:\n",
    "        plots = []\n",
    "        NCond = len(Conditions)\n",
    "        data_filtered['X'] = 0\n",
    "        data_filtered['X_jitter'] = 0.\n",
    "        dictTicks = {}\n",
    "        for i in range(NCond):\n",
    "            mask = data_filtered[CondCol] == Conditions[i]\n",
    "            data_filtered.loc[mask, 'X'] = i+1\n",
    "            dictTicks[i+1] = Conditions[i]\n",
    "        for i in data_filtered.index:\n",
    "            data_filtered.loc[i, 'X_jitter'] = data_filtered.loc[i, 'X'] + 0.4*(np.random.rand(1)[0]-0.5)\n",
    "        source = ColumnDataSource(\n",
    "            data=data_filtered[[cellID]+[CondCol]+Parameters+['X','X_jitter']]\n",
    "        )        \n",
    "        \n",
    "        for k in range(NPlots):\n",
    "            hover = HoverTool(\n",
    "                tooltips=[\n",
    "                    ('Cell ID', \"@\"+cellID),\n",
    "                    (Parameters[k], \"@\"+Parameters[k]),\n",
    "                ]\n",
    "            )\n",
    "            index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "            p = figure(plot_width=450, plot_height=500, tools=[hover], title=\"InteractivePlot\") # \n",
    "            p.circle('X_jitter', Parameters[k], size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "            # Format\n",
    "            p.x_range = Range1d(0, NCond+1)\n",
    "            p.y_range = Range1d(min(0,1.1*np.min(data_filtered[Parameters[0]])), 1.1*np.max(data_filtered[Parameters[k]]))\n",
    "            p.xaxis.ticker = [i for i in range(1,NCond+1)]\n",
    "            p.xaxis.major_label_overrides = dictTicks\n",
    "            p.xaxis.axis_label = CondCol\n",
    "            p.xaxis.axis_label_text_font_size = '18pt'\n",
    "            p.xaxis.major_label_text_font_size = '16pt'\n",
    "            p.yaxis.axis_label = Parameters[k]\n",
    "            p.yaxis.axis_label_text_font_size = '18pt'\n",
    "            p.yaxis.major_label_text_font_size = '16pt'\n",
    "            \n",
    "            plots.append(p)\n",
    "            \n",
    "        p = gridplot(plots, ncols=2, toolbar_location=None)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                ('Cell ID', \"@\"+cellID),\n",
    "                (Parameters[0], \"@\"+Parameters[0]),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        NCond = len(Conditions)\n",
    "        data_filtered['X'] = 0\n",
    "        data_filtered['X_jitter'] = 0.\n",
    "        dictTicks = {}\n",
    "        for i in range(NCond):\n",
    "            mask = data_filtered[CondCol] == Conditions[i]\n",
    "            data_filtered.loc[mask, 'X'] = i+1\n",
    "            dictTicks[i+1] = Conditions[i]\n",
    "        for i in data_filtered.index:\n",
    "            data_filtered.loc[i, 'X_jitter'] = data_filtered.loc[i, 'X'] + 0.4*(np.random.rand(1)[0]-0.5)\n",
    "        source = ColumnDataSource(\n",
    "            data=data_filtered[[cellID]+[CondCol]+Parameters+['X','X_jitter']]\n",
    "        )\n",
    "        index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "        TOOLS = \"hover,pan,box_zoom,wheel_zoom,reset,save,help\"\n",
    "        p = figure(plot_width=500, plot_height=500, tools=TOOLS, title=\"InteractivePlot\") # \n",
    "        p.circle('X_jitter', Parameters[0], size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "        # Format\n",
    "        p.x_range = Range1d(0, NCond+1)\n",
    "        p.y_range = Range1d(min(0,1.1*np.min(data_filtered[Parameters[0]])), 1.1*np.max(data_filtered[Parameters[0]]))\n",
    "        p.xaxis.ticker = [i for i in range(1,NCond+1)]\n",
    "        p.xaxis.major_label_overrides = dictTicks\n",
    "        p.xaxis.axis_label = CondCol\n",
    "        p.xaxis.axis_label_text_font_size = '18pt'\n",
    "        p.xaxis.major_label_text_font_size = '16pt'\n",
    "        p.yaxis.axis_label = Parameters[0]\n",
    "        p.yaxis.axis_label_text_font_size = '18pt'\n",
    "        p.yaxis.major_label_text_font_size = '16pt'\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data = GlobalTable_meca\n",
    "# CondCol='drug'\n",
    "# Parameters=['SurroundingThickness','EChadwick']\n",
    "# Filters = [(GlobalTable_meca['Validated'] == 1)]\n",
    "# AvgPerCell=True\n",
    "# cellID='CellName'\n",
    "\n",
    "# data_filtered = data\n",
    "# for fltr in Filters:\n",
    "#     data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "# group = data_filtered.groupby(cellID)\n",
    "# dictAggMean = getDictAggMean(data_filtered)\n",
    "# data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "# data_filtered.reset_index(level=0, inplace=True)\n",
    "# data_filtered=data_filtered[[cellID]+[CondCol]+Parameters]\n",
    "# print(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D2Plot(data, XCol='',YCol='',CondCol='',Filters=[], cellID='cellID' ,AvgPerCell=False, linFit=False):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "    \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "        \n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8,5))\n",
    "    markerSize = 5\n",
    "    for c in Conditions:\n",
    "        X = data_filtered[data_filtered[CondCol] == c][XCol]\n",
    "        Y = data_filtered[data_filtered[CondCol] == c][YCol]\n",
    "        ax.plot(X, Y, 'o', markersize = markerSize, markeredgecolor='k',markeredgewidth = 1)\n",
    "        if linFit:\n",
    "            print('Fitting condition: ' + c)\n",
    "            reg = LinearRegression()\n",
    "            reg.fit(X, Y)\n",
    "            print(\"The linear model is: Y = {:.5} + {:.5}X\".format(reg.intercept_[0], reg.coef_[0][0]))\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "    ax.set_xlabel(XCol)\n",
    "#     print(min(0,1.1*np.min(data_filtered[XCol])))\n",
    "    ax.set_xlim([min(0,1.1*np.min(data_filtered[XCol])), 1.1*np.max(data_filtered[XCol])])\n",
    "    ax.set_ylabel(YCol)\n",
    "    ax.set_ylim([min(0,1.1*np.min(data_filtered[YCol])), 1.1*np.max(data_filtered[YCol])])\n",
    "    return(fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D2PlotInteractive(data, XCol='',YCol='',CondCol='',Filters=[], cellID='cellID',AvgPerCell=False):\n",
    "    \n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "\n",
    "    NCond = len(Conditions)\n",
    "    dictTicks = {}\n",
    "    for i in range(NCond):\n",
    "        dictTicks[i+1] = Conditions[i]\n",
    "    \n",
    "    source = ColumnDataSource(\n",
    "        data=data_filtered[[cellID,CondCol,XCol,YCol]]\n",
    "    )\n",
    "    \n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            ('Cell ID', \"@\"+cellID),\n",
    "            (XCol, \"@\"+XCol),\n",
    "            (YCol, \"@\"+YCol),\n",
    "            (CondCol, \"@\"+CondCol),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "    TOOLS = \"pan,box_zoom,wheel_zoom,reset,save,help\"\n",
    "    p = figure(plot_width=900, plot_height=500, tools=TOOLS, title=\"InteractivePlot\",toolbar_location=\"below\") # \n",
    "    p.circle(XCol, YCol, size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "    p.add_tools(hover)\n",
    "    # Format\n",
    "    p.x_range = Range1d(0, 1.1*np.max(data_filtered[XCol]))\n",
    "    p.y_range = Range1d(0, 1.1*np.max(data_filtered[YCol]))\n",
    "    p.xaxis.axis_label = XCol\n",
    "    p.xaxis.axis_label_text_font_size = '18pt'\n",
    "    p.xaxis.major_label_text_font_size = '16pt'\n",
    "    p.yaxis.axis_label = YCol\n",
    "    p.yaxis.axis_label_text_font_size = '18pt'\n",
    "    p.yaxis.major_label_text_font_size = '16pt'\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     31
    ]
   },
   "outputs": [],
   "source": [
    "def makeOrder(M):\n",
    "    A = np.array(M)\n",
    "    n, m = A.shape\n",
    "    order = []\n",
    "    N = n**m\n",
    "    listeIdx = [0 for i in range(m)]\n",
    "    for i in range(N):\n",
    "        tmpText = ''\n",
    "        for j in range(m):\n",
    "            tmpText += A[j,listeIdx[j]]\n",
    "            tmpText += ' & '\n",
    "        tmpText = tmpText[:-3]\n",
    "        order.append(tmpText)\n",
    "        #\n",
    "#         print(listeIdx)\n",
    "        k = len(listeIdx)-1\n",
    "        increment = False\n",
    "        while k>=0 and not increment:\n",
    "            if listeIdx[k] < n-1:\n",
    "                listeIdx[k] += 1\n",
    "                increment = True\n",
    "            else:\n",
    "                listeIdx[k] = 0\n",
    "                k -= 1\n",
    "    return(order)\n",
    "\n",
    "# makeOrder([['none','doxycyclin'],['BSA coated glass','20um fibronectin discs']])\n",
    "\n",
    "def makeBoxPairs(O):\n",
    "    return(list(itertools.combinations(O, 2)))\n",
    "\n",
    "def renameAxes(axes, rD):\n",
    "    try:\n",
    "        N = len(axes)\n",
    "    except:\n",
    "        axes = [axes]\n",
    "        N = 1\n",
    "    for i in range(N):\n",
    "        # set xticks\n",
    "        xticksTextObject = axes[i].get_xticklabels()\n",
    "        xticksList = [xticksTextObject[j].get_text() for j in range(len(xticksTextObject))]\n",
    "        newXticksList = [rD.get(k, k) for k in xticksList]\n",
    "        axes[i].set_xticklabels(newXticksList)\n",
    "        # set xlabel\n",
    "        xlabel = axes[i].get_xlabel()\n",
    "        newXlabel = rD.get(xlabel, xlabel)\n",
    "        axes[i].set_xlabel(newXlabel)\n",
    "        # set ylabel\n",
    "        ylabel = axes[i].get_ylabel()\n",
    "        newYlabel = rD.get(ylabel, ylabel)\n",
    "        axes[i].set_ylabel(newYlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = makeOrder([['none','doxycyclin'],['BSA coated glass','20um fibronectin discs']])\n",
    "makeBoxPairs(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "# pd.reset_option('max_columns')\n",
    "pd.set_option('max_rows', None)\n",
    "# pd.reset_option('max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_ctField().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFluoData().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_ctField = getGlobalTable_ctField()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_ctField = pd.merge(table_ExpConditions, GlobalTable_ctField, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_ctField = pd.merge(GlobalTable_ctField, table_fluo, how=\"left\", on='cellID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_ctField.shape[0]) + ' lines and ' + str(GlobalTable_ctField.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_ctField.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_meca = getGlobalTable_meca()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_ExpConditions, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_fluo, how=\"left\", left_on='CellName', right_on='cellID'\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_meca.shape[0]) + ' lines and ' + str(GlobalTable_meca.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_meca.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawMecaTable = getGlobalTable_meca()\n",
    "# rawMecaTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawMecaTable.loc[]\n",
    "# Agréger tous les M450 WT sous un même nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M270') | (rawMecaTable['ExpType'] == 'DictyDB_M450')), (rawMecaTable['TpsComp'] == '1s')]\n",
    "# Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M450')), (rawMecaTable['TpsComp'] == '1s')]\n",
    "fig, ax = D1Plot(rawMecaTable, CondCol=['ExpType'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('M450 vs M270 for tps comp = 1s')\n",
    "fig.show()\n",
    "# rawMecaTable[Filters[0] & Filters[1] & Filters[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M450'))] #  | (rawMecaTable['ExpType'] == 'DictyDB_M450-Multi')\n",
    "fig, ax00 = D1Plot(rawMecaTable, CondCol=['TpsComp'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                   Filters=Filters,AvgPerCell=False,cellID='CellName',stats=False,figSizeFactor = 1.8,markerSizeFactor=0.5)\n",
    "fig.suptitle('M450, various rates')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M270'))]\n",
    "fig, ax01 = D1Plot(rawMecaTable, CondCol=['TpsComp'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "                   AvgPerCell=False,cellID='CellName',stats=False,figSizeFactor = 1.8,markerSizeFactor=0.5)\n",
    "fig.suptitle('M270, various rates')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['medianThickness'] <= 1000)]\n",
    "fig, ax = D1Plot(GlobalTable_ctField, CondCol=['drug','substrate'],Parameters=['medianThickness','fluctuAmpli'],Filters=Filters)\n",
    "fig.suptitle('3T3aSFL on patterns: Ct Field')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['none','doxycyclin'],['BSA coated glass','20um fibronectin discs']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['drug','substrate'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName', co_order=co_order)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['drug','substrate'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('3T3aSFL on patterns: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder([['aSFL','aSFL-6FP'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder([['aSFL','aSFL-6FP'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order)\n",
    "fig.suptitle('3T3aSFL SHORT vs LONG linker: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = 'drug', Filters=Filters)\n",
    "fig.suptitle('3T3aSFL - Dh = f(H)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xticksTextObject = ax.get_xticklabels()\n",
    "xticksList = [xticksTextObject[j].get_text() for j in range(len(xticksTextObject))]\n",
    "newXticksList = [renameDict1.get(k, k) for k in xticksList]\n",
    "newXticksList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "\n",
    "p = D1PlotInteractive(GlobalTable_ctField, CondCol='drug',Parameters=['medianThickness','fluctuAmpli'],Filters=Filters)\n",
    "p.children[0][0].title.text = '3T3aSFL on patterns: Ct Field'\n",
    "p.children[0][0].title.text_font_size = '16pt'\n",
    "p.children[1][0].title.text = ''\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D1PlotInteractive(GlobalTable_meca, CondCol='drug',Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "p.children[0][0].title.text = '3T3aSFL on patterns: Compressions'\n",
    "p.children[0][0].title.text_font_size = '14pt'\n",
    "p.children[1][0].title.text = ''\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = 'drug', Filters=Filters)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='SurroundingThickness',YCol='EChadwick',CondCol = 'drug', Filters=Filters, cellID = 'CellName')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True)]\n",
    "p = D2PlotInteractive(GlobalTable_ctField, XCol='meanFluoPeakAmplitude',YCol='medianThickness',CondCol = 'drug', Filters=Filters, cellID = 'cellID',AvgPerCell=True)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True)]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = '3T3aSFL expressing linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL expressing linker: E(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL expressing linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL-6FP expressing long linker: E(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='SurroundingThickness', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL-6FP expressing long linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thickness and module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameDict1 = {'SurroundingThickness':'Surrounding Thickness (nm)',\\\n",
    "               'EChadwick': 'E Chadwick (Pa)',\\\n",
    "               'medianThickness': 'Median Thickness (nm)',\\\n",
    "               'fluctuAmpli': 'Fluctuations Amplitude (nm)',\\\n",
    "               'meanFluoPeakAmplitude' : 'Fluo Intensity (a.u.)',\\\n",
    "               'none & BSA coated glass':'control & non adherent',\\\n",
    "               'doxycyclin & BSA coated glass':'iMC & non adherent',\\\n",
    "               'none & 20um fibronectin discs':'control & adherent on fibro',\\\n",
    "               'doxycyclin & 20um fibronectin discs':'iMC & adherent on fibro',\\\n",
    "               'BSA coated glass & none':'control & non adherent',\\\n",
    "               'BSA coated glass & doxycyclin':'iMC & non adherent',\\\n",
    "               '20um fibronectin discs & none':'control & adherent on fibro',\\\n",
    "               '20um fibronectin discs & doxycyclin':'iMC & adherent on fibro',\\\n",
    "               'aSFL & none':'aSFL control',\\\n",
    "               'aSFL & doxycyclin':'aSFL iMC',\\\n",
    "               'aSFL-6FP & none':'aSFL-6FP control',\\\n",
    "               'aSFL-6FP & doxycyclin':'aSFL-6FP long-iMC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['BSA coated glass','20um fibronectin discs'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['substrate','drug'],\\\n",
    "                 Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "                 AvgPerCell=True, cellID='CellName', co_order=co_order)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['BSA coated glass','20um fibronectin discs'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['substrate','drug'],\\\n",
    "                 Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "                 AvgPerCell=True, cellID='CellName', co_order=co_order, stats=False)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()\n",
    "# fig.savefig(todayFigDir + '//' + 'compressionsBSAvsFibro_woStats.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['medianThickness'] <= 1000)]\n",
    "fig, ax = D1Plot(GlobalTable_ctField, CondCol=['drug','substrate'],Parameters=['medianThickness','fluctuAmpli'],Filters=Filters,stats=False)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on patterns: Constant Field')\n",
    "fig.show()\n",
    "# fig.savefig(todayFigDir + '//' + 'constantFieldFibro_woStats.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder([['aSFL','aSFL-6FP'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order,stats=False)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL SHORT vs LONG linker: Compressions')\n",
    "fig.show()\n",
    "# fig.savefig(todayFigDir + '//' + 'compressionsShortvsLongLinker_woStats.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation with fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True)]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='meanFluoPeakAmplitude',YCol='medianThickness',CondCol = 'drug', Filters=Filters, cellID = 'cellID',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: medianH(fluo)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "# p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "# p.title.text = 'aSFL expressing linker: E(fluo)'\n",
    "# p.title.text_font_size = '16pt'\n",
    "# show(p)\n",
    "\n",
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: E(fluo)')\n",
    "# p.title.text_font_size = '16pt'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "# p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "# p.title.text = 'aSFL expressing linker: H(fluo)'\n",
    "# p.title.text_font_size = '16pt'\n",
    "# show(p)\n",
    "\n",
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: H(fluo)')\n",
    "# p.title.text_font_size = '16pt'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL-6FP expressing long linker: E(fluo)')\n",
    "# p.title.text_font_size = '16pt'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='SurroundingThickness', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL-6FP expressing long linker: H(fluo)')\n",
    "# p.title.text_font_size = '16pt'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create a test table to test different fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Script !\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "testFileName = 'testFitCompression.txt'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "list_mecaFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                  if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                  and ('R40' in f))] # Change to allow different formats in the future\n",
    "expDf = getExperimentalConditions()\n",
    "tableDictTest = {}\n",
    "\n",
    "for f in list_mecaFiles:\n",
    "    print(f)\n",
    "    tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "    tsDF = pd.read_csv(tS_DataFilePath, ';')\n",
    "    \n",
    "    split_f = f.split('_')\n",
    "    tsDF.dx, tsDF.dy, tsDF.dz, tsDF.D2, tsDF.D3 = tsDF.dx*1000, tsDF.dy*1000, tsDF.dz*1000, tsDF.D2*1000, tsDF.D3*1000\n",
    "    thisManipID = split_f[0] + '_' + split_f[1]\n",
    "    expDf['manipID'] = expDf['date'] + '_' + expDf['manip']\n",
    "    thisExpDf = expDf.loc[expDf['manipID'] == thisManipID]\n",
    "    DIAMETER = thisExpDf.at[thisExpDf.index.values[0], 'bead diameter']\n",
    "    thisCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "    Ncomp = max(tsDF['idxCompression'])\n",
    "    \n",
    "    for i in range(1, Ncomp+1): #Ncomp+1):\n",
    "        thisCompHiD = thisCellID + '__' + str(i) + '__h'\n",
    "        thisCompFiD = thisCellID + '__' + str(i) + '__f'\n",
    "        print(thisCompHiD)\n",
    "        \n",
    "        thisCompDf = tsDF.loc[tsDF['idxCompression'] == i, :]\n",
    "        iStart = (findFirst(tsDF['idxCompression'], i))\n",
    "        iStop = iStart + thisCompDf.shape[0]\n",
    "        \n",
    "        # Delimit the start of the increase of B (typically the moment when the field decrease from 5 to 3)\n",
    "        # and the end of its decrease (typically when it goes back from 3 to 5)\n",
    "        \n",
    "        listB = thisCompDf.B.values\n",
    "        offsetStart, offsetStop = 0, 0\n",
    "        minB, maxB = min(listB), max(listB)\n",
    "        thresholdB = (maxB-minB)/50\n",
    "        k = 0\n",
    "        \n",
    "        while (listB[k] > minB+thresholdB) or (listB[-1-k] > minB+thresholdB):\n",
    "            offsetStart += int(listB[k] > minB+thresholdB)\n",
    "            k += 1\n",
    "        \n",
    "        jStart = offsetStart\n",
    "        jMax = np.argmax(thisCompDf.B)\n",
    "        \n",
    "        hCompr = (thisCompDf.D3.values[jStart:jMax+1] - DIAMETER)\n",
    "        fCompr = (thisCompDf.F.values[jStart:jMax+1])\n",
    "        \n",
    "        tableDictTest[thisCompHiD] = hCompr\n",
    "        tableDictTest[thisCompFiD] = fCompr\n",
    "        \n",
    "saveFile = open(testFilePath, 'w')\n",
    "for k in tableDictTest.keys():\n",
    "    saveFile.write(k)\n",
    "    for i in range(len(tableDictTest[k])):\n",
    "        saveFile.write(';')\n",
    "        saveFile.write(str(tableDictTest[k][i]))\n",
    "    saveFile.write('\\n')\n",
    "saveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create a test table to try statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Script !\n",
    "\n",
    "# Create a test table to try statistical tests\n",
    "\n",
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "GlobalTable_meca = getGlobalTable_meca()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_ExpConditions, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_fluo, how=\"left\", left_on='CellName', right_on='cellID'\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_meca.shape[0]) + ' lines and ' + str(GlobalTable_meca.shape[1]) + ' columns.')\n",
    "\n",
    "\n",
    "\n",
    "# Table 1\n",
    "\n",
    "testFileName = 'testTestStats01_allComp.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['BSA coated glass','20um fibronectin discs'],['none','doxycyclin']])\n",
    "data = GlobalTable_meca\n",
    "CondCol=['substrate','drug']\n",
    "Parameters=['SurroundingThickness','EChadwick']\n",
    "AvgPerCell=False\n",
    "cellID='CellName'\n",
    "\n",
    "data_filtered = data\n",
    "for fltr in Filters:\n",
    "    data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "NCond = len(CondCol)    \n",
    "if NCond == 1:\n",
    "    CondCol = CondCol[0]\n",
    "elif NCond > 1:\n",
    "    newColName = ''\n",
    "    for i in range(NCond):\n",
    "        newColName += CondCol[i]\n",
    "        newColName += ' & '\n",
    "    newColName = newColName[:-3]\n",
    "    data_filtered[newColName] = ''\n",
    "    for i in range(NCond):\n",
    "        data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "    data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "    CondCol = newColName\n",
    "    \n",
    "if AvgPerCell:\n",
    "    group = data_filtered.groupby(cellID)\n",
    "    dictAggMean = getDictAggMean(data_filtered)\n",
    "    data_filtered = group.agg(dictAggMean)\n",
    "\n",
    "data_filtered.sort_values(CondCol, axis=0, ascending=True, inplace=True)\n",
    "\n",
    "df_output = data_filtered[[cellID, 'CompNum', newColName] + Parameters]\n",
    "\n",
    "df_output.to_csv(testFilePath)\n",
    "\n",
    "\n",
    "\n",
    "# Table 2\n",
    "\n",
    "testFileName = 'testTestStats02_avgPerCell.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['BSA coated glass','20um fibronectin discs'],['none','doxycyclin']])\n",
    "data = GlobalTable_meca\n",
    "CondCol=['substrate','drug']\n",
    "Parameters=['SurroundingThickness','EChadwick']\n",
    "AvgPerCell=True\n",
    "cellID='CellName'\n",
    "\n",
    "data_filtered = data\n",
    "for fltr in Filters:\n",
    "    data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "NCond = len(CondCol)    \n",
    "if NCond == 1:\n",
    "    CondCol = CondCol[0]\n",
    "elif NCond > 1:\n",
    "    newColName = ''\n",
    "    for i in range(NCond):\n",
    "        newColName += CondCol[i]\n",
    "        newColName += ' & '\n",
    "    newColName = newColName[:-3]\n",
    "    data_filtered[newColName] = ''\n",
    "    for i in range(NCond):\n",
    "        data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "    data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "    CondCol = newColName\n",
    "    \n",
    "if AvgPerCell:\n",
    "    group = data_filtered.groupby(cellID)\n",
    "    dictAggMean = getDictAggMean(data_filtered)\n",
    "    data_filtered = group.agg(dictAggMean)\n",
    "\n",
    "data_filtered.sort_values(CondCol, axis=0, ascending=True, inplace=True)\n",
    "\n",
    "df_output = data_filtered[[cellID, 'CompNum', newColName] + Parameters]\n",
    "\n",
    "df_output.to_csv(testFilePath)\n",
    "\n",
    "\n",
    "# Table 3\n",
    "# Fake data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1,2, figsize = (8, 5))\n",
    "# axes[0].plot(np.ones(len(resDict['nodrug']['median'])), resDict['nodrug']['median'], 'co')\n",
    "# axes[0].plot(2*np.ones(len(resDict['doxy']['median'])), resDict['doxy']['median'], 'ro')\n",
    "# axes[0].set_xlim(0.5, 2.5)\n",
    "# axes[0].set_ylabel('Median Thickness (nm)')\n",
    "# axes[0].set_xticks([1,2])\n",
    "# axes[0].set_xticklabels(['Control','Doxycylin'])\n",
    "# axes[1].plot(np.ones(len(resDict['nodrug']['fluctu'])), resDict['nodrug']['fluctu'], 'co')\n",
    "# axes[1].plot(2*np.ones(len(resDict['doxy']['fluctu'])), resDict['doxy']['fluctu'], 'ro')\n",
    "# axes[1].set_xlim(0.5, 2.5)\n",
    "# axes[1].set_ylabel('Thickness Fluctuations (nm)')\n",
    "# axes[1].set_xticks([1,2])\n",
    "# axes[1].set_xticklabels(['Control','Doxycylin'])\n",
    "# fig.savefig(\"C://Users//JosephVermeil//Desktop//ActinCortexAnalysis//DataAnalysis//constantField.png\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Old code\n",
    "conditions = ['nodrug', 'doxy']\n",
    "correspondance = {conditions[0] : 'M1', conditions[1] : 'M2'}\n",
    "allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".txt\"))]\n",
    "dates = ['21-02-10']\n",
    "resDict = {conditions[0] : {}, conditions[1] : {}}\n",
    "for C in conditions:\n",
    "    resDict[C]['accepted'] = []\n",
    "    resDict[C]['rejected'] = []\n",
    "    resDict[C]['median'] = []\n",
    "    resDict[C]['fluctu'] = []\n",
    "    for D in dates:\n",
    "        for f in allTimeSeriesDataFiles:\n",
    "            if correspondance[C] in f and D in f:\n",
    "                split_f = f.split('_')\n",
    "                cellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "                currentCellTS = getCellTimeSeriesData(cellID)\n",
    "                D3 = currentCellTS.D3.values\n",
    "                decile_1 = np.percentile(D3, 10)\n",
    "                median = np.median(D3)\n",
    "                decile_9 = np.percentile(D3, 90)\n",
    "                if decile_1 < 0:\n",
    "                    resDict[C]['rejected'].append(cellID)\n",
    "                else:\n",
    "                    resDict[C]['accepted'].append(cellID)\n",
    "                    resDict[C]['median'].append(median)\n",
    "                    resDict[C]['fluctu'].append(decile_9-decile_1)\n",
    "#resDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GlobalTable_ctField = getGlobalTable_ctField()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "GlobalTable_ctField = pd.merge(table_ExpConditions, GlobalTable_ctField, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "\n",
    "# data=pd.DataFrame(dict(\n",
    "#             x=[1, 2, 3, 4, 5],\n",
    "#             y=[2, 5, 8, 2, 7],\n",
    "#             desc=['A', 'A', 'C', 'd', 'E'],\n",
    "#         ))\n",
    "\n",
    "data = GlobalTable_ctField[['medianThickness','fluctuAmpli','cellID']]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "# hover = HoverTool(\n",
    "#         tooltips=[\n",
    "#             (\"index\", \"$index\"),\n",
    "#             (\"(x,y)\", \"($x, $y)\"),\n",
    "#             (\"desc\", \"@desc\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"medianThickness\", \"@medianThickness\"),\n",
    "            (\"fluctuAmpli\", \"@fluctuAmpli\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(plot_width=300, plot_height=300, tools=[hover], title=\"Mouse over the dots\")\n",
    "\n",
    "p.circle('medianThickness', 'fluctuAmpli', size=20, source=data)\n",
    "\n",
    "show(p)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(dict(\n",
    "            x=[1, 2, 3, 4, 5],\n",
    "            y=[2, 5, 8, 2, 7],\n",
    "            desc=['A', 'A', 'C', 'd', 'E'],\n",
    "        ))\n",
    "\n",
    "Conditions = list(data['desc'].unique())\n",
    "NCond = len(Conditions)\n",
    "data['X'] = 0\n",
    "for i in range(NCond):\n",
    "    mask = data['desc'] == Conditions[i]\n",
    "    data.loc[mask, ['X']] = i+1\n",
    "data.index = data.x\n",
    "data = data.drop(['x'], axis = 1)\n",
    "\n",
    "data.reset_index(level=0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
