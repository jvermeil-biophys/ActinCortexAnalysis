{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from cycler import cycler\n",
    "from copy import copy\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dateFormatExcel = re.compile('\\d{2}/\\d{2}/\\d{4}')\n",
    "dateFormatOk = re.compile('\\d{2}-\\d{2}-\\d{2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Range1d\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.layouts import gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "# colors = prop_cycle.by_key()['color']\n",
    "new_color_cycle = cycler(color=['#ff7f0e', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
    "plt.rcParams['axes.prop_cycle'] = new_color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mainDir = \"C://Users//JosephVermeil//Desktop//ActinCortexAnalysis\"\n",
    "experimentalDataDir = os.path.join(mainDir, \"ExperimentalData\")\n",
    "dataDir = os.path.join(mainDir, \"DataAnalysis\")\n",
    "timeSeriesDataDir = os.path.join(dataDir, \"TimeSeriesData\")\n",
    "allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\"))]\n",
    "allTimeSeriesDataFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8
    ]
   },
   "outputs": [],
   "source": [
    "def get_R2(Y1, Y2):\n",
    "    meanY = np.mean(Y1)\n",
    "    meanYarray = meanY*np.ones(len(Y1))\n",
    "    SST = np.sum((Y1-meanYarray)**2)\n",
    "    SSE = np.sum((Y2-meanYarray)**2)\n",
    "    R2 = SSE/SST\n",
    "    return(R2)\n",
    "\n",
    "def getDictAggMean(df):\n",
    "    dictAggMean = {}\n",
    "    for c in df.columns:\n",
    "#         t = df[c].dtype\n",
    "#         print(c, t)\n",
    "        try :\n",
    "            if np.array_equal(df[c], df[c].astype(bool)):\n",
    "                dictAggMean[c] = 'first'\n",
    "            else:\n",
    "                try:\n",
    "                    np.mean(df[c])\n",
    "                    dictAggMean[c] = 'mean'\n",
    "                except:\n",
    "                    dictAggMean[c] = 'first'\n",
    "        except:\n",
    "                dictAggMean[c] = 'first'\n",
    "    return(dictAggMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16,
     35,
     56
    ]
   },
   "outputs": [],
   "source": [
    "def getCellTimeSeriesData(cellID):\n",
    "    allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\"))]\n",
    "    fileFound = False\n",
    "    nFile = len(allTimeSeriesDataFiles)\n",
    "    iFile = 0\n",
    "    while (not fileFound) and (iFile < nFile):\n",
    "        f = allTimeSeriesDataFiles[iFile]\n",
    "        if f.startswith(cellID):\n",
    "            timeSeriesDataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "            timeSeriesDataFrame = pd.read_csv(timeSeriesDataFilePath, sep=';')\n",
    "            fileFound = True\n",
    "        iFile += 1\n",
    "    if not fileFound:\n",
    "        timeSeriesDataFrame = pd.DataFrame([])\n",
    "    return(timeSeriesDataFrame)\n",
    "\n",
    "def plotCellTimeSeriesData(cellID):\n",
    "    X = 'T'\n",
    "    Y = np.array(['B', 'F', 'dx', 'dy', 'dz', 'D2', 'D3'])\n",
    "    units = np.array([' (mT)', ' (pN)', ' (µm)', ' (µm)', ' (µm)', ' (µm)', ' (µm)'])\n",
    "    timeSeriesDataFrame = getCellTimeSeriesData(cellID)\n",
    "    if not timeSeriesDataFrame.size == 0:\n",
    "#         plt.tight_layout()\n",
    "#         fig.show() # figsize=(20,20)\n",
    "        axes = timeSeriesDataFrame.plot(x=X, y=Y, kind='line', ax=None, subplots=True, sharex=True, sharey=False, layout=None, \\\n",
    "                       figsize=(8,10), use_index=True, title = cellID + '- Time dependant data', grid=None, legend=False, style=None, logx=False, logy=False, \\\n",
    "                       loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, \\\n",
    "                       table=False, yerr=None, xerr=None, secondary_y=False, sort_columns=False)\n",
    "        plt.gcf().tight_layout()\n",
    "        for i in range(len(Y)):\n",
    "            axes[i].set_ylabel(Y[i] + units[i])\n",
    "        \n",
    "    else:\n",
    "        print('cell not found')\n",
    "        \n",
    "def addExcludedCell(cellID, motive):\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    nLines = len(lines)\n",
    "    excludedCellsList = []\n",
    "    for iLine in range(nLines):\n",
    "        line = lines[iLine]\n",
    "        splitLine = line[:-1].split(',')\n",
    "        excludedCellsList.append(splitLine[0])\n",
    "    if cellID in excludedCellsList:\n",
    "        newlines = copy(lines)\n",
    "        iLineOfInterest = excludedCellsList.index(cellID)\n",
    "        if motive not in newlines[iLineOfInterest][:-1].split(','):\n",
    "            newlines[iLineOfInterest] = newlines[iLineOfInterest][:-1] + ',' + motive + '\\n'            \n",
    "    else:\n",
    "        newlines = copy(lines)\n",
    "        newlines.append('' + cellID + ',' + motive + '\\n')\n",
    "    f.close()\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'w')\n",
    "    f.writelines(newlines)\n",
    "    \n",
    "def getExcludedCells():\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    nLines = len(lines)\n",
    "    excludedCellsDict = {}\n",
    "    for iLine in range(nLines):\n",
    "        line = lines[iLine]\n",
    "        splitLine = line[:-1].split(',')\n",
    "        excludedCellsDict[splitLine[0]] = splitLine[1:]\n",
    "    return(excludedCellsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = getCellTimeSeriesData('21-02-10_M1_P1_C1')\n",
    "plotCellTimeSeriesData('21-02-10_M2_P1_C4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCellTimeSeriesData('21-02-10_M1_P1_C2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of the get R2 function.\n",
    "# T = df['T']\n",
    "# Y = df['D3']\n",
    "# plt.plot(T,Y)\n",
    "# p, residuals, rank, singular_values, rcond = np.polyfit(T, Y, deg=5, full=True)\n",
    "# plt.plot(T, Y)\n",
    "# Y2 = np.zeros(len(T))\n",
    "# for i in range(len(T)):\n",
    "#     deg = len(p)-1\n",
    "#     for k in range(deg+1):\n",
    "#         Y2[i] += p[k]*(T[i]**(deg-k))\n",
    "# plt.plot(T,Y2)\n",
    "# get_R2(Y, Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GlobalTables functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getExperimentalConditions(save = False):\n",
    "    # Getting the table\n",
    "    experimentalDataFile = 'ExperimentalConditions.csv'\n",
    "    experimentalDataFilePath = os.path.join(experimentalDataDir, experimentalDataFile)\n",
    "    expConditionsDF = pd.read_csv(experimentalDataFilePath, sep=';',header=0)\n",
    "    print('Extracted a table with ' + str(expConditionsDF.shape[0]) + ' lines and ' + str(expConditionsDF.shape[1]) + ' columns.')\n",
    "    # Cleaning the table\n",
    "    try:\n",
    "        for c in expConditionsDF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                expConditionsDF = expConditionsDF.drop([c], axis=1)\n",
    "        expConditionsDF = expConditionsDF.convert_dtypes()\n",
    "\n",
    "        listTextColumns = []\n",
    "        for col in expConditionsDF.columns:\n",
    "            if expConditionsDF[col].dtype == 'string':\n",
    "                listTextColumns.append(col)\n",
    "\n",
    "        expConditionsDF[listTextColumns] = expConditionsDF[listTextColumns].apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "        expConditionsDF['scale pixel per um'] = expConditionsDF['scale pixel per um'].astype(float)\n",
    "        expConditionsDF['optical index correction'] = \\\n",
    "                  expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[0]).astype(float) \\\n",
    "                / expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[1]).astype(float)\n",
    "        expConditionsDF['magnetic field correction'] = expConditionsDF['magnetic field correction'].astype(float)\n",
    "        expConditionsDF['with fluo images'] = expConditionsDF['with fluo images'].astype(bool)\n",
    "\n",
    "        expConditionsDF['ramp field'] = \\\n",
    "        expConditionsDF['ramp field'].apply(lambda x: [x.split(';')[0], x.split(';')[1]] if not pd.isnull(x) else [])\n",
    "\n",
    "\n",
    "        dateExemple = expConditionsDF.loc[expConditionsDF.index[1],'date']\n",
    "\n",
    "        if re.match(dateFormatExcel, dateExemple):\n",
    "            print('dates corrected')\n",
    "            expConditionsDF.loc[1:,'date'] = expConditionsDF.loc[1:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])        \n",
    "        \n",
    "    except:\n",
    "        print('Unexpected bug with the cleaning step')\n",
    "\n",
    "    if save:\n",
    "        saveName = 'ExperimentalConditions.csv'\n",
    "        savePath = os.path.join(experimentalDataDir, saveName)\n",
    "        expConditionsDF.to_csv(savePath, sep=';')\n",
    "\n",
    "    expConditionsDF['manipID'] = expConditionsDF['date'] + '_' + expConditionsDF['manip']\n",
    "    \n",
    "    return(expConditionsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getExperimentalConditions(save=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     19,
     38,
     85
    ]
   },
   "outputs": [],
   "source": [
    "def analyseTimeSeries_ctField(tS_df):\n",
    "    results = {}\n",
    "    results['duration'] = np.max(tS_df['T'])\n",
    "    results['medianRawB'] = np.median(tS_df.B)\n",
    "    results['medianThickness'] = np.median(tS_df.D3)\n",
    "    results['1stDThickness'] = np.percentile(tS_df.D3, 10)\n",
    "    results['9thDThickness'] = np.percentile(tS_df.D3, 90)\n",
    "    results['fluctuAmpli'] = results['9thDThickness'] - results['1stDThickness']\n",
    "    results['validated'] = (results['1stDThickness'] > 0)\n",
    "    X, Y = tS_df['T'], tS_df['D3']\n",
    "    p, residuals, rank, singular_values, rcond = np.polyfit(X, Y, deg=5, full=True)\n",
    "    Y2 = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        deg = len(p)-1\n",
    "        for k in range(deg+1):\n",
    "            Y2[i] += p[k]*(X[i]**(deg-k))\n",
    "    results['R2_polyFit'] = get_R2(Y, Y2)\n",
    "    return(results)\n",
    "\n",
    "def createCtFieldDataDict(list_ctFieldFiles):\n",
    "    tableDict = {}\n",
    "    tableDict['date'], tableDict['cellName'], tableDict['cellID'], tableDict['manipID'] = [], [], [], []\n",
    "    tableDict['duration'], tableDict['medianRawB'], tableDict['medianThickness'] = [], [], []\n",
    "    tableDict['1stDThickness'], tableDict['9thDThickness'], tableDict['fluctuAmpli'] = [], [], []\n",
    "    tableDict['R2_polyFit'], tableDict['validated'] = [], []\n",
    "    for f in list_ctFieldFiles:\n",
    "        split_f = f.split('_')\n",
    "        tableDict['date'].append(split_f[0])\n",
    "        tableDict['cellName'].append(split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['cellID'].append(split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['manipID'].append(split_f[0] + '_' + split_f[1])\n",
    "        tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "        current_tS_df = pd.read_csv(tS_DataFilePath, ';')\n",
    "        current_resultDict = analyseTimeSeries_ctField(current_tS_df)\n",
    "        for k in current_resultDict.keys():\n",
    "            tableDict[k].append(current_resultDict[k])\n",
    "    return(tableDict)\n",
    "\n",
    "def computeGlobalTable_ctField(task = 'fromScratch', fileName = 'Global_CtFieldData', save = False):\n",
    "    ctFieldTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                                  if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                                      and ('thickness' in f))]\n",
    "#     print(ctFieldTimeSeriesDataFiles)\n",
    "    if task == 'fromScratch':\n",
    "        # create a dict containing the data\n",
    "        tableDict = createCtFieldDataDict(ctFieldTimeSeriesDataFiles)\n",
    "        # create the table\n",
    "        CtField_DF = pd.DataFrame(tableDict)\n",
    "        \n",
    "    elif task == 'updateExisting':\n",
    "        # get existing table\n",
    "        try:\n",
    "            savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "            existing_CtField_DF = pd.read_csv(savePath, sep=';')\n",
    "            for c in existing_CtField_DF.columns:\n",
    "                if 'Unnamed' in c:\n",
    "                    existing_CtField_DF = existing_CtField_DF.drop([c], axis=1)\n",
    "        except:\n",
    "            print('No existing table found')\n",
    "        # find which of the time series files are new\n",
    "        new_ctFieldTimeSeriesDataFiles = []\n",
    "        for f in ctFieldTimeSeriesDataFiles:\n",
    "            split_f = f.split('_')\n",
    "            currentCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "            if currentCellID not in existing_CtField_DF.cellID.values:\n",
    "                new_ctFieldTimeSeriesDataFiles.append(f)\n",
    "        new_tableDict = createCtFieldDataDict(new_ctFieldTimeSeriesDataFiles)\n",
    "        # create the table with new data\n",
    "        new_CtField_DF = pd.DataFrame(new_tableDict)\n",
    "        # fuse the two\n",
    "        new_CtField_DF.index += existing_CtField_DF.shape[0]\n",
    "        CtField_DF = pd.concat([existing_CtField_DF, new_CtField_DF])\n",
    "    \n",
    "    dateExemple = CtField_DF.loc[CtField_DF.index[0],'date']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        CtField_DF.loc[:,'date'] = CtField_DF.loc[:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])\n",
    "    \n",
    "    if save:\n",
    "        saveName = fileName + '.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        CtField_DF.to_csv(savePath, sep=';')\n",
    "        \n",
    "    return(CtField_DF)\n",
    "\n",
    "def getGlobalTable_ctField(fileName = 'Global_CtFieldData'):\n",
    "    try:\n",
    "        savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "        CtField_DF = pd.read_csv(savePath, sep=';')\n",
    "        for c in CtField_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                CtField_DF = CtField_DF.drop([c], axis=1)\n",
    "        print('Extracted a table with ' + str(CtField_DF.shape[0]) + ' lines and ' + str(CtField_DF.shape[1]) + ' columns.')\n",
    "        \n",
    "    except:\n",
    "        print('No existing table found')\n",
    "        \n",
    "    dateExemple = CtField_DF.loc[CtField_DF.index[0],'date']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        print('dates corrected')\n",
    "        CtField_DF.loc[:,'date'] = CtField_DF.loc[:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])\n",
    "#         mecaDF['ManipID'] = mecaDF['ExpDay'] + '_' + mecaDF['CellName'].apply(lambda x: x.split('_')[0])\n",
    "    return(CtField_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "computeGlobalTable_ctField(task='updateExisting',save=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_ctField().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meca data\n",
    "\n",
    "TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     24,
     63
    ]
   },
   "outputs": [],
   "source": [
    "def analyseTimeSeries_meca(tS_df):\n",
    "    results = {}\n",
    "    ### TBC\n",
    "    return(results)\n",
    "\n",
    "def createMecaDataDict(list_ctFieldFiles):\n",
    "    tableDict = {}\n",
    "    tableDict['date'], tableDict['cellName'], tableDict['cellID'] = [], [], []\n",
    "#     tableDict[''], tableDict[''], tableDict[''] = [], [], []\n",
    "#     tableDict[''], tableDict[''], tableDict[''] = [], [], []\n",
    "#     tableDict[''] = []\n",
    "# TBC\n",
    "    for f in list_ctFieldFiles:\n",
    "        split_f = f.split('_')\n",
    "        tableDict['date'].append(split_f[0])\n",
    "        tableDict['cellName'].append(split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['cellID'].append(split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "        current_tS_df = pd.read_csv(tS_DataFilePath, ';')\n",
    "        current_resultDict = analyseTimeSeries_meca(current_tS_df)\n",
    "        for k in current_resultDict.keys():\n",
    "            tableDict[k].append(current_resultDict[k])\n",
    "    return(tableDict)\n",
    "\n",
    "def computeGlobalTable_meca(task = 'fromScratch', fileName = 'Global_CtFieldData', save = True):\n",
    "    mecaTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                                  if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                                      and ('R40' in f))] # Change to allow different formats in the future\n",
    "    print(ctFieldTimeSeriesDataFiles)\n",
    "    if task == 'fromScratch':\n",
    "        # create a dict containing the data\n",
    "        tableDict = createMecaDataDict(mecaTimeSeriesDataFiles)\n",
    "        # create the table\n",
    "        meca_DF = pd.DataFrame(tableDict)\n",
    "        \n",
    "    elif task == 'updateExisting':\n",
    "        # get existing table\n",
    "        try:\n",
    "            savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "            existing_meca_DF = pd.read_csv(savePath, sep=';')\n",
    "        except:\n",
    "            print('No existing table found')\n",
    "        # find which of the time series files are new\n",
    "        new_mecaTimeSeriesDataFiles = []\n",
    "        for f in mecaTimeSeriesDataFiles:\n",
    "            split_f = f.split('_')\n",
    "            currentCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "            if currentCellID not in existing_meca_DF.cellID:\n",
    "                new_mecaTimeSeriesDataFiles.append(f)\n",
    "        new_tableDict = createCtFieldDataDict(new_mecaTimeSeriesDataFiles)\n",
    "        # create the table with new data\n",
    "        new_meca_DF = pd.dataframe(tableDict)\n",
    "        # fuse the two\n",
    "        meca_DF = pd.concat([existing_meca_DF, new_meca_DF])\n",
    "        \n",
    "    if save:\n",
    "        saveName = fileName + '.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        meca_DF.to_csv(savePath, sep=';')\n",
    "        \n",
    "    return(meca_DF)\n",
    "              \n",
    "def getGlobalTable_meca(fileName = 'Global_mecaData'):\n",
    "    try:\n",
    "        savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "        meca_DF = pd.read_csv(savePath, sep=';')\n",
    "        print('Extracted a table with ' + str(meca_DF.shape[0]) + ' lines and ' + str(meca_DF.shape[1]) + ' columns.')\n",
    "    except:\n",
    "        print('No existing table found')\n",
    "    for c in meca_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                meca_DF = meca_DF.drop([c], axis=1)\n",
    "    if not ('manipID' in meca_DF.columns):\n",
    "        meca_DF['manipID'] = meca_DF['ExpDay'] + '_' + meca_DF['CellID'].apply(lambda x: x.split('_')[0])\n",
    "    dateExemple = meca_DF.loc[meca_DF.index[0],'ExpDay']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        print('bad date')\n",
    "    return(meca_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_meca().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getFluoData(save = False):\n",
    "    # Getting the table\n",
    "    fluoDataFile = 'FluoQuantification.csv'\n",
    "    fluoDataFilePath = os.path.join(dataDir, fluoDataFile)\n",
    "    fluoDF = pd.read_csv(fluoDataFilePath, sep=';',header=0)\n",
    "    print('Extracted a table with ' + str(fluoDF.shape[0]) + ' lines and ' + str(fluoDF.shape[1]) + ' columns.')\n",
    "    # Cleaning the table\n",
    "    try:\n",
    "        for c in fluoDF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                fluoDF = fluoDF.drop([c], axis=1)\n",
    "        \n",
    "    except:\n",
    "        print('Unexpected bug with the cleaning step')\n",
    "\n",
    "    if save:\n",
    "        saveName = 'FluoQuantification.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        fluoDF.to_csv(savePath, sep=';')\n",
    "\n",
    "    \n",
    "    return(fluoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFluoData().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data formatting and filtering\n",
    "\n",
    "TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecaDataFile = 'Global_MecaData.csv'\n",
    "mecaDataFilePath = os.path.join(dataDir, mecaDataFile)\n",
    "mecaDF = pd.read_csv(mecaDataFilePath, sep=';')\n",
    "print('Extracted a table with ' + str(mecaDF.shape[0]) + ' lines and ' + str(mecaDF.shape[1]) + ' columns.')\n",
    "\n",
    "mecaDF = mecaDF.rename(columns={\"CellID\": \"CellName\", \"CellName\": \"CellID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mecaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentalDataFile = 'ExperimentalConditions.csv'\n",
    "experimentalDataFilePath = os.path.join(experimentalDataDir, experimentalDataFile)\n",
    "expConditionsDF = pd.read_csv(experimentalDataFilePath, sep=';',header=1)\n",
    "print('Extracted a table with ' + str(expConditionsDF.shape[0]) + ' lines and ' + str(expConditionsDF.shape[1]) + ' columns.')\n",
    "\n",
    "# Cleaning the table\n",
    "try:\n",
    "    expConditionsDF = expConditionsDF.convert_dtypes()\n",
    "\n",
    "    listTextColumns = []\n",
    "    for col in expConditionsDF.columns:\n",
    "        if expConditionsDF[col].dtype == 'string':\n",
    "            listTextColumns.append(col)\n",
    "\n",
    "    expConditionsDF[listTextColumns] = expConditionsDF[listTextColumns].apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "    expConditionsDF['scale pixel per um'] = expConditionsDF['scale pixel per um'].astype(float)\n",
    "    expConditionsDF['optical index correction'] = \\\n",
    "              expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[0]).astype(float) \\\n",
    "            / expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[1]).astype(float)\n",
    "    expConditionsDF['magnetic field correction'] = expConditionsDF['magnetic field correction'].astype(float)\n",
    "    expConditionsDF['with fluo images'] = expConditionsDF['with fluo images'].astype(bool)\n",
    "\n",
    "    expConditionsDF['ramp field'] = \\\n",
    "    expConditionsDF['ramp field'].apply(lambda x: [x.split(';')[0], x.split(';')[1]] if not pd.isnull(x) else [])\n",
    "\n",
    "except:\n",
    "    print('Unexpected bug with the cleaning step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# expConditionsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused for now\n",
    "cellDescriptionDataFile = 'CellDescription.csv'\n",
    "cellDescriptionDataFilePath = os.path.join(experimentalDataDir, cellDescriptionDataFile)\n",
    "cellDescriptionDF = pd.read_csv(cellDescriptionDataFilePath, ',')\n",
    "print('Extracted a table with ' + str(cellDescriptionDF.shape[0]) + ' lines and ' + str(cellDescriptionDF.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecaDF['ManipID'] = mecaDF['ExpDay'] + '_' + mecaDF['CellName'].apply(lambda x: x.split('_')[0])\n",
    "expConditionsDF['ManipID'] = expConditionsDF['date'] + '_' + expConditionsDF['manip']\n",
    "\n",
    "mainMecaDF = pd.merge(\n",
    "    expConditionsDF,\n",
    "    mecaDF,\n",
    "    how=\"inner\",\n",
    "    on='ManipID',\n",
    "#     left_on=None,\n",
    "#     right_on=None,\n",
    "#     left_index=False,\n",
    "#     right_index=False,\n",
    "#     sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),\n",
    "#     copy=True,\n",
    "#     indicator=False,\n",
    "#     validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mainMecaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mainMecaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.reset_option('max_columns')\n",
    "# mainMecaDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainMecaDF_f = mainMecaDF.loc[(mainMecaDF[\"Validated\"] == 1)]\n",
    "# mainMecaDF_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "listCells = mainMecaDF_f['CellID'].drop_duplicates().astype('string').values\n",
    "timeSeriesDict = {}\n",
    "for cell in listCells:\n",
    "    currentCell_TimeSeriesData = getCellTimeSeriesData(cell)\n",
    "    timeSeriesDict[cell] = currentCell_TimeSeriesData\n",
    "start, stop = 80, 100\n",
    "fig, axes = plt.subplots((stop-start),1, figsize = (7,4*(stop-start)))\n",
    "fig.tight_layout()\n",
    "for k in range(start, stop):\n",
    "    if k < len(listCells):\n",
    "        currentCell_TimeSeriesData = timeSeriesDict[listCells[k]]\n",
    "        T = currentCell_TimeSeriesData['T'].values\n",
    "        idxCompression = currentCell_TimeSeriesData['idxCompression'].values\n",
    "        D3 = currentCell_TimeSeriesData['D3'].values\n",
    "        maskConstant = (idxCompression == 0)\n",
    "        maskCompression = (idxCompression > 0)\n",
    "        axes[k - start].plot(T, D3*1000-4503, 'k-', linewidth = 0.5)\n",
    "        axes[k - start].plot(T[maskCompression], D3[maskCompression]*1000-4503, 'ro', markersize=2)\n",
    "        axes[k - start].plot(T[maskConstant], D3[maskConstant]*1000-4503, 'co', markersize=2)\n",
    "        axes[k - start].set_title(listCells[k])\n",
    "        axes[k - start].set_xlabel('T (s)')\n",
    "        axes[k - start].set_ylabel('D3 (µm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addExcludedCell('21-01-18_M1_P1_C2', 'passive')\n",
    "addExcludedCell('21-01-18_M1_P1_C3', 'passive')\n",
    "addExcludedCell('21-01-18_M1_P1_C4', 'passive')\n",
    "addExcludedCell('21-01-21_M3_P1_C4', 'passive')\n",
    "addExcludedCell('21-01-21_M3_P1_C5', 'passive')\n",
    "addExcludedCell('20-08-07_M1_P1_C6', 'too thick')\n",
    "addExcludedCell('20-08-07_M1_P1_C62', 'too thick')\n",
    "\n",
    "excludedCellsDict = getExcludedCells()\n",
    "# # excludedMask = (mainMecaDF_f[\"CellID\"].values not in excludedCellsDict.keys())\n",
    "# # mainMecaDF_f = mainMecaDF_f.loc[(mainMecaDF_f[\"CellID\"].values not in excludedCellsDict.keys())]\n",
    "# for i in range(len(excludedCellsDict)):\n",
    "#     print('a')\n",
    "# mainMecaDF_f[\"CellID\"].drop_duplicates().astype('string').values\n",
    "excludedCellsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# currentCell_TimeSeriesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainMecaDF_GroupedPerCell = mainMecaDF_f.groupby('CellID')\n",
    "mainMecaDF_DataPerCell = mainMecaDF_GroupedPerCell.agg({\"EChadwick\": np.median, \"SurroundingThickness\": np.median, \"H0Chadwick\" : np.median})\n",
    "# mainMecaDF_GroupedPerCell.agg({\"EChadwick\": np.median, \"D\": lambda x: np.std(x, ddof=1)})\n",
    "cols = ['date', 'manip', 'experimentType', 'drug', 'substrate',\n",
    "       'objective magnification', 'scale pixel per um', 'objective immersion',\n",
    "       'optical index correction', 'magnetic field correction', 'cell type',\n",
    "       'cell subtype', 'bead type', 'bead diameter', 'normal field',\n",
    "       'ramp field', 'compression duration', 'with fluo images', 'comments',\n",
    "       'ManipID', 'ExpType', 'CellName', 'CellID']\n",
    "mainMecaDF_DataPerCell.dropna(inplace = True)\n",
    "mainMecaDF_DataPerCell = pd.merge(mainMecaDF_DataPerCell,\n",
    "                                  mainMecaDF_f[cols].drop_duplicates(subset=['CellID']),\n",
    "                                  how=\"inner\",\n",
    "                                  on='CellID',\n",
    "                                  #     left_on='CellID',\n",
    "                                  #     right_on='CellID',\n",
    "                                  #     left_index=False,\n",
    "                                  #     right_index=False,\n",
    "                                  #     sort=True,\n",
    "                                  #     suffixes=(\"_x\", \"_y\"),\n",
    "                                  #     copy=True,\n",
    "                                  #     indicator=False,\n",
    "                                  #     validate=None,\n",
    "                                  )\n",
    "# mainMecaDF_DataPerCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainMecaDF_DataPerCell_Count = mainMecaDF_DataPerCell.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "mainMecaDF_DataPerCell_Count.loc[:, ['CellID']].rename(columns={'CellID' : 'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Styles = {''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D1Plot(data, CondCol=[],Parameters=[],Filters=[],Boxplot=True,AvgPerCell=False,cellID='cellID'):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "    NCond = len(CondCol)    \n",
    "    if NCond == 1:\n",
    "        CondCol = CondCol[0]\n",
    "    elif NCond > 1:\n",
    "        newColName = ''\n",
    "        for i in range(NCond):\n",
    "            newColName += CondCol[i]\n",
    "            newColName += ' & '\n",
    "        newColName = newColName[:-3]\n",
    "        data_filtered[newColName] = ''\n",
    "        for i in range(NCond):\n",
    "            data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "            data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "        CondCol = newColName\n",
    "    \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean)\n",
    "        \n",
    "    data_filtered.sort_values(CondCol, axis=0, ascending=False, inplace=True)\n",
    "    \n",
    "    NPlots = len(Parameters)\n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    fig, ax = plt.subplots(1, NPlots, figsize = (4*NPlots*NCond,5))\n",
    "    markerSize = 5\n",
    "    if NPlots > 1:\n",
    "        for k in range(NPlots):\n",
    "#             vals = []\n",
    "#             for i in range(len(Conditions)):\n",
    "#                 vals.append(data_filtered[data_filtered[CondCol] == Conditions[i]][Parameters[k]])\n",
    "            sns.swarmplot(x=CondCol, y=Parameters[k], data=data_filtered, ax=ax[k], \n",
    "                          size=markerSize, edgecolor='k',linewidth = 1)\n",
    "            if Boxplot:\n",
    "                sns.boxplot(x=CondCol, y=Parameters[k], data=data_filtered, ax=ax[k], \n",
    "                            color='w', linewidth = 2, width = 0.5, showfliers = False)\n",
    "#                 data_filtered.boxplot(column=Parameters[k], by = CondCol, ax=ax[k],\n",
    "#                             showfliers = False) # linewidth = 2, width = 0.5, \n",
    "            ax[k].set_ylabel(Parameters[k])\n",
    "            ax[k].tick_params(axis='x', labelrotation = 10)\n",
    "    else:\n",
    "#         vals = []\n",
    "#         for i in range(len(Conditions)):\n",
    "#             vals.append(data_filtered[data_filtered[CondCol] == Conditions[i]][Parameters[0]])\n",
    "        sns.swarmplot(x=CondCol, y=Parameters[0], data=data_filtered, ax=ax, \n",
    "                          size=markerSize, edgecolor='k',linewidth = 0.5)\n",
    "        if Boxplot:\n",
    "            sns.boxplot(x=CondCol, y=Parameters[0], data=data_filtered, ax=ax, \n",
    "                        color='w', linewidth = 2, width = 0.5, showfliers = False)\n",
    "        ax.tick_params(axis='x', labelrotation = 25)\n",
    "    return(fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D1PlotInteractive(data, CondCol='',Parameters=[],Filters=[],AvgPerCell=False,cellID='cellID'):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "#     print(data_filtered[cellID])\n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "    \n",
    "#     return(data_filtered)\n",
    "    \n",
    "    NPlots = len(Parameters)\n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    if NPlots > 1:\n",
    "        plots = []\n",
    "        NCond = len(Conditions)\n",
    "        data_filtered['X'] = 0\n",
    "        data_filtered['X_jitter'] = 0.\n",
    "        dictTicks = {}\n",
    "        for i in range(NCond):\n",
    "            mask = data_filtered[CondCol] == Conditions[i]\n",
    "            data_filtered.loc[mask, 'X'] = i+1\n",
    "            dictTicks[i+1] = Conditions[i]\n",
    "        for i in data_filtered.index:\n",
    "            data_filtered.loc[i, 'X_jitter'] = data_filtered.loc[i, 'X'] + 0.4*(np.random.rand(1)[0]-0.5)\n",
    "        source = ColumnDataSource(\n",
    "            data=data_filtered[[cellID]+[CondCol]+Parameters+['X','X_jitter']]\n",
    "        )        \n",
    "        \n",
    "        for k in range(NPlots):\n",
    "            hover = HoverTool(\n",
    "                tooltips=[\n",
    "                    ('Cell ID', \"@\"+cellID),\n",
    "                    (Parameters[k], \"@\"+Parameters[k]),\n",
    "                ]\n",
    "            )\n",
    "            index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "            p = figure(plot_width=450, plot_height=500, tools=[hover], title=\"InteractivePlot\") # \n",
    "            p.circle('X_jitter', Parameters[k], size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "            # Format\n",
    "            p.x_range = Range1d(0, NCond+1)\n",
    "            p.y_range = Range1d(min(0,1.1*np.min(data_filtered[Parameters[0]])), 1.1*np.max(data_filtered[Parameters[k]]))\n",
    "            p.xaxis.ticker = [i for i in range(1,NCond+1)]\n",
    "            p.xaxis.major_label_overrides = dictTicks\n",
    "            p.xaxis.axis_label = CondCol\n",
    "            p.xaxis.axis_label_text_font_size = '18pt'\n",
    "            p.xaxis.major_label_text_font_size = '16pt'\n",
    "            p.yaxis.axis_label = Parameters[k]\n",
    "            p.yaxis.axis_label_text_font_size = '18pt'\n",
    "            p.yaxis.major_label_text_font_size = '16pt'\n",
    "            \n",
    "            plots.append(p)\n",
    "            \n",
    "        p = gridplot(plots, ncols=2, toolbar_location=None)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                ('Cell ID', \"@\"+cellID),\n",
    "                (Parameters[0], \"@\"+Parameters[0]),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        NCond = len(Conditions)\n",
    "        data_filtered['X'] = 0\n",
    "        data_filtered['X_jitter'] = 0.\n",
    "        dictTicks = {}\n",
    "        for i in range(NCond):\n",
    "            mask = data_filtered[CondCol] == Conditions[i]\n",
    "            data_filtered.loc[mask, 'X'] = i+1\n",
    "            dictTicks[i+1] = Conditions[i]\n",
    "        for i in data_filtered.index:\n",
    "            data_filtered.loc[i, 'X_jitter'] = data_filtered.loc[i, 'X'] + 0.4*(np.random.rand(1)[0]-0.5)\n",
    "        source = ColumnDataSource(\n",
    "            data=data_filtered[[cellID]+[CondCol]+Parameters+['X','X_jitter']]\n",
    "        )\n",
    "        index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "        TOOLS = \"hover,pan,box_zoom,wheel_zoom,reset,save,help\"\n",
    "        p = figure(plot_width=500, plot_height=500, tools=TOOLS, title=\"InteractivePlot\") # \n",
    "        p.circle('X_jitter', Parameters[0], size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "        # Format\n",
    "        p.x_range = Range1d(0, NCond+1)\n",
    "        p.y_range = Range1d(min(0,1.1*np.min(data_filtered[Parameters[0]])), 1.1*np.max(data_filtered[Parameters[0]]))\n",
    "        p.xaxis.ticker = [i for i in range(1,NCond+1)]\n",
    "        p.xaxis.major_label_overrides = dictTicks\n",
    "        p.xaxis.axis_label = CondCol\n",
    "        p.xaxis.axis_label_text_font_size = '18pt'\n",
    "        p.xaxis.major_label_text_font_size = '16pt'\n",
    "        p.yaxis.axis_label = Parameters[0]\n",
    "        p.yaxis.axis_label_text_font_size = '18pt'\n",
    "        p.yaxis.major_label_text_font_size = '16pt'\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data = GlobalTable_meca\n",
    "# CondCol='drug'\n",
    "# Parameters=['SurroundingThickness','EChadwick']\n",
    "# Filters = [(GlobalTable_meca['Validated'] == 1)]\n",
    "# AvgPerCell=True\n",
    "# cellID='CellName'\n",
    "\n",
    "# data_filtered = data\n",
    "# for fltr in Filters:\n",
    "#     data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "# group = data_filtered.groupby(cellID)\n",
    "# dictAggMean = getDictAggMean(data_filtered)\n",
    "# data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "# data_filtered.reset_index(level=0, inplace=True)\n",
    "# data_filtered=data_filtered[[cellID]+[CondCol]+Parameters]\n",
    "# print(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D2Plot(data, XCol='',YCol='',CondCol='',Filters=[],AvgPerCell=False):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "    \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "        \n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8,5))\n",
    "    markerSize = 5\n",
    "    for c in Conditions:\n",
    "        ax.plot(data_filtered[data_filtered[CondCol] == c][XCol], \n",
    "                data_filtered[data_filtered[CondCol] == c][YCol],\n",
    "               'o', markersize = markerSize, markeredgecolor='k',markeredgewidth = 1)\n",
    "    ax.set_xlabel(XCol)\n",
    "#     print(min(0,1.1*np.min(data_filtered[XCol])))\n",
    "    ax.set_xlim([min(0,1.1*np.min(data_filtered[XCol])), 1.1*np.max(data_filtered[XCol])])\n",
    "    ax.set_ylabel(YCol)\n",
    "    ax.set_ylim([min(0,1.1*np.min(data_filtered[YCol])), 1.1*np.max(data_filtered[YCol])])\n",
    "    return(fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D2PlotInteractive(data, XCol='',YCol='',CondCol='',Filters=[], cellID='cellID',AvgPerCell=False):\n",
    "    \n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "\n",
    "    NCond = len(Conditions)\n",
    "    dictTicks = {}\n",
    "    for i in range(NCond):\n",
    "        dictTicks[i+1] = Conditions[i]\n",
    "    \n",
    "    source = ColumnDataSource(\n",
    "        data=data_filtered[[cellID,CondCol,XCol,YCol]]\n",
    "    )\n",
    "    \n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            ('Cell ID', \"@\"+cellID),\n",
    "            (XCol, \"@\"+XCol),\n",
    "            (YCol, \"@\"+YCol),\n",
    "            (CondCol, \"@\"+CondCol),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "    TOOLS = \"pan,box_zoom,wheel_zoom,reset,save,help\"\n",
    "    p = figure(plot_width=900, plot_height=500, tools=TOOLS, title=\"InteractivePlot\",toolbar_location=\"below\") # \n",
    "    p.circle(XCol, YCol, size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "    p.add_tools(hover)\n",
    "    # Format\n",
    "    p.x_range = Range1d(0, 1.1*np.max(data_filtered[XCol]))\n",
    "    p.y_range = Range1d(0, 1.1*np.max(data_filtered[YCol]))\n",
    "    p.xaxis.axis_label = XCol\n",
    "    p.xaxis.axis_label_text_font_size = '18pt'\n",
    "    p.xaxis.major_label_text_font_size = '16pt'\n",
    "    p.yaxis.axis_label = YCol\n",
    "    p.yaxis.axis_label_text_font_size = '18pt'\n",
    "    p.yaxis.major_label_text_font_size = '16pt'\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "# pd.reset_option('max_columns')\n",
    "pd.set_option('max_rows', None)\n",
    "# pd.reset_option('max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_ctField().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFluoData().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_ctField = getGlobalTable_ctField()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_ctField = pd.merge(table_ExpConditions, GlobalTable_ctField, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_ctField = pd.merge(GlobalTable_ctField, table_fluo, how=\"left\", on='cellID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_ctField.shape[0]) + ' lines and ' + str(GlobalTable_ctField.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_ctField.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_meca = getGlobalTable_meca()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_meca = pd.merge(table_ExpConditions, GlobalTable_meca, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_fluo, how=\"left\", left_on='CellName', right_on='cellID'\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_meca.shape[0]) + ' lines and ' + str(GlobalTable_meca.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_meca.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True)]\n",
    "fig, ax = D1Plot(GlobalTable_ctField, CondCol=['drug','substrate'],Parameters=['medianThickness','fluctuAmpli'],Filters=Filters)\n",
    "fig.suptitle('3T3aSFL on patterns: Ct Field')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['drug','substrate'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Ct Field')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['drug','substrate'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('3T3aSFL on patterns: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "\n",
    "p = D1PlotInteractive(GlobalTable_ctField, CondCol='drug',Parameters=['medianThickness','fluctuAmpli'],Filters=Filters)\n",
    "p.children[0][0].title.text = '3T3aSFL on patterns: Ct Field'\n",
    "p.children[0][0].title.text_font_size = '16pt'\n",
    "p.children[1][0].title.text = ''\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D1PlotInteractive(GlobalTable_meca, CondCol='drug',Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "p.children[0][0].title.text = '3T3aSFL on patterns: Compressions'\n",
    "p.children[0][0].title.text_font_size = '14pt'\n",
    "p.children[1][0].title.text = ''\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = 'drug', Filters=Filters)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = 'drug', Filters=Filters)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='SurroundingThickness',YCol='EChadwick',CondCol = 'drug', Filters=Filters, cellID = 'CellName')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True)]\n",
    "p = D2PlotInteractive(GlobalTable_ctField, XCol='meanFluoPeakAmplitude',YCol='medianThickness',CondCol = 'drug', Filters=Filters, cellID = 'cellID',AvgPerCell=True)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True)]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = '3T3aSFL expressing linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL expressing linker: E(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL-6FP expressing long linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL-6FP expressing long linker: E(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='SurroundingThickness', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "P.title.text = 'aSFL expressing linker: E(fluo)'\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1,2, figsize = (8, 5))\n",
    "# axes[0].plot(np.ones(len(resDict['nodrug']['median'])), resDict['nodrug']['median'], 'co')\n",
    "# axes[0].plot(2*np.ones(len(resDict['doxy']['median'])), resDict['doxy']['median'], 'ro')\n",
    "# axes[0].set_xlim(0.5, 2.5)\n",
    "# axes[0].set_ylabel('Median Thickness (nm)')\n",
    "# axes[0].set_xticks([1,2])\n",
    "# axes[0].set_xticklabels(['Control','Doxycylin'])\n",
    "# axes[1].plot(np.ones(len(resDict['nodrug']['fluctu'])), resDict['nodrug']['fluctu'], 'co')\n",
    "# axes[1].plot(2*np.ones(len(resDict['doxy']['fluctu'])), resDict['doxy']['fluctu'], 'ro')\n",
    "# axes[1].set_xlim(0.5, 2.5)\n",
    "# axes[1].set_ylabel('Thickness Fluctuations (nm)')\n",
    "# axes[1].set_xticks([1,2])\n",
    "# axes[1].set_xticklabels(['Control','Doxycylin'])\n",
    "# fig.savefig(\"C://Users//JosephVermeil//Desktop//ActinCortexAnalysis//DataAnalysis//constantField.png\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Old code\n",
    "conditions = ['nodrug', 'doxy']\n",
    "correspondance = {conditions[0] : 'M1', conditions[1] : 'M2'}\n",
    "allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".txt\"))]\n",
    "dates = ['21-02-10']\n",
    "resDict = {conditions[0] : {}, conditions[1] : {}}\n",
    "for C in conditions:\n",
    "    resDict[C]['accepted'] = []\n",
    "    resDict[C]['rejected'] = []\n",
    "    resDict[C]['median'] = []\n",
    "    resDict[C]['fluctu'] = []\n",
    "    for D in dates:\n",
    "        for f in allTimeSeriesDataFiles:\n",
    "            if correspondance[C] in f and D in f:\n",
    "                split_f = f.split('_')\n",
    "                cellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "                currentCellTS = getCellTimeSeriesData(cellID)\n",
    "                D3 = currentCellTS.D3.values\n",
    "                decile_1 = np.percentile(D3, 10)\n",
    "                median = np.median(D3)\n",
    "                decile_9 = np.percentile(D3, 90)\n",
    "                if decile_1 < 0:\n",
    "                    resDict[C]['rejected'].append(cellID)\n",
    "                else:\n",
    "                    resDict[C]['accepted'].append(cellID)\n",
    "                    resDict[C]['median'].append(median)\n",
    "                    resDict[C]['fluctu'].append(decile_9-decile_1)\n",
    "#resDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GlobalTable_ctField = getGlobalTable_ctField()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "GlobalTable_ctField = pd.merge(table_ExpConditions, GlobalTable_ctField, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "\n",
    "# data=pd.DataFrame(dict(\n",
    "#             x=[1, 2, 3, 4, 5],\n",
    "#             y=[2, 5, 8, 2, 7],\n",
    "#             desc=['A', 'A', 'C', 'd', 'E'],\n",
    "#         ))\n",
    "\n",
    "data = GlobalTable_ctField[['medianThickness','fluctuAmpli','cellID']]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "# hover = HoverTool(\n",
    "#         tooltips=[\n",
    "#             (\"index\", \"$index\"),\n",
    "#             (\"(x,y)\", \"($x, $y)\"),\n",
    "#             (\"desc\", \"@desc\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"medianThickness\", \"@medianThickness\"),\n",
    "            (\"fluctuAmpli\", \"@fluctuAmpli\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(plot_width=300, plot_height=300, tools=[hover], title=\"Mouse over the dots\")\n",
    "\n",
    "p.circle('medianThickness', 'fluctuAmpli', size=20, source=data)\n",
    "\n",
    "show(p)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(dict(\n",
    "            x=[1, 2, 3, 4, 5],\n",
    "            y=[2, 5, 8, 2, 7],\n",
    "            desc=['A', 'A', 'C', 'd', 'E'],\n",
    "        ))\n",
    "\n",
    "Conditions = list(data['desc'].unique())\n",
    "NCond = len(Conditions)\n",
    "data['X'] = 0\n",
    "for i in range(NCond):\n",
    "    mask = data['desc'] == Conditions[i]\n",
    "    data.loc[mask, ['X']] = i+1\n",
    "data.index = data.x\n",
    "data = data.drop(['x'], axis = 1)\n",
    "\n",
    "data.reset_index(level=0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.549px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
