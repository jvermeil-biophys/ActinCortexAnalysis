{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from copy import copy\n",
    "from cycler import cycler\n",
    "from datetime import date\n",
    "from scipy.optimize import curve_fit\n",
    "from statannot import add_stat_annotation\n",
    "\n",
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dateFormatExcel = re.compile('\\d{2}/\\d{2}/\\d{4}')\n",
    "dateFormatOk = re.compile('\\d{2}-\\d{2}-\\d{2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget \n",
    "# %matplotlib inline\n",
    "matplotlib.rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "\n",
    "SMALLER_SIZE = 10\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALLER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "# colors = prop_cycle.by_key()['color']\n",
    "new_color_list = color=['#ff7f0e', '#1f77b4', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "new_color_cycle = cycler(color=new_color_list)\n",
    "plt.rcParams['axes.prop_cycle'] = new_color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Range1d\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.layouts import gridplot\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mainDir = \"C://Users//JosephVermeil//Desktop//ActinCortexAnalysis\"\n",
    "# mainDir = \"C://Users//josep//Desktop//ActinCortexAnalysis\"\n",
    "experimentalDataDir = os.path.join(mainDir, \"ExperimentalData\")\n",
    "dataDir = os.path.join(mainDir, \"DataAnalysis\")\n",
    "figDir = os.path.join(dataDir, \"Figures\")\n",
    "todayFigDir = os.path.join(figDir, \"Historique//\" + str(date.today()))\n",
    "timeSeriesDataDir = os.path.join(dataDir, \"TimeSeriesData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Display all detected time Series Data Files.\n",
    "\n",
    "# allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\"))]\n",
    "# allTimeSeriesDataFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some generic useful subfunctions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     26,
     30,
     54
    ]
   },
   "outputs": [],
   "source": [
    "def get_R2(Y1, Y2):\n",
    "    meanY = np.mean(Y1)\n",
    "    meanYarray = meanY*np.ones(len(Y1))\n",
    "    SST = np.sum((Y1-meanYarray)**2)\n",
    "    SSE = np.sum((Y2-meanYarray)**2)\n",
    "    R2 = SSE/SST\n",
    "    return(R2)\n",
    "\n",
    "def getDictAggMean(df):\n",
    "    dictAggMean = {}\n",
    "    for c in df.columns:\n",
    "#         t = df[c].dtype\n",
    "#         print(c, t)\n",
    "        try :\n",
    "            if np.array_equal(df[c], df[c].astype(bool)):\n",
    "                dictAggMean[c] = 'min'\n",
    "            else:\n",
    "                try:\n",
    "                    np.mean(df[c])\n",
    "                    dictAggMean[c] = 'mean'\n",
    "                except:\n",
    "                    dictAggMean[c] = 'first'\n",
    "        except:\n",
    "                dictAggMean[c] = 'first'\n",
    "    return(dictAggMean)\n",
    "\n",
    "def findFirst(x, A):\n",
    "    idx = (A==x).view(bool).argmax()\n",
    "    return(idx)\n",
    "\n",
    "def fitLine(X, Y):\n",
    "    X = sm.add_constant(X)\n",
    "    model = sm.OLS(Y, X)\n",
    "    results = model.fit()\n",
    "    params = results.params # Y=a*X+b ; params[0] = b,  params[1] = a\n",
    "#     print(dir(results))\n",
    "#     R2 = results.rsquared\n",
    "#     ci = results.conf_int(alpha=0.05)\n",
    "#     CovM = results.cov_params()\n",
    "#     p = results.pvalues\n",
    "\n",
    "# This is how are computed conf_int:\n",
    "#\n",
    "#     bse = results.bse\n",
    "#     dist = stats.t\n",
    "#     alpha = 0.05\n",
    "#     q = dist.ppf(1 - alpha / 2, results.df_resid)\n",
    "#     params = results.params\n",
    "#     lower = params - q * bse\n",
    "#     upper = params + q * bse\n",
    "#     print(lower, upper)\n",
    "    \n",
    "    return(results.params, results)\n",
    "\n",
    "def archiveFig(fig, ax, name='auto', figDir = todayFigDir, figSubDir=''):\n",
    "    if not os.path.exists(figDir):\n",
    "        os.makedirs(figDir)\n",
    "    \n",
    "    saveDir = os.path.join(todayFigDir, figSubDir)\n",
    "    if not os.path.exists(saveDir):\n",
    "        os.makedirs(saveDir)\n",
    "    \n",
    "    if name != 'auto':\n",
    "        fig.savefig(os.path.join(saveDir, name + '.png'))\n",
    "    \n",
    "    else:\n",
    "        suptitle = fig._suptitle.get_text()\n",
    "        if len(suptitle) > 0:\n",
    "            name = suptitle\n",
    "            fig.savefig(os.path.join(saveDir, name + '.png'))\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                N = len(ax)\n",
    "                ax = ax[0]\n",
    "            except:\n",
    "                N = 1\n",
    "                ax = ax\n",
    "                \n",
    "            xlabel = ax.get_xlabel()\n",
    "            ylabel = ax.get_ylabel()\n",
    "            if len(xlabel) > 0 and len(ylabel) > 0:\n",
    "                name = ylabel + ' Vs ' + xlabel\n",
    "                if N > 1:\n",
    "                    name = name + '___etc'\n",
    "                fig.savefig(os.path.join(saveDir, name + '.png'))\n",
    "            \n",
    "            else:\n",
    "                title = ax.get_title()\n",
    "                if len(title) > 0:\n",
    "                    if N > 1:\n",
    "                        name = name + '___etc'\n",
    "                    fig.savefig(os.path.join(saveDir, name + '.png'))\n",
    "                \n",
    "                else:\n",
    "                    figNum = gcf().number\n",
    "                    name = 'figure ' + str(figNum) \n",
    "                    fig.savefig(os.path.join(saveDir, name + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: How to compute confidence invetervals of fitted parameters with (1-alpha) confidence:\n",
    "\n",
    "    0) from scipy import stats\n",
    "    1) df = nb_pts - nb_parms ; se = diag(cov)**0.5\n",
    "    2) Student t coefficient : q = stat.t.ppf(1 - alpha / 2, df)\n",
    "    3) ConfInt = [params - q*se, params + q*se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of the fitLine function\n",
    "\n",
    "# Npts = 10\n",
    "# seed = 10\n",
    "# std = 2\n",
    "\n",
    "# np.random.seed(seed)\n",
    "# X = np.array([i for i in range(Npts)])\n",
    "# Y = np.array([i for i in range(Npts)])\n",
    "# Y = Y + np.random.normal(0, std, Npts)\n",
    "# p, R = fitLine(X, Y)\n",
    "\n",
    "# Ypred = p[1]*X+p[0]\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# ax.plot(X, Y, 'bo')\n",
    "# ax.plot(X, Ypred, 'r-', label='best fit')\n",
    "# ax.legend(loc='best')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of the lmfit library // <examples/doc_model_gaussian.py>\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from lmfit import Model\n",
    "\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# x = np.array([i for i in range(Npts)])\n",
    "# y = np.array([i for i in range(Npts)])\n",
    "# y = y + np.random.normal(0, std, Npts)\n",
    "\n",
    "\n",
    "# # def gaussian(x, amp, cen, wid):\n",
    "# #     \"\"\"1-d gaussian: gaussian(x, amp, cen, wid)\"\"\"\n",
    "# #     return (amp / (sqrt(2*pi) * wid)) * exp(-(x-cen)**2 / (2*wid**2))\n",
    "\n",
    "# def linear(x, a, b):\n",
    "#     \"\"\"linear function\"\"\"\n",
    "#     return (a*x + b)\n",
    "\n",
    "# # gmodel = Model(gaussian)\n",
    "# # result = gmodel.fit(y, x=x, amp=5, cen=5, wid=1)\n",
    "\n",
    "# # print(result.fit_report())\n",
    "\n",
    "# # plt.plot(x, y, 'bo')\n",
    "# # plt.plot(x, result.init_fit, 'k--', label='initial fit')\n",
    "# # plt.plot(x, result.best_fit, 'r-', label='best fit')\n",
    "# # plt.legend(loc='best')\n",
    "# # plt.show()\n",
    "\n",
    "# lmodel = Model(linear)\n",
    "# result = lmodel.fit(y, x=x, a=1.5, b=-1)\n",
    "\n",
    "# print(result.fit_report())\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# ax.plot(x, y, 'bo')\n",
    "# # ax.plot(x, result.init_fit, 'k--', label='initial fit')\n",
    "# ax.plot(x, result.best_fit, 'r-', label='best fit')\n",
    "# ax.legend(loc='best')\n",
    "# fig.show()\n",
    "\n",
    "# # print(dir(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test of the curve_fit function and subsequent computation of confidence intervals\n",
    "\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# x = np.array([i for i in range(Npts)])\n",
    "# y = np.array([i for i in range(Npts)])\n",
    "# y = y + np.random.normal(0, std, Npts)\n",
    "\n",
    "# def linear(x, a, b):\n",
    "#     \"\"\"linear function\"\"\"\n",
    "#     return (a*x + b)\n",
    "\n",
    "\n",
    "# initialParameters = [1.5, -1]\n",
    "# #     print(initialParameters)\n",
    "\n",
    "# # bounds on parameters - initial parameters must be within these\n",
    "# lowerBounds = (-np.Inf, -np.Inf)\n",
    "# upperBounds = (np.Inf, np.Inf)\n",
    "# parameterBounds = [lowerBounds, upperBounds]\n",
    "\n",
    "# # fittedParameters, pcov = curve_fit(chadwickModel, hCompr, fCompr, initialParameters, bounds = parameterBounds)\n",
    "# fittedParameters, pcov = curve_fit(linear, x, y, initialParameters, bounds = parameterBounds)\n",
    "\n",
    "# A, B = fittedParameters\n",
    "# print(A, B)\n",
    "# ypredict = linear(x, A, B)\n",
    "\n",
    "# varA = pcov[0,0]\n",
    "# print(pcov)\n",
    "# print(pcov[0,0]**0.5)\n",
    "# SSR = np.sum((np.array(y)-np.array(ypredict))**2)\n",
    "# seA = ((SSR/(len(x)-2))*varA)**0.5\n",
    "# seA = (varA)**0.5\n",
    "# confIntA = st.t.interval(alpha=0.95, df=len(x)-2, loc=A, scale=seA)\n",
    "# confIntAHalfWidthBis = st.t.ppf(0.975, len(x)-2) * seA\n",
    "# print(confIntAHalfWidthBis)\n",
    "# # confIntA = (A + seA*st.t.interval(alpha=0.95, df=len(x), loc=0, scale=1)[0], A + seA*st.t.interval(alpha=0.95, df=len(x), loc=0, scale=1)[1])\n",
    "# confIntAWidth = confIntA[1] - confIntA[0]\n",
    "# confIntAHalfWidth = confIntAWidth/2\n",
    "# R2 = get_R2(y,ypredict)\n",
    "\n",
    "# print(str(A) + ' +/- ' + str(confIntAHalfWidth))\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# ax.plot(x, y, 'bo')\n",
    "# ax.plot(x, ypredict, 'r-', label='best fit')\n",
    "# ax.legend(loc='best')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of the get_R2 function.\n",
    "# T = df['T']\n",
    "# Y = df['D3']\n",
    "# plt.plot(T,Y)\n",
    "# p, residuals, rank, singular_values, rcond = np.polyfit(T, Y, deg=5, full=True)\n",
    "# plt.plot(T, Y)\n",
    "# Y2 = np.zeros(len(T))\n",
    "# for i in range(len(T)):\n",
    "#     deg = len(p)-1\n",
    "#     for k in range(deg+1):\n",
    "#         Y2[i] += p[k]*(T[i]**(deg-k))\n",
    "# plt.plot(T,Y2)\n",
    "# get_R2(Y, Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSeries functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16,
     35,
     56
    ]
   },
   "outputs": [],
   "source": [
    "def getCellTimeSeriesData(cellID):\n",
    "    allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\"))]\n",
    "    fileFound = False\n",
    "    nFile = len(allTimeSeriesDataFiles)\n",
    "    iFile = 0\n",
    "    while (not fileFound) and (iFile < nFile):\n",
    "        f = allTimeSeriesDataFiles[iFile]\n",
    "        if f.startswith(cellID):\n",
    "            timeSeriesDataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "            timeSeriesDataFrame = pd.read_csv(timeSeriesDataFilePath, sep=';')\n",
    "            fileFound = True\n",
    "        iFile += 1\n",
    "    if not fileFound:\n",
    "        timeSeriesDataFrame = pd.DataFrame([])\n",
    "    return(timeSeriesDataFrame)\n",
    "\n",
    "def plotCellTimeSeriesData(cellID):\n",
    "    X = 'T'\n",
    "    Y = np.array(['B', 'F', 'dx', 'dy', 'dz', 'D2', 'D3'])\n",
    "    units = np.array([' (mT)', ' (pN)', ' (µm)', ' (µm)', ' (µm)', ' (µm)', ' (µm)'])\n",
    "    timeSeriesDataFrame = getCellTimeSeriesData(cellID)\n",
    "    if not timeSeriesDataFrame.size == 0:\n",
    "#         plt.tight_layout()\n",
    "#         fig.show() # figsize=(20,20)\n",
    "        axes = timeSeriesDataFrame.plot(x=X, y=Y, kind='line', ax=None, subplots=True, sharex=True, sharey=False, layout=None, \\\n",
    "                       figsize=(8,10), use_index=True, title = cellID + '- Time dependant data', grid=None, legend=False, style=None, logx=False, logy=False, \\\n",
    "                       loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, \\\n",
    "                       table=False, yerr=None, xerr=None, secondary_y=False, sort_columns=False)\n",
    "        plt.gcf().tight_layout()\n",
    "        for i in range(len(Y)):\n",
    "            axes[i].set_ylabel(Y[i] + units[i])\n",
    "        \n",
    "    else:\n",
    "        print('cell not found')\n",
    "        \n",
    "def addExcludedCell(cellID, motive):\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    nLines = len(lines)\n",
    "    excludedCellsList = []\n",
    "    for iLine in range(nLines):\n",
    "        line = lines[iLine]\n",
    "        splitLine = line[:-1].split(',')\n",
    "        excludedCellsList.append(splitLine[0])\n",
    "    if cellID in excludedCellsList:\n",
    "        newlines = copy(lines)\n",
    "        iLineOfInterest = excludedCellsList.index(cellID)\n",
    "        if motive not in newlines[iLineOfInterest][:-1].split(','):\n",
    "            newlines[iLineOfInterest] = newlines[iLineOfInterest][:-1] + ',' + motive + '\\n'            \n",
    "    else:\n",
    "        newlines = copy(lines)\n",
    "        newlines.append('' + cellID + ',' + motive + '\\n')\n",
    "    f.close()\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'w')\n",
    "    f.writelines(newlines)\n",
    "    \n",
    "def getExcludedCells():\n",
    "    f = open(os.path.join(experimentalDataDir, 'ExcludedCells.txt'), 'r')\n",
    "    lines = f.readlines()\n",
    "    nLines = len(lines)\n",
    "    excludedCellsDict = {}\n",
    "    for iLine in range(nLines):\n",
    "        line = lines[iLine]\n",
    "        splitLine = line[:-1].split(',')\n",
    "        excludedCellsDict[splitLine[0]] = splitLine[1:]\n",
    "    return(excludedCellsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = getCellTimeSeriesData('21-02-10_M1_P1_C1')\n",
    "# plotCellTimeSeriesData('21-01-21_M1_P1_C11')\n",
    "# plotCellTimeSeriesData('21-04-21_M1_P1_C5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GlobalTables functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getExperimentalConditions(save = False):\n",
    "    # Getting the table\n",
    "    experimentalDataFile = 'ExperimentalConditions.csv'\n",
    "    experimentalDataFilePath = os.path.join(experimentalDataDir, experimentalDataFile)\n",
    "    expConditionsDF = pd.read_csv(experimentalDataFilePath, sep=';',header=0)\n",
    "    print('Extracted a table with ' + str(expConditionsDF.shape[0]) + ' lines and ' + str(expConditionsDF.shape[1]) + ' columns.')\n",
    "    \n",
    "    # Cleaning the table\n",
    "    try:\n",
    "        for c in expConditionsDF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                expConditionsDF = expConditionsDF.drop([c], axis=1)\n",
    "        expConditionsDF = expConditionsDF.convert_dtypes()\n",
    "\n",
    "        listTextColumns = []\n",
    "        for col in expConditionsDF.columns:\n",
    "            try:\n",
    "                if expConditionsDF[col].dtype == 'string':\n",
    "                    listTextColumns.append(col)\n",
    "            except:\n",
    "                aaaa=0\n",
    "                #Ok\n",
    "\n",
    "        expConditionsDF[listTextColumns] = expConditionsDF[listTextColumns].apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "        expConditionsDF['scale pixel per um'] = expConditionsDF['scale pixel per um'].astype(float)\n",
    "        try:\n",
    "            expConditionsDF['optical index correction'] = \\\n",
    "                      expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[0]).astype(float) \\\n",
    "                    / expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[1]).astype(float)\n",
    "        except:\n",
    "            print('optical index correction already in ' + str(expConditionsDF['optical index correction'].dtype) + ' type.')\n",
    "\n",
    "        expConditionsDF['magnetic field correction'] = expConditionsDF['magnetic field correction'].astype(float)\n",
    "        expConditionsDF['with fluo images'] = expConditionsDF['with fluo images'].astype(bool)\n",
    "\n",
    "        try:\n",
    "            expConditionsDF['ramp field'] = \\\n",
    "            expConditionsDF['ramp field'].apply(lambda x: [x.split(';')[0], x.split(';')[1]] if not pd.isnull(x) else [])\n",
    "        except:\n",
    "            aaaa=0\n",
    "            #Ok\n",
    "\n",
    "        dateExemple = expConditionsDF.loc[expConditionsDF.index[1],'date']\n",
    "\n",
    "        if re.match(dateFormatExcel, dateExemple):\n",
    "            print('dates corrected')\n",
    "            expConditionsDF.loc[1:,'date'] = expConditionsDF.loc[1:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])        \n",
    "        \n",
    "    except:\n",
    "        print('Unexpected bug with the cleaning step')\n",
    "\n",
    "    if save:\n",
    "        saveName = 'ExperimentalConditions.csv'\n",
    "        savePath = os.path.join(experimentalDataDir, saveName)\n",
    "        expConditionsDF.to_csv(savePath, sep=';')\n",
    "\n",
    "    expConditionsDF['manipID'] = expConditionsDF['date'] + '_' + expConditionsDF['manip']\n",
    "    \n",
    "    return(expConditionsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getExperimentalConditions(save=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5,
     26,
     47,
     95
    ]
   },
   "outputs": [],
   "source": [
    "listColumnsCtField = ['date','cellName','cellID','manipID',\\\n",
    "                      'duration','medianRawB','medianThickness',\\\n",
    "                      '1stDThickness','9thDThickness','fluctuAmpli',\\\n",
    "                      'R2_polyFit','validated']\n",
    "\n",
    "def analyseTimeSeries_ctField(tsDf):\n",
    "    results = {}\n",
    "    results['duration'] = np.max(tsDf['T'])\n",
    "    results['medianRawB'] = np.median(tsDf.B)\n",
    "    results['medianThickness'] = np.median(tsDf.D3)\n",
    "    results['1stDThickness'] = np.percentile(tsDf.D3, 10)\n",
    "    results['9thDThickness'] = np.percentile(tsDf.D3, 90)\n",
    "    results['fluctuAmpli'] = results['9thDThickness'] - results['1stDThickness']\n",
    "    results['validated'] = (results['1stDThickness'] > 0)\n",
    "    X, Y = tsDf['T'], tsDf['D3']\n",
    "    p, residuals, rank, singular_values, rcond = np.polyfit(X, Y, deg=5, full=True)\n",
    "    Y2 = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        deg = len(p)-1\n",
    "        for k in range(deg+1):\n",
    "            Y2[i] += p[k]*(X[i]**(deg-k))\n",
    "    results['R2_polyFit'] = get_R2(Y, Y2)\n",
    "    return(results)\n",
    "\n",
    "\n",
    "\n",
    "def createDataDict_ctField(list_ctFieldFiles):\n",
    "    tableDict = {}\n",
    "    tableDict['date'], tableDict['cellName'], tableDict['cellID'], tableDict['manipID'] = [], [], [], []\n",
    "    tableDict['duration'], tableDict['medianRawB'], tableDict['medianThickness'] = [], [], []\n",
    "    tableDict['1stDThickness'], tableDict['9thDThickness'], tableDict['fluctuAmpli'] = [], [], []\n",
    "    tableDict['R2_polyFit'], tableDict['validated'] = [], []\n",
    "    for f in list_ctFieldFiles:\n",
    "        split_f = f.split('_')\n",
    "        tableDict['date'].append(split_f[0])\n",
    "        tableDict['cellName'].append(split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['cellID'].append(split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        tableDict['manipID'].append(split_f[0] + '_' + split_f[1])\n",
    "        tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "        current_tsDf = pd.read_csv(tS_DataFilePath, ';')\n",
    "        current_resultDict = analyseTimeSeries_ctField(current_tsDf)\n",
    "        for k in current_resultDict.keys():\n",
    "            tableDict[k].append(current_resultDict[k])\n",
    "    return(tableDict)\n",
    "\n",
    "\n",
    "\n",
    "def computeGlobalTable_ctField(task = 'fromScratch', fileName = 'Global_CtFieldData', save = False):\n",
    "    ctFieldTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                                  if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                                      and ('thickness' in f))]\n",
    "#     print(ctFieldTimeSeriesDataFiles)\n",
    "    if task == 'fromScratch':\n",
    "        # create a dict containing the data\n",
    "        tableDict = createDataDict_ctField(ctFieldTimeSeriesDataFiles) # MAIN SUBFUNCTION\n",
    "        # create the table\n",
    "        CtField_DF = pd.DataFrame(tableDict)\n",
    "        \n",
    "    elif task == 'updateExisting':\n",
    "        # get existing table\n",
    "        try:\n",
    "            savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "            existing_CtField_DF = pd.read_csv(savePath, sep=';')\n",
    "            for c in existing_CtField_DF.columns:\n",
    "                if 'Unnamed' in c:\n",
    "                    existing_CtField_DF = existing_CtField_DF.drop([c], axis=1)\n",
    "        except:\n",
    "            print('No existing table found')\n",
    "        # find which of the time series files are new\n",
    "        new_ctFieldTimeSeriesDataFiles = []\n",
    "        for f in ctFieldTimeSeriesDataFiles:\n",
    "            split_f = f.split('_')\n",
    "            currentCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "            if currentCellID not in existing_CtField_DF.cellID.values:\n",
    "                new_ctFieldTimeSeriesDataFiles.append(f)\n",
    "        new_tableDict = createDataDict_ctField(new_ctFieldTimeSeriesDataFiles) # MAIN SUBFUNCTION\n",
    "        # create the table with new data\n",
    "        new_CtField_DF = pd.DataFrame(new_tableDict)\n",
    "        # fuse the two\n",
    "        new_CtField_DF.index += existing_CtField_DF.shape[0]\n",
    "        CtField_DF = pd.concat([existing_CtField_DF, new_CtField_DF])\n",
    "    \n",
    "    dateExemple = CtField_DF.loc[CtField_DF.index[0],'date']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        CtField_DF.loc[:,'date'] = CtField_DF.loc[:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])\n",
    "    \n",
    "    if save:\n",
    "        saveName = fileName + '.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        CtField_DF.to_csv(savePath, sep=';')\n",
    "        \n",
    "    return(CtField_DF)\n",
    "\n",
    "\n",
    "\n",
    "def getGlobalTable_ctField(fileName = 'Global_CtFieldData'):\n",
    "    try:\n",
    "        savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "        CtField_DF = pd.read_csv(savePath, sep=';')\n",
    "        for c in CtField_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                CtField_DF = CtField_DF.drop([c], axis=1)\n",
    "        print('Extracted a table with ' + str(CtField_DF.shape[0]) + ' lines and ' + str(CtField_DF.shape[1]) + ' columns.')\n",
    "        \n",
    "    except:\n",
    "        print('No existing table found')\n",
    "        \n",
    "    dateExemple = CtField_DF.loc[CtField_DF.index[0],'date']\n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        print('dates corrected')\n",
    "        CtField_DF.loc[:,'date'] = CtField_DF.loc[:,'date'].apply(lambda x: x.split('/')[0] + '-' + x.split('/')[1] + '-' + x.split('/')[2][2:])\n",
    "#         mecaDF['ManipID'] = mecaDF['ExpDay'] + '_' + mecaDF['CellName'].apply(lambda x: x.split('_')[0])\n",
    "    return(CtField_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "computeGlobalTable_ctField(task='updateExisting',save=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_ctField().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanics\n",
    "\n",
    "Workflow\n",
    "* analyseTimeSeries_meca() analyse 1 file and return the dict (with the results of the analysis)\n",
    "* createMecaDataDict() call the previous function on the given list of files and concatenate the results\n",
    "* computeGlobalTable_meca() call the previous function and convert the dict to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     81,
     285,
     306,
     365
    ]
   },
   "outputs": [],
   "source": [
    "listColumnsMeca = ['date','cellName','cellID','manipID',\\\n",
    "                   'compNum','compDuration','compStartTime',\\\n",
    "                   'initialThickness','minThickness','maxIndent','previousThickness','surroundingThickness',\\\n",
    "                   'validatedThickness',\\\n",
    "                   'ctFieldThickness','ctFieldFluctuAmpli',\\\n",
    "                   'H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth',\\\n",
    "                   'hysteresis',\\\n",
    "                   'critFit', 'validatedFit','comments'] # 'fitParams',\n",
    "\n",
    "def compressionFitChadwick(hCompr, fCompr, DIAMETER):\n",
    "    \n",
    "    error = False\n",
    "    \n",
    "    def chadwickModel(h, E, H0):\n",
    "        R = DIAMETER/2\n",
    "        f = (np.pi*E*R*((H0-h)**2))/(3*H0)\n",
    "        return(f)\n",
    "\n",
    "    def inversedChadwickModel(f, E, H0):\n",
    "        R = DIAMETER/2\n",
    "        h = H0 - ((3*H0*f)/(np.pi*E*R))**0.5\n",
    "        return(h)\n",
    "\n",
    "    # some initial parameter values - must be within bounds\n",
    "    initH0 = max(hCompr) # H0 ~ h_max\n",
    "    initE = (3*max(hCompr)*max(fCompr))/(np.pi*(DIAMETER/2)*(max(hCompr)-min(hCompr))**2) # E ~ 3*H0*F_max / pi*R*(H0-h_min)²\n",
    "#     initH0, initE = initH0*(initH0>0), initE*(initE>0)\n",
    "    \n",
    "    initialParameters = [initE, initH0]\n",
    "#     print(initialParameters)\n",
    "\n",
    "    # bounds on parameters - initial parameters must be within these\n",
    "    lowerBounds = (0, 0)\n",
    "    upperBounds = (np.Inf, np.Inf)\n",
    "    parameterBounds = [lowerBounds, upperBounds]\n",
    "    \n",
    "#     testH = inversedChadwickModel(fCompr, initE, initH0)\n",
    "#     fig, ax = plt.subplots(1,1)\n",
    "#     ax.plot(hCompr,fCompr,'b-', linewidth = 0.8)\n",
    "#     ax.plot(testH,fCompr,'kx', linewidth = 0.8)\n",
    "#     ax.set_xlabel('h (nm)')\n",
    "#     ax.set_ylabel('f (pN)')\n",
    "#     fig.show()\n",
    "#     print(initialParameters)\n",
    "\n",
    "    try:\n",
    "        params, covM = curve_fit(inversedChadwickModel, fCompr, hCompr, initialParameters, bounds = parameterBounds)\n",
    "\n",
    "        # Previously I fitted with y=F and x=H, but it didn't work so well cause H(t) isn't monotonous:\n",
    "        # params, covM = curve_fit(chadwickModel, hCompr, fCompr, initialParameters, bounds = parameterBounds)\n",
    "        # Fitting with the 'inverse Chadwick model', with y=H and x=F is more convenient\n",
    "\n",
    "        E, H0 = params\n",
    "        hPredict = inversedChadwickModel(fCompr, E, H0)\n",
    "\n",
    "        SSR = np.sum((hCompr-hPredict)**2)\n",
    "        alpha = 0.975\n",
    "        df = len(fCompr)-len(params)\n",
    "        q = st.t.ppf(alpha, df) # Student coefficient\n",
    "        R2 = get_R2(hCompr,hPredict)\n",
    "\n",
    "        varE = covM[0,0]\n",
    "        seE = (varE)**0.5\n",
    "        E, seE = E*1e6, seE*1e6\n",
    "        confIntE = [E-q*seE, E+q*seE]\n",
    "        confIntEWidth = 2*q*seE\n",
    "\n",
    "        varH0 = covM[1,1]\n",
    "        seH0 = (varH0)**0.5\n",
    "        confIntH0 = [H0-q*seH0, H0+q*seH0]\n",
    "        confIntH0Width = 2*q*seH0\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        error = True\n",
    "        E, H0, hPredict, R2, confIntE, confIntH0 = -1, -1, np.ones(len(hCompr))*(-1), -1, [-1,-1], [-1,-1]\n",
    "    \n",
    "    return(E, H0, hPredict, R2, confIntE, confIntH0, error)\n",
    "\n",
    "\n",
    "\n",
    "def analyseTimeSeries_meca(f, tsDF, expDf, listColumnsMeca, PLOT, PLOT_SHOW):\n",
    "    \n",
    "    split_f = f.split('_')\n",
    "    tsDF.dx, tsDF.dy, tsDF.dz, tsDF.D2, tsDF.D3 = tsDF.dx*1000, tsDF.dy*1000, tsDF.dz*1000, tsDF.D2*1000, tsDF.D3*1000\n",
    "    thisManipID = split_f[0] + '_' + split_f[1]\n",
    "    expDf['manipID'] = expDf['date'] + '_' + expDf['manip']\n",
    "    thisExpDf = expDf.loc[expDf['manipID'] == thisManipID]\n",
    "    DIAMETER = thisExpDf.at[thisExpDf.index.values[0], 'bead diameter']\n",
    "    \n",
    "    results = {}\n",
    "    for c in listColumnsMeca:\n",
    "        results[c] = []\n",
    "        \n",
    "    Ncomp = max(tsDF['idxCompression'])\n",
    "    NimgComp = np.sum((tsDF['idxCompression'] != 0))/Ncomp\n",
    "    NimgCompTh = round(0.49999999999 + np.sum((tsDF['idxCompression'] != 0))/Ncomp)\n",
    "    NimgBtwComp = np.sum((tsDF['idxCompression'] == 0))/Ncomp\n",
    "    NimgBtwCompTh = round(0.4999999999 + np.sum((tsDF['idxCompression'] == 0))/Ncomp)\n",
    "#     print('Ncomp : ' + str(Ncomp) + ' ; ' + 'NimgComp : ' + str(NimgComp) + '/' + str(NimgCompTh) + ' ; ' + 'NimgBtwComp : ' + str(NimgBtwComp) + '/' + str(NimgBtwCompTh))\n",
    "#     if not NimgBtwComp%2 == 0:\n",
    "#         print('Bug with the compressions sequence delimitation')\n",
    "\n",
    "\n",
    "    # These values are computed once for the whole cell D3 time series, but since the table has 1 line per compression, \n",
    "    # that same value will be put in the table for each line corresponding to that cell\n",
    "    ctFieldH = (tsDF.loc[tsDF['idxCompression'] == 0, 'D3'].values - DIAMETER)\n",
    "    ctFieldThickness   = np.median(ctFieldH)\n",
    "    ctFieldFluctuAmpli = np.percentile(ctFieldH,90) - np.percentile(ctFieldH,10)\n",
    "    \n",
    "    if PLOT:\n",
    "        nColsSubplot = 5\n",
    "        nRowsSubplot = ((Ncomp-1) // nColsSubplot) + 1\n",
    "        fig1, ax1 = plt.subplots(1,1,figsize=(tsDF.shape[0]*(1/100),3))\n",
    "#         fig2, ax2 = plt.subplots(1,Ncomp,figsize=(3*Ncomp,3))\n",
    "        fig2, ax2 = plt.subplots(nRowsSubplot,nColsSubplot,figsize=(3*nColsSubplot,3*nRowsSubplot))\n",
    "        ax1.plot(tsDF['T'].values, tsDF['D3'].values-DIAMETER, 'b-', linewidth = 0.75)\n",
    "\n",
    "    \n",
    "    for i in range(1, Ncomp+1):#Ncomp+1):\n",
    "\n",
    "        # (1) Identifiers\n",
    "        results['date'].append(split_f[0])\n",
    "        results['cellName'].append(split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        results['cellID'].append(split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3])\n",
    "        results['manipID'].append(split_f[0] + '_' + split_f[1])\n",
    "        \n",
    "        # (2) Segment the compression n°i\n",
    "        thisCompDf = tsDF.loc[tsDF['idxCompression'] == i,:]\n",
    "        iStart = (findFirst(tsDF['idxCompression'], i))\n",
    "        iStop = iStart+thisCompDf.shape[0]\n",
    "        \n",
    "        # Easy-to-get parameters\n",
    "        results['compNum'].append(i)\n",
    "        results['compDuration'].append(thisExpDf.at[thisExpDf.index.values[0], 'compression duration'])\n",
    "        results['compStartTime'].append(thisCompDf['T'].values[0])\n",
    "        \n",
    "        # (3) Inside the compression n°i, delimit the compression and relaxation phases\n",
    "        \n",
    "        # Delimit the start of the increase of B (typically the moment when the field decrease from 5 to 3)\n",
    "        # and the end of its decrease (typically when it goes back from 3 to 5)\n",
    "        listB = thisCompDf.B.values\n",
    "        offsetStart, offsetStop = 0, 0\n",
    "        minB, maxB = min(listB), max(listB)\n",
    "        thresholdB = (maxB-minB)/50\n",
    "        \n",
    "        k = 0\n",
    "        while (listB[k] > minB+thresholdB) or (listB[-1-k] > minB+thresholdB):\n",
    "            offsetStart += int(listB[k] > minB+thresholdB)\n",
    "            offsetStop += int(listB[-1-k] > minB+thresholdB)\n",
    "            k += 1\n",
    "        jStart = offsetStart # Beginning of compression\n",
    "        jMax = np.argmax(thisCompDf.B) # End of compression, beginning of relaxation\n",
    "        jStop = thisCompDf.shape[0] - offsetStop # End of relaxation\n",
    "        \n",
    "        # Four arrays\n",
    "        hCompr = (thisCompDf.D3.values[jStart:jMax+1] - DIAMETER)\n",
    "        hRelax = (thisCompDf.D3.values[jMax+1:jStop] - DIAMETER)\n",
    "        fCompr = (thisCompDf.F.values[jStart:jMax+1])\n",
    "        fRelax = (thisCompDf.F.values[jMax+1:jStop])\n",
    "        \n",
    "        # Refinement of the compression delimitation.\n",
    "        # Remove the 1-2 points at the begining where there is just the viscous relaxation of the cortex\n",
    "        # because of the initial decrease of B and the cortex thickness increases.\n",
    "        offsetStart2 = 0\n",
    "        k = 0\n",
    "        while (hCompr[k] < np.max(hCompr[k+1:min(k+10, len(hCompr))])) and k<len(hCompr)-10:\n",
    "            offsetStart2 += 1\n",
    "            k += 1\n",
    "        # Better compressions arrays\n",
    "        hCompr = hCompr[offsetStart2:]\n",
    "        fCompr = fCompr[offsetStart2:]\n",
    "        \n",
    "        # Get the points of constant field preceding and surrounding the current compression\n",
    "        # Ex : if the labview code was set so that there is 6 points of ct field before and after each compression,\n",
    "        # previousPoints will contains D3[iStart-12:iStart]\n",
    "        # surroundingPoints will contains D3[iStart-6:iStart] and D3[iStop:iStop+6]\n",
    "        previousPoints = (tsDF.D3.values[max(0,iStart-(NimgBtwCompTh)):iStart]) - DIAMETER\n",
    "        surroundingPoints = np.concatenate([tsDF.D3.values[max(0,iStart-(NimgBtwCompTh//2)):iStart],tsDF.D3.values[iStop:iStop+(NimgBtwCompTh//2)]]) - DIAMETER\n",
    "        \n",
    "        # Parameters relative to the thickness ( = D3-DIAMETER)\n",
    "        results['initialThickness'].append(np.mean(hCompr[0:3]))\n",
    "        results['minThickness'].append(np.min(hCompr))\n",
    "        results['maxIndent'].append(results['initialThickness'][-1] - results['minThickness'][-1])\n",
    "        results['previousThickness'].append(np.median(previousPoints))\n",
    "        results['surroundingThickness'].append(np.median(surroundingPoints))\n",
    "        results['ctFieldThickness'].append(ctFieldThickness)\n",
    "        results['ctFieldFluctuAmpli'].append(ctFieldFluctuAmpli)\n",
    "        \n",
    "        validatedThickness = np.min([results['initialThickness'],results['minThickness'],results['previousThickness'],\\\n",
    "                                    results['surroundingThickness'],results['ctFieldThickness']]) > 0\n",
    "        results['validatedThickness'].append(validatedThickness)\n",
    "\n",
    "        # (4) Fit with Chadwick model of the force-thickness curve\n",
    "        \n",
    "        E, H0, hPredict, R2, confIntE, confIntH0, fitError = compressionFitChadwick(hCompr, fCompr, DIAMETER) # IMPORTANT SUBFUNCTION\n",
    "        \n",
    "        R2CRITERION = 0.9\n",
    "        critFit = 'R2 > ' + str(R2CRITERION)\n",
    "        results['critFit'].append(critFit)\n",
    "        validatedFit = (R2 > R2CRITERION)\n",
    "        \n",
    "        if PLOT:\n",
    "            # fig1\n",
    "            if not fitError:\n",
    "                if validatedFit:\n",
    "                    ax1.plot(thisCompDf['T'].values, thisCompDf['D3'].values-DIAMETER, color = 'chartreuse', linestyle = '-', linewidth = 1)\n",
    "                else:\n",
    "                    ax1.plot(thisCompDf['T'].values, thisCompDf['D3'].values-DIAMETER, color = 'gold', linestyle = '-', linewidth = 1)\n",
    "            else:\n",
    "                ax1.plot(thisCompDf['T'].values, thisCompDf['D3'].values-DIAMETER, color = 'crimson', linestyle = '-', linewidth = 1)\n",
    "            ax1.set_xlabel('t (s)')\n",
    "            ax1.set_ylabel('h (nm)')\n",
    "            fig1.suptitle(results['cellID'][-1])\n",
    "            \n",
    "            # fig2\n",
    "            colSp = (i-1) % nColsSubplot\n",
    "            rowSp = (i-1) // nColsSubplot\n",
    "            # ax2[i-1] with the 1 line plot\n",
    "            if nRowsSubplot == 1:\n",
    "                thisAx2 = ax2[colSp]\n",
    "            elif nRowsSubplot >= 1:\n",
    "                thisAx2 = ax2[rowSp,colSp]\n",
    "            thisAx2.plot(hCompr,fCompr,'b-', linewidth = 0.8)\n",
    "            thisAx2.plot(hRelax,fRelax,'r-', linewidth = 0.8)\n",
    "            titleText = results['cellID'][-1] + '__c' + str(i)\n",
    "            legendText = ''\n",
    "            thisAx2.set_xlabel('h (nm)')\n",
    "            thisAx2.set_ylabel('f (pN)')\n",
    "            if not fitError:\n",
    "                legendText += 'H0 = {:.1f}nm\\nE = {:.2e}Pa\\nR2 = {:.3f}'.format(H0, E, R2)\n",
    "                thisAx2.plot(hPredict,fCompr,'k--', linewidth = 0.8, label = legendText)\n",
    "                thisAx2.legend(loc = 'upper right', prop={'size': 6})\n",
    "                if not validatedFit:\n",
    "                    titleText += '\\nNON VALIDATED'\n",
    "            else:\n",
    "                titleText += '\\nFIT ERROR'\n",
    "            thisAx2.title.set_text(titleText)\n",
    "            for item in ([thisAx2.title, thisAx2.xaxis.label, \\\n",
    "                          thisAx2.yaxis.label] + thisAx2.get_xticklabels() + thisAx2.get_yticklabels()):\n",
    "                item.set_fontsize(9)\n",
    "                \n",
    "\n",
    "        if not fitError:\n",
    "            confIntEWidth = abs(confIntE[0] - confIntE[1])\n",
    "\n",
    "            results['H0Chadwick'].append(H0)\n",
    "            results['EChadwick'].append(E)\n",
    "            results['R2Chadwick'].append(R2)\n",
    "            results['EChadwick_CIWidth'].append(confIntEWidth)\n",
    "            \n",
    "\n",
    "            results['validatedFit'].append(validatedFit)\n",
    "            if validatedFit:\n",
    "                results['comments'].append('ok')\n",
    "            else:\n",
    "                results['comments'].append('R2 < ' + str(R2CRITERION))\n",
    "                \n",
    "        if fitError:\n",
    "            validatedFit = False\n",
    "            results['H0Chadwick'].append(np.nan)\n",
    "            results['EChadwick'].append(np.nan)\n",
    "            results['R2Chadwick'].append(np.nan)\n",
    "            results['EChadwick_CIWidth'].append(np.nan)\n",
    "            results['validatedFit'].append(validatedFit)\n",
    "            results['comments'].append('fitFailure')\n",
    "            \n",
    "        \n",
    "        # (5) hysteresis (its definition may change)\n",
    "        results['hysteresis'].append(hCompr[0] - hRelax[-1])\n",
    "    \n",
    "    if PLOT:\n",
    "        archiveFig(fig1, ax1, name=results['cellID'][-1] + '_h(t)', figSubDir = 'MecaAnalysis_allCells')\n",
    "        archiveFig(fig2, ax2, name=results['cellID'][-1] + '_F(h)', figSubDir = 'MecaAnalysis_allCells')\n",
    "        if PLOT_SHOW:\n",
    "            fig1.show()\n",
    "            fig2.tight_layout()\n",
    "            fig2.show()\n",
    "        else:\n",
    "            plt.close('all')\n",
    "    \n",
    "    return(results)\n",
    "\n",
    "\n",
    "\n",
    "def createDataDict_meca(list_mecaFiles, listColumnsMeca, PLOT):\n",
    "    expDf = getExperimentalConditions()\n",
    "    tableDict = {}\n",
    "    Nfiles = len(list_mecaFiles)\n",
    "    PLOT_SHOW = (Nfiles<11)\n",
    "    if not PLOT_SHOW:\n",
    "        plt.ioff()\n",
    "    for c in listColumnsMeca:\n",
    "        tableDict[c] = []\n",
    "    for f in list_mecaFiles: #[1:2]:\n",
    "#         print(f)\n",
    "        tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "        current_tsDF = pd.read_csv(tS_DataFilePath, ';')\n",
    "        current_resultDict = analyseTimeSeries_meca(f, current_tsDF, expDf, listColumnsMeca, PLOT, PLOT_SHOW) # MAIN SUBFUNCTION\n",
    "        for k in current_resultDict.keys():\n",
    "            tableDict[k] += current_resultDict[k]\n",
    "    plt.ion()\n",
    "    return(tableDict)\n",
    "\n",
    "\n",
    "\n",
    "def computeGlobalTable_meca(task = 'fromScratch', fileName = 'Global_MecaData', save = False, PLOT = False, \\\n",
    "                            listColumnsMeca=listColumnsMeca):\n",
    "    \"\"\"\n",
    "    Compute the GlobalTable_meca from the time series data files.\n",
    "    Option task='fromScratch' will analyse all the time series data files and construct a new GlobalTable from them regardless of the existing GlobalTable.\n",
    "    Option task='updateExisting' will open the existing GlobalTable and determine which of the time series data files are new ones, and will append the existing GlobalTable with the data analysed from those new fils.\n",
    "    listColumnsMeca have to contain all the fields of the table that will be constructed.\n",
    "    \"\"\"\n",
    "    top = time.time()\n",
    "    list_mecaFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                      if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                      and ('R40' in f))] # Change to allow different formats in the future\n",
    "#     print(list_mecaFiles)\n",
    "    \n",
    "    if task == 'fromScratch':\n",
    "        # create a dict containing the data\n",
    "        tableDict = createDataDict_meca(list_mecaFiles, listColumnsMeca, PLOT) # MAIN SUBFUNCTION\n",
    "        # create the dataframe from it\n",
    "        meca_DF = pd.DataFrame(tableDict)\n",
    "        \n",
    "    elif task == 'updateExisting':\n",
    "        # get existing table\n",
    "        try:\n",
    "            savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "            existing_meca_DF = pd.read_csv(savePath, sep=';')\n",
    "        except:\n",
    "            print('No existing table found')\n",
    "            \n",
    "        # find which of the time series files are new\n",
    "        new_list_mecaFiles = []\n",
    "        for f in list_mecaFiles:\n",
    "            split_f = f.split('_')\n",
    "            currentCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "            if currentCellID not in existing_meca_DF.cellID.values:\n",
    "                new_list_mecaFiles.append(f)\n",
    "                \n",
    "        # create the dict with new data\n",
    "        new_tableDict = createDataDict_meca(new_list_mecaFiles, listColumnsMeca, PLOT) # MAIN SUBFUNCTION\n",
    "        # create the dataframe from it\n",
    "        new_meca_DF = pd.DataFrame(new_tableDict)\n",
    "        # fuse the existing table with the new one\n",
    "        meca_DF = pd.concat([existing_meca_DF, new_meca_DF])\n",
    "    \n",
    "    for c in meca_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                meca_DF = meca_DF.drop([c], axis=1)\n",
    "    \n",
    "    if save:\n",
    "        saveName = fileName + '.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        meca_DF.to_csv(savePath, sep=';')\n",
    "    \n",
    "    delta = time.time() - top\n",
    "    print(delta)\n",
    "    \n",
    "    return(meca_DF)\n",
    "            \n",
    "\n",
    "    \n",
    "def getGlobalTable_meca(fileName = 'Global_mecaData'):\n",
    "    try:\n",
    "        savePath = os.path.join(dataDir, (fileName + '.csv'))\n",
    "        meca_DF = pd.read_csv(savePath, sep=';')\n",
    "        print('Extracted a table with ' + str(meca_DF.shape[0]) + ' lines and ' + str(meca_DF.shape[1]) + ' columns.')\n",
    "    except:\n",
    "        print('No existing table found')\n",
    "    for c in meca_DF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                meca_DF = meca_DF.drop([c], axis=1)\n",
    "    \n",
    "    if 'ExpDay'in meca_DF.columns:\n",
    "        dateExemple = meca_DF.loc[meca_DF.index[0],'ExpDay']\n",
    "        if not ('manipID' in meca_DF.columns):\n",
    "            meca_DF['manipID'] = meca_DF['ExpDay'] + '_' + meca_DF['CellID'].apply(lambda x: x.split('_')[0])\n",
    "            \n",
    "    elif 'date'in meca_DF.columns:\n",
    "        dateExemple = meca_DF.loc[meca_DF.index[0],'date']\n",
    "        if not ('manipID' in meca_DF.columns):\n",
    "            meca_DF['manipID'] = meca_DF['date'] + '_' + meca_DF['cellName'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    if re.match(dateFormatExcel, dateExemple):\n",
    "        print('bad date')\n",
    "    return(meca_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "computeGlobalTable_meca(task = 'fromScratch', fileName = 'Global_MecaData_Py', save = True, PLOT = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_meca().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getGlobalTable_meca('Global_MecaData_Py').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparison - Per compression\n",
    "\n",
    "Problems with an offset in the indexation of compression in the matlab table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Comparison of values obtained with the python code vs matlab\n",
    "\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "\n",
    "mecaPy = getGlobalTable_meca('Global_MecaData_Py')\n",
    "mecaPy = pd.merge(table_ExpConditions, mecaPy, how=\"inner\", on='manipID')\n",
    "mecaPy = pd.merge(mecaPy, table_fluo, how=\"left\", on='cellID')\n",
    "mecaPy = mecaPy.sort_values([\"cellID\", \"compNum\"], ascending = (True, True),ignore_index=True)\n",
    "# mecaPy = mecaPy.loc[(mecaPy['cell subtype'] == 'aSFL') | (mecaPy['cell subtype'] == 'aSFL-6FP')]\n",
    "mecaMat = getGlobalTable_meca('Global_MecaData')\n",
    "mecaMat = pd.merge(table_ExpConditions, mecaMat, how=\"inner\", on='manipID')\n",
    "mecaMat = pd.merge(mecaMat, table_fluo, how=\"left\", left_on='CellName', right_on='cellID')\n",
    "mecaMat = mecaMat.sort_values([\"CellName\", \"CompNum\"], ascending = (True, True),ignore_index=True)\n",
    "\n",
    "FiltersPy = [(mecaMat['Validated'] == True), (mecaPy['validatedFit'] == True)]\n",
    "filterGlobal = np.ones((mecaPy.shape[0]), dtype=bool)\n",
    "for fltr in FiltersPy:\n",
    "    filterGlobal = filterGlobal & fltr\n",
    "mecaPyF = mecaPy.loc[filterGlobal]\n",
    "mecaPyF_light = mecaPyF[['compNum','H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth']]\n",
    "mecaPyF.tail()\n",
    "np.sum(mecaPyF['validatedFit'].values)\n",
    "mecaPyF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "FiltersMat = [(mecaPy['validatedFit'] == True), (mecaMat['Validated'] == True)]\n",
    "filterGlobal = np.ones((mecaMat.shape[0]), dtype=bool)\n",
    "for fltr in FiltersPy:\n",
    "    filterGlobal = filterGlobal & fltr\n",
    "mecaMatF = mecaMat.loc[filterGlobal]\n",
    "mecaMatF = mecaMatF.rename(columns={'CompNum': 'compNum', 'CiEChadwick': 'EChadwick_CIWidth'})\n",
    "mecaMatF_light = mecaMatF[['compNum','H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth']]\n",
    "# mecaMat_light.index = [i for i in range(Bbis.shape[0])]\n",
    "mecaMatF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "mecaCompare = (mecaPyF_light-mecaMatF_light)/mecaMatF_light\n",
    "Th = 0.1\n",
    "mecaCompare['Evaries'] = (np.abs(mecaCompare.EChadwick) > Th)\n",
    "mecaCompare['H0varies'] = (np.abs(mecaCompare.H0Chadwick) > Th)\n",
    "mecaCompare['cellID'] = mecaPyF['cellID']\n",
    "mecaCompare['compNum'] = mecaPyF['compNum']\n",
    "mecaCompare['cellID2'] = mecaMatF['CellName']\n",
    "mecaCompare['compNum2'] = mecaMatF['compNum']\n",
    "mecaCompare = mecaCompare[['H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth','Evaries','H0varies','cellID','compNum','cellID2','compNum2']]\n",
    "print('Nb of signif changes in E : {:.0f} / {:.0f} ; in H0 : {:.0f} / {:.0f}'.format(np.sum(mecaCompare['Evaries']), mecaCompare.shape[0], np.sum(mecaCompare['H0varies']), mecaCompare.shape[0]))\n",
    "mecaCompare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparison - Per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Comparison of values obtained with the python code vs matlab\n",
    "\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "\n",
    "mecaPy = getGlobalTable_meca('Global_MecaData_Py')\n",
    "mecaPy = pd.merge(table_ExpConditions, mecaPy, how=\"inner\", on='manipID')\n",
    "mecaPy = pd.merge(mecaPy, table_fluo, how=\"left\", on='cellID')\n",
    "mecaPy = mecaPy.sort_values([\"cellID\", \"compNum\"], ascending = (True, True),ignore_index=True)\n",
    "# mecaPy = mecaPy.loc[(mecaPy['cell subtype'] == 'aSFL') | (mecaPy['cell subtype'] == 'aSFL-6FP')]\n",
    "mecaMat = getGlobalTable_meca('Global_MecaData')\n",
    "mecaMat = pd.merge(table_ExpConditions, mecaMat, how=\"inner\", on='manipID')\n",
    "mecaMat = pd.merge(mecaMat, table_fluo, how=\"left\", left_on='CellName', right_on='cellID')\n",
    "mecaMat = mecaMat.sort_values([\"CellName\", \"CompNum\"], ascending = (True, True),ignore_index=True)\n",
    "\n",
    "FiltersPy = [(mecaMat['Validated'] == True), (mecaPy['validatedFit'] == True)]\n",
    "filterGlobal = np.ones((mecaPy.shape[0]), dtype=bool)\n",
    "for fltr in FiltersPy:\n",
    "    filterGlobal = filterGlobal & fltr\n",
    "mecaPyF = mecaPy.loc[filterGlobal]\n",
    "\n",
    "cellID = 'cellID'\n",
    "group = mecaPyF.groupby(cellID)\n",
    "dictAggMean = getDictAggMean(mecaPyF)\n",
    "mecaPyF_perCell = group.agg(dictAggMean)\n",
    "\n",
    "mecaPyF_perCell_light = mecaPyF_perCell[['H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth']]\n",
    "# mecaPyF_perCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "FiltersMat = [(mecaPy['validatedFit'] == True), (mecaMat['Validated'] == True)]\n",
    "filterGlobal = np.ones((mecaMat.shape[0]), dtype=bool)\n",
    "for fltr in FiltersPy:\n",
    "    filterGlobal = filterGlobal & fltr\n",
    "mecaMatF = mecaMat.loc[filterGlobal]\n",
    "mecaMatF = mecaMatF.rename(columns={'CompNum': 'compNum', 'CiEChadwick': 'EChadwick_CIWidth'})\n",
    "\n",
    "cellID = 'CellName'\n",
    "group = mecaMatF.groupby(cellID)\n",
    "dictAggMean = getDictAggMean(mecaMatF)\n",
    "mecaMatF_perCell = group.agg(dictAggMean)\n",
    "\n",
    "\n",
    "mecaMatF_perCell_light = mecaMatF_perCell[['H0Chadwick','EChadwick','R2Chadwick','EChadwick_CIWidth']]\n",
    "# mecaMat_light.index = [i for i in range(Bbis.shape[0])]\n",
    "# mecaMatF_perCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mecaCompare = (mecaPyF_perCell_light-mecaMatF_perCell_light)/mecaMatF_perCell_light\n",
    "Th = 0.1\n",
    "mecaCompare['Evaries'] = (np.abs(mecaCompare.EChadwick) > Th)\n",
    "mecaCompare['H0varies'] = (np.abs(mecaCompare.H0Chadwick) > Th)\n",
    "mecaCompare['cellID'] = mecaPyF_perCell['cellID']\n",
    "mecaCompare['cellID2'] = mecaMatF_perCell['CellName']\n",
    "print('Nb of signif changes in E : {:.0f} / {:.0f} ; in H0 : {:.0f} / {:.0f}'.format(np.sum(mecaCompare['Evaries']), mecaCompare.shape[0], np.sum(mecaCompare['H0varies']), mecaCompare.shape[0]))\n",
    "mecaCompare #.loc[mecaCompare['Evaries']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getFluoData(save = False):\n",
    "    # Getting the table\n",
    "    fluoDataFile = 'FluoQuantification.csv'\n",
    "    fluoDataFilePath = os.path.join(dataDir, fluoDataFile)\n",
    "    fluoDF = pd.read_csv(fluoDataFilePath, sep=';',header=0)\n",
    "    print('Extracted a table with ' + str(fluoDF.shape[0]) + ' lines and ' + str(fluoDF.shape[1]) + ' columns.')\n",
    "    # Cleaning the table\n",
    "    try:\n",
    "        for c in fluoDF.columns:\n",
    "            if 'Unnamed' in c:\n",
    "                fluoDF = fluoDF.drop([c], axis=1)\n",
    "        \n",
    "    except:\n",
    "        print('Unexpected bug with the cleaning step')\n",
    "\n",
    "    if save:\n",
    "        saveName = 'FluoQuantification.csv'\n",
    "        savePath = os.path.join(dataDir, saveName)\n",
    "        fluoDF.to_csv(savePath, sep=';')\n",
    "\n",
    "    \n",
    "    return(fluoDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFluoData().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import & DataFrame formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "pd.reset_option('max_columns')\n",
    "pd.set_option('max_rows', None)\n",
    "pd.reset_option('max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getExperimentalConditions().head()\n",
    "# getGlobalTable_ctField().head()\n",
    "# getGlobalTable_meca().head()\n",
    "# getFluoData().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalTable_ctField\n",
    "\n",
    "GlobalTable_ctField = getGlobalTable_ctField()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_ctField = pd.merge(table_ExpConditions, GlobalTable_ctField, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_ctField = pd.merge(GlobalTable_ctField, table_fluo, how=\"left\", on='cellID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_ctField.shape[0]) + ' lines and ' + str(GlobalTable_ctField.shape[1]) + ' columns.')\n",
    "\n",
    "GlobalTable_ctField.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalTable_meca\n",
    "\n",
    "GlobalTable_meca = getGlobalTable_meca()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_ExpConditions, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_fluo, how=\"left\", left_on='CellName', right_on='cellID'\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_meca.shape[0]) + ' lines and ' + str(GlobalTable_meca.shape[1]) + ' columns.')\n",
    "\n",
    "GlobalTable_meca.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GlobalTable_mecaBis\n",
    "\n",
    "GlobalTable_meca_Py = getGlobalTable_meca('Global_MecaData_Py')\n",
    "\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_meca_Py = pd.merge(GlobalTable_meca_Py, table_ExpConditions, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_meca_Py = pd.merge(GlobalTable_meca_Py, table_fluo, how=\"left\", left_on='cellID', right_on='cellID'\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_meca_Py.shape[0]) + ' lines and ' + str(GlobalTable_meca_Py.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalTable_meca_Py.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment counter - Matlab table\n",
    "\n",
    "# count ct field cells\n",
    "cellID = 'cellID'\n",
    "GlobalTable_ctField_CountCell = GlobalTable_ctField.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_ctField_CountCell = GlobalTable_ctField_CountCell.loc[:, [cellID]].rename(columns={cellID : 'Count cells - ctField'})\n",
    "\n",
    "# count meca compressions\n",
    "cellID = 'CellName'\n",
    "GlobalTable_meca_CountComp = GlobalTable_meca.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_CountComp = GlobalTable_meca_CountComp.loc[:, [cellID]].rename(columns={cellID : 'Count compressions'})\n",
    "\n",
    "# count valid meca compressions\n",
    "cellID = 'CellName'\n",
    "GlobalTable_meca_CountCompOK = GlobalTable_meca[GlobalTable_meca['Validated'] == True].groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_CountCompOK = GlobalTable_meca_CountCompOK.loc[:, [cellID]].rename(columns={cellID : 'Count OK compressions'})\n",
    "\n",
    "# count meca cells\n",
    "cellID = 'CellName'\n",
    "group = GlobalTable_meca.groupby(cellID)\n",
    "dictAggMean = getDictAggMean(GlobalTable_meca)\n",
    "GlobalTable_meca_perCell = group.agg(dictAggMean)\n",
    "GlobalTable_meca_CountCell = GlobalTable_meca_perCell.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_CountCell = GlobalTable_meca_CountCell.loc[:, [cellID]].rename(columns={cellID : 'Count cells - meca'})\n",
    "\n",
    "\n",
    "# Fuse all the previous tables\n",
    "GlobalTable_CountAll = pd.concat([GlobalTable_ctField_CountCell, GlobalTable_meca_CountCell, GlobalTable_meca_CountComp, GlobalTable_meca_CountCompOK], axis=1)\n",
    "GlobalTable_CountAll = GlobalTable_CountAll.fillna(0)\n",
    "GlobalTable_CountAll = GlobalTable_CountAll.loc[:,:].astype(int)\n",
    "GlobalTable_CountAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment counter - Python table\n",
    "\n",
    "# count ct field cells\n",
    "cellID = 'cellID'\n",
    "GlobalTable_ctField_CountCell = GlobalTable_ctField.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_ctField_CountCell = GlobalTable_ctField_CountCell.loc[:, [cellID]].rename(columns={cellID : 'Count cells - ctField'})\n",
    "\n",
    "# count meca compressions\n",
    "cellID = 'cellID'\n",
    "GlobalTable_meca_CountComp = GlobalTable_meca_Py.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_CountComp = GlobalTable_meca_CountComp.loc[:, [cellID]].rename(columns={cellID : 'Count compressions'})\n",
    "\n",
    "# count valid meca compressions\n",
    "cellID = 'cellID'\n",
    "GlobalTable_meca_CountCompOK = GlobalTable_meca_Py[GlobalTable_meca_Py['validatedFit'] == True].groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_CountCompOK = GlobalTable_meca_CountCompOK.loc[:, [cellID]].rename(columns={cellID : 'Count OK compressions'})\n",
    "\n",
    "# count meca cells\n",
    "cellID = 'cellID'\n",
    "group = GlobalTable_meca_Py.groupby(cellID)\n",
    "dictAggMean = getDictAggMean(GlobalTable_meca_Py)\n",
    "GlobalTable_meca_perCell = group.agg(dictAggMean)\n",
    "GlobalTable_meca_CountCell = GlobalTable_meca_perCell.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_CountCell = GlobalTable_meca_CountCell.loc[:, [cellID]].rename(columns={cellID : 'Count cells - meca'})\n",
    "\n",
    "\n",
    "# Fuse all the previous tables\n",
    "GlobalTable_CountAll = pd.concat([GlobalTable_ctField_CountCell, GlobalTable_meca_CountCell, GlobalTable_meca_CountComp, GlobalTable_meca_CountCompOK], axis=1)\n",
    "GlobalTable_CountAll = GlobalTable_CountAll.fillna(0)\n",
    "GlobalTable_CountAll = GlobalTable_CountAll.loc[:,:].astype(int)\n",
    "GlobalTable_CountAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filters = [(GlobalTable_meca_Py['validatedFit'] == True), (GlobalTable_meca_Py['validatedThickness'] == True), (GlobalTable_meca_Py['cell subtype'] == 'aSFL-A8')]\n",
    "# GlobalTable_meca_PyF = GlobalTable_meca_Py\n",
    "# for fltr in Filters:\n",
    "#         GlobalTable_meca_PyF = GlobalTable_meca_PyF.loc[fltr]\n",
    "# # GlobalTable_mecaBis\n",
    "# cellID = 'cellID'\n",
    "# group = GlobalTable_meca_PyF.groupby(cellID)\n",
    "# dictAggMean = getDictAggMean(GlobalTable_meca_PyF)\n",
    "# GlobalTable_meca_PyF_perCell = group.agg(dictAggMean)\n",
    "# GlobalTable_meca_PyF_perCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Styles = {''} # Project of automatic formatting according to the type of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions use matplotlib.pyplot and seaborn libraries to display 1D categorical or 2D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D1Plot(data, CondCol=[],Parameters=[],Filters=[],Boxplot=True,AvgPerCell=False,cellID='cellID', co_order=[],\\\n",
    "           stats=True,statMethod='Mann-Whitney',box_pairs=[],figSizeFactor = 1,markerSizeFactor=1):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "    NCond = len(CondCol)    \n",
    "    if NCond == 1:\n",
    "        CondCol = CondCol[0]\n",
    "    elif NCond > 1:\n",
    "        newColName = ''\n",
    "        for i in range(NCond):\n",
    "            newColName += CondCol[i]\n",
    "            newColName += ' & '\n",
    "        newColName = newColName[:-3]\n",
    "        data_filtered[newColName] = ''\n",
    "        for i in range(NCond):\n",
    "            data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "            data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "        CondCol = newColName\n",
    "    \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean)\n",
    "        \n",
    "    data_filtered.sort_values(CondCol, axis=0, ascending=True, inplace=True)\n",
    "    \n",
    "    NPlots = len(Parameters)\n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    if len(co_order) == 0:\n",
    "        co_order = Conditions\n",
    "        \n",
    "    fig, ax = plt.subplots(1, NPlots, figsize = (5*NPlots*NCond*figSizeFactor,5))\n",
    "    markerSize = 5*markerSizeFactor\n",
    "    \n",
    "    if NPlots > 1:\n",
    "        for k in range(NPlots):\n",
    "            \n",
    "            if Parameters[k] == 'EChadwick':\n",
    "                ax[k].set_yscale('log')\n",
    "                \n",
    "\n",
    "            if Boxplot:\n",
    "                sns.boxplot(x=CondCol, y=Parameters[k], data=data_filtered, ax=ax[k], \n",
    "                            color='w', linewidth = 2, width = 0.5, showfliers = False, order= co_order)\n",
    "                # data_filtered.boxplot(column=Parameters[k], by = CondCol, ax=ax[k],showfliers = False) # linewidth = 2, width = 0.5\n",
    "            \n",
    "            if stats:\n",
    "                if len(box_pairs) == 0:\n",
    "                    box_pairs = makeBoxPairs(co_order)\n",
    "                addStat(ax[k], data_filtered, box_pairs, Parameters[k], CondCol, test = statMethod)\n",
    "#                 add_stat_annotation(ax[k], x=CondCol, y=Parameters[k], data=data_filtered,box_pairs = box_pairs,test=statMethod, text_format='star',loc='inside', verbose=2)\n",
    "                    \n",
    "            sns.swarmplot(x=CondCol, y=Parameters[k], data=data_filtered, ax=ax[k], \n",
    "                          size=markerSize, edgecolor='k',linewidth = 1*markerSizeFactor, order= co_order)\n",
    "            \n",
    "            ax[k].set_xlabel('')\n",
    "            ax[k].set_ylabel(Parameters[k])\n",
    "            ax[k].tick_params(axis='x', labelrotation = 10)\n",
    "            ax[k].yaxis.grid(True)\n",
    "#             if Parameters[k] == 'EChadwick_log':\n",
    "                \n",
    "            \n",
    "            \n",
    "    else:\n",
    "#         vals = []\n",
    "#         for i in range(len(Conditions)):\n",
    "#             vals.append(data_filtered[data_filtered[CondCol] == Conditions[i]][Parameters[0]])\n",
    "        sns.swarmplot(x=CondCol, y=Parameters[0], data=data_filtered, ax=ax, \n",
    "                          size=markerSize, edgecolor='k',linewidth = 0.5, order= co_order)\n",
    "        if Boxplot:\n",
    "            sns.boxplot(x=CondCol, y=Parameters[0], data=data_filtered, ax=ax, \n",
    "                        color='w', linewidth = 2, width = 0.5, showfliers = False, order= co_order)\n",
    "        if stats:\n",
    "            if len(box_pairs) == 0:\n",
    "                box_pairs = makeBoxPairs(co_order)\n",
    "            add_stat_annotation(ax, x=CondCol, y=Parameters[0], data=data_filtered,\n",
    "                               box_pairs = box_pairs,\n",
    "                               test=statMethod, text_format='star',\n",
    "                               loc='inside', verbose=statVerbose)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(Parameters[0])\n",
    "        ax.tick_params(axis='x', labelrotation = 10)\n",
    "        ax.yaxis.grid(True)\n",
    "    return(fig, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D2Plot(data, XCol='',YCol='',CondCol='',Filters=[], cellID='cellID', AvgPerCell=False, modelFit=False, modelType='y=ax+b'):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "    \n",
    "    NCond = len(CondCol)    \n",
    "    if NCond == 1:\n",
    "        CondCol = CondCol[0]\n",
    "    elif NCond > 1:\n",
    "        newColName = ''\n",
    "        for i in range(NCond):\n",
    "            newColName += CondCol[i]\n",
    "            newColName += ' & '\n",
    "        newColName = newColName[:-3]\n",
    "        data_filtered[newColName] = ''\n",
    "        for i in range(NCond):\n",
    "            data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "            data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "        CondCol = newColName\n",
    "    \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "        \n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (8,5))\n",
    "    markerSize = 5\n",
    "    \n",
    "    if modelFit:\n",
    "        # Tweak the style cycle to plot for each condition: the points ('o') and then the fit ('-') with the same color.\n",
    "        ncustom_color_list = list(np.array([new_color_list, new_color_list]).flatten(order='F'))\n",
    "        # if new_color_list was ['red', 'green', blue'], custom_color_list is now ['red', 'red', 'green', 'green', blue', blue']\n",
    "        cc = cycler(color=ncustom_color_list)\n",
    "        ax.set_prop_cycle(cc)\n",
    "    \n",
    "    for c in Conditions:\n",
    "        Xraw = data_filtered[data_filtered[CondCol] == c][XCol].values\n",
    "        Yraw = data_filtered[data_filtered[CondCol] == c][YCol].values\n",
    "        XYraw = np.array([Xraw,Yraw]).T\n",
    "        XY = XYraw[~np.isnan(XYraw).any(axis=1), :]\n",
    "        X, Y = XY[:,0], XY[:,1]\n",
    "        if len(X) > 0:\n",
    "            eqnText = ''\n",
    "\n",
    "            if modelFit:\n",
    "                print('Fitting condition: ' + c + ' with model ' + modelType)\n",
    "                if modelType == 'y=ax+b':\n",
    "                    params, results = fitLine(X, Y) # Y=a*X+b ; params[0] = b,  params[1] = a\n",
    "                    pval = results.pvalues[1] # pvalue on the param 'a'\n",
    "                    eqnText += \" ; Y = {:.1f} X + {:.1f}\".format(params[1], params[0])\n",
    "                    eqnText += \" ; p-val = {:.2e}\".format(pval)\n",
    "                    print(\"Y = {:.5} X + {:.5}\".format(params[1], params[0]))\n",
    "                    print(\"p-value on the 'a' coefficient: {:.4e}\".format(pval))\n",
    "                    print('\\n')\n",
    "                    fitY = params[1]*X + params[0]\n",
    "                    imin = np.argmin(X)\n",
    "                    imax = np.argmax(X)\n",
    "                    ax.plot([X[imin],X[imax]], [fitY[imin],fitY[imax]], '--', lw = '1')\n",
    "\n",
    "                elif modelType == 'y=A*exp(kx)':\n",
    "                    params, results = fitLine(X, np.log(Y)) # Y=a*X+b ; params[0] = b,  params[1] = a\n",
    "                    pval = results.pvalues[1] # pvalue on the param 'a'\n",
    "                    eqnText += \" ; Y = {:.1f}*exp({:.1f}*X)\".format(params[0], params[1])\n",
    "                    eqnText += \" ; p-val = {:.2e}\".format(pval)\n",
    "                    print(\"Y = {:.5}*exp({:.5}*X)\".format(np.exp(params[0]), params[1]))\n",
    "                    print(\"p-value on the 'k' coefficient: {:.4e}\".format(pval))\n",
    "                    print('\\n')\n",
    "                    fitY = np.exp(params[0])*np.exp(params[1]*X)\n",
    "                    imin = np.argmin(X)\n",
    "                    imax = np.argmax(X)\n",
    "                    ax.plot([X[imin],X[imax]], [fitY[imin],fitY[imax]], '--', lw = '1')\n",
    "                    \n",
    "            ax.plot(X, Y, 'o', markersize = markerSize, markeredgecolor='k', markeredgewidth = 1, label=c + eqnText)\n",
    "            \n",
    "            \n",
    "    ax.set_xlabel(XCol)\n",
    "    ax.set_xlim([min(0,1.1*np.min(data_filtered[XCol])), 1.1*np.max(data_filtered[XCol])])\n",
    "    ax.set_ylabel(YCol)\n",
    "    ax.set_ylim([min(0,1.1*np.min(data_filtered[YCol])), 1.1*np.max(data_filtered[YCol])])\n",
    "    ax.legend(loc='upper left')\n",
    "    return(fig, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions use the Bokeh library to display 1D categorical or 2D plots with interactive plots. They are less flexible but can be nice to explore the data set since you can display the cellID which is the source of each point by passing your pointer over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D1PlotInteractive(data, CondCol='',Parameters=[],Filters=[],AvgPerCell=False,cellID='cellID'):\n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "#     print(data_filtered[cellID])\n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "    \n",
    "#     return(data_filtered)\n",
    "    \n",
    "    NPlots = len(Parameters)\n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "    if NPlots > 1:\n",
    "        plots = []\n",
    "        NCond = len(Conditions)\n",
    "        data_filtered['X'] = 0\n",
    "        data_filtered['X_jitter'] = 0.\n",
    "        dictTicks = {}\n",
    "        for i in range(NCond):\n",
    "            mask = data_filtered[CondCol] == Conditions[i]\n",
    "            data_filtered.loc[mask, 'X'] = i+1\n",
    "            dictTicks[i+1] = Conditions[i]\n",
    "        for i in data_filtered.index:\n",
    "            data_filtered.loc[i, 'X_jitter'] = data_filtered.loc[i, 'X'] + 0.4*(np.random.rand(1)[0]-0.5)\n",
    "        source = ColumnDataSource(\n",
    "            data=data_filtered[[cellID]+[CondCol]+Parameters+['X','X_jitter']]\n",
    "        )        \n",
    "        \n",
    "        for k in range(NPlots):\n",
    "            hover = HoverTool(\n",
    "                tooltips=[\n",
    "                    ('Cell ID', \"@\"+cellID),\n",
    "                    (Parameters[k], \"@\"+Parameters[k]),\n",
    "                ]\n",
    "            )\n",
    "            index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "            p = figure(plot_width=450, plot_height=500, tools=[hover], title=\"InteractivePlot\") # \n",
    "            p.circle('X_jitter', Parameters[k], size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "            # Format\n",
    "            p.x_range = Range1d(0, NCond+1)\n",
    "            p.y_range = Range1d(min(0,1.1*np.min(data_filtered[Parameters[0]])), 1.1*np.max(data_filtered[Parameters[k]]))\n",
    "            p.xaxis.ticker = [i for i in range(1,NCond+1)]\n",
    "            p.xaxis.major_label_overrides = dictTicks\n",
    "            p.xaxis.axis_label = CondCol\n",
    "            p.xaxis.axis_label_text_font_size = '18pt'\n",
    "            p.xaxis.major_label_text_font_size = '16pt'\n",
    "            p.yaxis.axis_label = Parameters[k]\n",
    "            p.yaxis.axis_label_text_font_size = '18pt'\n",
    "            p.yaxis.major_label_text_font_size = '16pt'\n",
    "            \n",
    "            plots.append(p)\n",
    "            \n",
    "        p = gridplot(plots, ncols=2, toolbar_location=None)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        hover = HoverTool(\n",
    "            tooltips=[\n",
    "                ('Cell ID', \"@\"+cellID),\n",
    "                (Parameters[0], \"@\"+Parameters[0]),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        NCond = len(Conditions)\n",
    "        data_filtered['X'] = 0\n",
    "        data_filtered['X_jitter'] = 0.\n",
    "        dictTicks = {}\n",
    "        for i in range(NCond):\n",
    "            mask = data_filtered[CondCol] == Conditions[i]\n",
    "            data_filtered.loc[mask, 'X'] = i+1\n",
    "            dictTicks[i+1] = Conditions[i]\n",
    "        for i in data_filtered.index:\n",
    "            data_filtered.loc[i, 'X_jitter'] = data_filtered.loc[i, 'X'] + 0.4*(np.random.rand(1)[0]-0.5)\n",
    "        source = ColumnDataSource(\n",
    "            data=data_filtered[[cellID]+[CondCol]+Parameters+['X','X_jitter']]\n",
    "        )\n",
    "        index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "        TOOLS = \"hover,pan,box_zoom,wheel_zoom,reset,save,help\"\n",
    "        p = figure(plot_width=500, plot_height=500, tools=TOOLS, title=\"InteractivePlot\") # \n",
    "        p.circle('X_jitter', Parameters[0], size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "        # Format\n",
    "        p.x_range = Range1d(0, NCond+1)\n",
    "        p.y_range = Range1d(min(0,1.1*np.min(data_filtered[Parameters[0]])), 1.1*np.max(data_filtered[Parameters[0]]))\n",
    "        p.xaxis.ticker = [i for i in range(1,NCond+1)]\n",
    "        p.xaxis.major_label_overrides = dictTicks\n",
    "        p.xaxis.axis_label = CondCol\n",
    "        p.xaxis.axis_label_text_font_size = '18pt'\n",
    "        p.xaxis.major_label_text_font_size = '16pt'\n",
    "        p.yaxis.axis_label = Parameters[0]\n",
    "        p.yaxis.axis_label_text_font_size = '18pt'\n",
    "        p.yaxis.major_label_text_font_size = '16pt'\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def D2PlotInteractive(data, XCol='',YCol='',CondCol='',Filters=[], cellID='cellID',AvgPerCell=False):\n",
    "    \n",
    "    data_filtered = data\n",
    "    for fltr in Filters:\n",
    "        data_filtered = data_filtered.loc[fltr]\n",
    "        \n",
    "    if AvgPerCell:\n",
    "        group = data_filtered.groupby(cellID)\n",
    "        dictAggMean = getDictAggMean(data_filtered)\n",
    "        data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "        data_filtered.reset_index(level=0, inplace=True)\n",
    "    \n",
    "    Conditions = list(data_filtered[CondCol].unique())\n",
    "\n",
    "    NCond = len(Conditions)\n",
    "    dictTicks = {}\n",
    "    for i in range(NCond):\n",
    "        dictTicks[i+1] = Conditions[i]\n",
    "    \n",
    "    source = ColumnDataSource(\n",
    "        data=data_filtered[[cellID,CondCol,XCol,YCol]]\n",
    "    )\n",
    "    \n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            ('Cell ID', \"@\"+cellID),\n",
    "            (XCol, \"@\"+XCol),\n",
    "            (YCol, \"@\"+YCol),\n",
    "            (CondCol, \"@\"+CondCol),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    index_cmap = factor_cmap(CondCol, palette=Category10[10], factors=sorted(data_filtered[CondCol].unique()), end=1)\n",
    "    TOOLS = \"pan,box_zoom,wheel_zoom,reset,save,help\"\n",
    "    p = figure(plot_width=900, plot_height=500, tools=TOOLS, title=\"InteractivePlot\",toolbar_location=\"below\") # \n",
    "    p.circle(XCol, YCol, size=8, alpha = 0.6, source=source,fill_color=index_cmap,line_color='black')\n",
    "    p.add_tools(hover)\n",
    "    # Format\n",
    "    p.x_range = Range1d(0, 1.1*np.max(data_filtered[XCol]))\n",
    "    p.y_range = Range1d(0, 1.1*np.max(data_filtered[YCol]))\n",
    "    p.xaxis.axis_label = XCol\n",
    "    p.xaxis.axis_label_text_font_size = '18pt'\n",
    "    p.xaxis.major_label_text_font_size = '16pt'\n",
    "    p.yaxis.axis_label = YCol\n",
    "    p.yaxis.axis_label_text_font_size = '18pt'\n",
    "    p.yaxis.major_label_text_font_size = '16pt'\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other subfunctions useful to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     13,
     17,
     42
    ]
   },
   "outputs": [],
   "source": [
    "def makeOrder(*args):\n",
    "    order = []\n",
    "    listeTuple = list(itertools.product(*args, repeat=1))\n",
    "    for tup in listeTuple:\n",
    "        tmpText = ''\n",
    "        for word in tup:\n",
    "            tmpText += word\n",
    "            tmpText += ' & '\n",
    "        tmpText = tmpText[:-3]\n",
    "        order.append(tmpText)\n",
    "    return(order)\n",
    "\n",
    "\n",
    "def makeBoxPairs(O):\n",
    "    return(list(itertools.combinations(O, 2)))\n",
    "\n",
    "\n",
    "def renameAxes(axes, rD):\n",
    "    try:\n",
    "        N = len(axes)\n",
    "    except:\n",
    "        axes = [axes]\n",
    "        N = 1\n",
    "    for i in range(N):\n",
    "        # set xticks\n",
    "        xticksTextObject = axes[i].get_xticklabels()\n",
    "        xticksList = [xticksTextObject[j].get_text() for j in range(len(xticksTextObject))]\n",
    "        test_hasXLabels = (len(''.join(xticksList)) > 0)\n",
    "        if test_hasXLabels:\n",
    "            newXticksList = [rD.get(k, k) for k in xticksList]\n",
    "            axes[i].set_xticklabels(newXticksList)\n",
    "        \n",
    "        # set xlabel\n",
    "        xlabel = axes[i].get_xlabel()\n",
    "        newXlabel = rD.get(xlabel, xlabel)\n",
    "        axes[i].set_xlabel(newXlabel)\n",
    "        # set ylabel\n",
    "        ylabel = axes[i].get_ylabel()\n",
    "        newYlabel = rD.get(ylabel, ylabel)\n",
    "        axes[i].set_ylabel(newYlabel)\n",
    "        \n",
    "\n",
    "def addStat(ax, data, box_pairs, param, cond, test = 'MannWhitney', percentHeight = 98):\n",
    "    refHeight = np.percentile(data[param].values, percentHeight)\n",
    "    currentHeight = refHeight\n",
    "    scale = ax.get_yscale()\n",
    "    xTicks = ax.get_xticklabels()\n",
    "    dictXTicks = {xTicks[i].get_text() : xTicks[i].get_position()[0] for i in range(len(xTicks))}\n",
    "    for bp in box_pairs:\n",
    "        c1 = data[data[cond] == bp[0]][param].values\n",
    "        c2 = data[data[cond] == bp[1]][param].values\n",
    "        if test=='Mann-Whitney':\n",
    "            statistic, pval = st.mannwhitneyu(c1,c2)\n",
    "        elif test=='t-test':\n",
    "            statistic, pval = st.ttest_ind(c1,c2)\n",
    "        text = 'ns'\n",
    "        if pval < 0.05 and pval > 0.01:\n",
    "            text = '*'\n",
    "        elif pval < 0.01 and pval > 0.001:\n",
    "            text = '**'\n",
    "        elif pval < 0.001 and pval < 0.001:\n",
    "            text = '***'\n",
    "        elif pval < 0.0001:\n",
    "            text = '****'\n",
    "        ax.plot([bp[0], bp[1]], [currentHeight, currentHeight], 'k-', lw = 1)\n",
    "        XposText = (dictXTicks[bp[0]]+dictXTicks[bp[1]])/2\n",
    "        if scale == 'log':\n",
    "            power = 0.006 * (text=='ns') + 0.000 * (text!='ns')\n",
    "            YposText = currentHeight*(refHeight**power)\n",
    "        else:\n",
    "            factor = 0.025 * (text=='ns') + 0.000 * (text!='ns')\n",
    "            YposText = currentHeight + factor*refHeight\n",
    "        ax.text(XposText, YposText, text, ha = 'center')\n",
    "#         if text=='ns':\n",
    "#             ax.text(posText, currentHeight + 0.025*refHeight, text, ha = 'center')\n",
    "#         else:\n",
    "#             ax.text(posText, currentHeight, text, ha = 'center')\n",
    "        if scale == 'log':\n",
    "            currentHeight = currentHeight*(refHeight**0.05)\n",
    "        else:\n",
    "            currentHeight =  currentHeight + 0.15*refHeight\n",
    "    ax.set_ylim([ax.get_ylim()[0], currentHeight])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of the averaging per cell routine\n",
    "\n",
    "# data = GlobalTable_meca\n",
    "# CondCol='drug'\n",
    "# Parameters=['SurroundingThickness','EChadwick']\n",
    "# Filters = [(GlobalTable_meca['Validated'] == 1)]\n",
    "# AvgPerCell=True\n",
    "# cellID='CellName'\n",
    "\n",
    "# data_filtered = data\n",
    "# for fltr in Filters:\n",
    "#     data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "# group = data_filtered.groupby(cellID)\n",
    "# dictAggMean = getDictAggMean(data_filtered)\n",
    "# data_filtered = group.agg(dictAggMean.pop(cellID)) #.reset_index(level=0, inplace=True)\n",
    "# data_filtered.reset_index(level=0, inplace=True)\n",
    "# data_filtered=data_filtered[[cellID]+[CondCol]+Parameters]\n",
    "# print(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of a routine to remove points of a list of XY positions where at least 1 of the coordinates is 'nan'\n",
    "\n",
    "# XYraw = np.array([[np.nan, 2, 3, np.nan, 5], [10,20,30,40,50]])\n",
    "# XYraw = XYraw.T\n",
    "# XY = XYraw[~np.isnan(XYraw).any(axis=1), :]\n",
    "# X, Y = XY[:,0], XY[:,1]\n",
    "# X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of a routine to double each element in a list ; example [1, 2, 3] -> [1, 1, 2, 2, 3, 3]\n",
    "\n",
    "# newnew_color_list = np.array([new_color_list, new_color_list])\n",
    "# custom_color_list = list(np.array([new_color_list, new_color_list]).flatten(order='F'))\n",
    "# custom_color_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of makeOrder function\n",
    "\n",
    "# print(makeOrder(['none','doxycyclin'],['BSA coated glass','20um fibronectin discs']))\n",
    "# print(makeOrder(['A','B']))\n",
    "# print(makeOrder(['A','B'], ['C','D']))\n",
    "# print(makeOrder(['A','B'], ['C','D'], ['E','F']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Test of makeBoxPairs function\n",
    "\n",
    "# O = makeOrder(['none','doxycyclin'],['BSA coated glass','20um fibronectin discs'])\n",
    "# makeBoxPairs(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawMecaTable = getGlobalTable_meca()\n",
    "# rawMecaTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rawMecaTable.loc[]\n",
    "# Agréger tous les M450 WT sous un même nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M270') | (rawMecaTable['ExpType'] == 'DictyDB_M450')), (rawMecaTable['TpsComp'] == '1s')]\n",
    "# Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M450')), (rawMecaTable['TpsComp'] == '1s')]\n",
    "fig, ax = D1Plot(rawMecaTable, CondCol=['ExpType'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('M450 vs M270 for tps comp = 1s')\n",
    "fig.show()\n",
    "# rawMecaTable[Filters[0] & Filters[1] & Filters[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M450'))] #  | (rawMecaTable['ExpType'] == 'DictyDB_M450-Multi')\n",
    "fig, ax00 = D1Plot(rawMecaTable, CondCol=['TpsComp'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                   Filters=Filters,AvgPerCell=False,cellID='CellName',stats=False,figSizeFactor = 1.8,markerSizeFactor=0.5)\n",
    "fig.suptitle('M450, various rates')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(rawMecaTable['Validated'] == 1), ((rawMecaTable['ExpType'] == 'DictyDB_M270'))]\n",
    "fig, ax01 = D1Plot(rawMecaTable, CondCol=['TpsComp'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "                   AvgPerCell=False,cellID='CellName',stats=False,figSizeFactor = 1.8,markerSizeFactor=0.5)\n",
    "fig.suptitle('M270, various rates')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['medianThickness'] <= 1000)]\n",
    "fig, ax = D1Plot(GlobalTable_ctField, CondCol=['drug','substrate'],Parameters=['medianThickness','fluctuAmpli'],Filters=Filters)\n",
    "fig.suptitle('3T3aSFL on patterns: Ct Field')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['none','doxycyclin'],['BSA coated glass','20um fibronectin discs']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['drug','substrate'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName', co_order=co_order)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['drug','substrate'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "fig.suptitle('3T3aSFL on patterns: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder([['aSFL','aSFL-6FP'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder([['aSFL','aSFL-6FP'],['none','doxycyclin']])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order)\n",
    "fig.suptitle('3T3aSFL SHORT vs LONG linker: Compressions')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = ['drug'], Filters=Filters, modelFit=True)\n",
    "fig.suptitle('3T3aSFL - Dh = f(H)')\n",
    "archiveFig(fig, ax, name='aSFL_Dh(h)_drug', figDir = todayFigDir, figSubDir='ThicknessPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Same as above without the 2 thickest cells\n",
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['cell subtype'] == 'aSFL'), (GlobalTable_ctField['medianThickness'] <= 700)]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = ['drug'], Filters=Filters, modelFit=True)\n",
    "fig.suptitle('3T3aSFL - Dh = f(H)')\n",
    "archiveFig(fig, ax, name='aSFL_Dh(h)_drug_wo2LastPoints', figDir = todayFigDir, figSubDir='ThicknessPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='SurroundingThickness',YCol='EChadwick',CondCol = ['substrate','drug'],\\\n",
    "           Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=True, modelType='y=A*exp(kx)')\n",
    "fig.suptitle('3T3aSFL: E(h)')\n",
    "archiveFig(fig, ax, name='aSFL_E(h)_drug&substrate', figDir = todayFigDir, figSubDir='')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# xticksTextObject = ax.get_xticklabels()\n",
    "# xticksList = [xticksTextObject[j].get_text() for j in range(len(xticksTextObject))]\n",
    "# newXticksList = [renameDict1.get(k, k) for k in xticksList]\n",
    "# newXticksList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_mecaBis['validatedFit'] == True), (GlobalTable_mecaBis['validatedThickness'] == True), (GlobalTable_mecaBis['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder(['aSFL','aSFL-A8','aSFL-6FP'],['none','doxycyclin'])\n",
    "box_pairs=[('aSFL & none', 'aSFL & doxycyclin'),\n",
    " ('aSFL-A8 & none', 'aSFL-A8 & doxycyclin'),\n",
    " ('aSFL-6FP & none', 'aSFL-6FP & doxycyclin'),\n",
    " ('aSFL & none', 'aSFL-A8 & none'),\n",
    " ('aSFL & none', 'aSFL-6FP & none')]\n",
    "fig, ax = D1Plot(GlobalTable_mecaBis, CondCol=['cell subtype','drug'],Parameters=['surroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=False,cellID='cellID',co_order=co_order,box_pairs=box_pairs,stats=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL - All linker types: All Compressions')\n",
    "fig.show()\n",
    "# fig.savefig(todayFigDir + '//' + 'compressionsShortvsLongLinker_woStats.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "\n",
    "p = D1PlotInteractive(GlobalTable_ctField, CondCol='drug',Parameters=['medianThickness','fluctuAmpli'],Filters=Filters)\n",
    "p.children[0][0].title.text = '3T3aSFL on patterns: Ct Field'\n",
    "p.children[0][0].title.text_font_size = '16pt'\n",
    "p.children[1][0].title.text = ''\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D1PlotInteractive(GlobalTable_meca, CondCol='drug',Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,AvgPerCell=True,cellID='CellName')\n",
    "p.children[0][0].title.text = '3T3aSFL on patterns: Compressions'\n",
    "p.children[0][0].title.text_font_size = '14pt'\n",
    "p.children[1][0].title.text = ''\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = 'drug', Filters=Filters)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='SurroundingThickness',YCol='EChadwick',CondCol = 'drug', Filters=Filters, cellID = 'CellName')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True)]\n",
    "p = D2PlotInteractive(GlobalTable_ctField, XCol='meanFluoPeakAmplitude',YCol='medianThickness',CondCol = 'drug', Filters=Filters, cellID = 'cellID',AvgPerCell=True)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True)]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = '3T3aSFL expressing linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL expressing linker: E(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL expressing linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL-6FP expressing long linker: E(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "p = D2PlotInteractive(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='SurroundingThickness', CondCol = 'drug', Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "p.title.text = 'aSFL-6FP expressing long linker: H(fluo)'\n",
    "p.title.text_font_size = '16pt'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renameDict1 = {'SurroundingThickness':'Surrounding Thickness (nm)',\\\n",
    "               'surroundingThickness':'Surrounding Thickness (nm)',\\\n",
    "               'EChadwick': 'E Chadwick (Pa)',\\\n",
    "               'medianThickness': 'Median Thickness (nm)',\\\n",
    "               'fluctuAmpli': 'Fluctuations Amplitude (nm)',\\\n",
    "               'meanFluoPeakAmplitude' : 'Fluo Intensity (a.u.)',\\\n",
    "               'none & BSA coated glass':'control & non adherent',\\\n",
    "               'doxycyclin & BSA coated glass':'iMC & non adherent',\\\n",
    "               'none & 20um fibronectin discs':'control & adherent on fibro',\\\n",
    "               'doxycyclin & 20um fibronectin discs':'iMC & adherent on fibro',\\\n",
    "               'BSA coated glass & none':'control & non adherent',\\\n",
    "               'BSA coated glass & doxycyclin':'iMC & non adherent',\\\n",
    "               '20um fibronectin discs & none':'control & adherent on fibro',\\\n",
    "               '20um fibronectin discs & doxycyclin':'iMC & adherent on fibro',\\\n",
    "               'aSFL & none':'aSFL control',\\\n",
    "               'aSFL & doxycyclin':'aSFL iMC',\\\n",
    "               'aSFL-6FP & none':'aSFL-6FP control',\\\n",
    "               'aSFL-6FP & doxycyclin':'aSFL-6FP long-iMC',\\\n",
    "               'aSFL-A8 & none':'aSFL-A8 control',\\\n",
    "               'aSFL-A8 & doxycyclin':'aSFL-A8 iMC'}\n",
    "\n",
    "figSubDir = 'CleanPlots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thickness and module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder(['BSA coated glass','20um fibronectin discs'],['none','doxycyclin'])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['substrate','drug'],\\\n",
    "                 Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "                 AvgPerCell=True, cellID='CellName', co_order=co_order)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "archiveFig(fig, ax, name='3T3aSFL_substrate&drug_SurroundingThickness&EChadwick', figSubDir = figSubDir)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca_Py['validatedFit'] == True), (GlobalTable_meca_Py['validatedThickness'] == True), (GlobalTable_meca_Py['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder(['BSA coated glass','20um fibronectin discs'],['none','doxycyclin'])\n",
    "fig, ax = D1Plot(GlobalTable_meca_Py, CondCol=['substrate','drug'],\\\n",
    "                 Parameters=['surroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "                 AvgPerCell=True, cellID='cellID', co_order=co_order)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "archiveFig(fig, ax, name='3T3aSFL_substrate&drug_SurroundingThickness&EChadwick_NEWTABLE', figSubDir = figSubDir)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "# co_order = makeOrder(['BSA coated glass','20um fibronectin discs'],['none','doxycyclin'])\n",
    "# fig, ax = D1Plot(GlobalTable_meca, CondCol=['substrate','drug'],\\\n",
    "#                  Parameters=['EChadwick'],Filters=Filters,\\\n",
    "#                  AvgPerCell=True, cellID='CellName', co_order=co_order,statMethod = 'Mann-Whitney', statVerbose = 2)\n",
    "# renameAxes(ax,renameDict1)\n",
    "# fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "# fig.show()\n",
    "\n",
    "# # test value should be one of the following: t-test_ind, t-test_welch, t-test_paired, Mann-Whitney, Mann-Whitney-gt, Mann-Whitney-ls, Levene, Wilcoxon, Kruskal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "# co_order = makeOrder(['BSA coated glass','20um fibronectin discs'],['none','doxycyclin'])\n",
    "# fig, ax = D1Plot(GlobalTable_meca, CondCol=['substrate','drug'],\\\n",
    "#                  Parameters=['SurroundingThickness','EChadwick'],Filters=Filters,\\\n",
    "#                  AvgPerCell=True, cellID='CellName', co_order=co_order, stats=False)\n",
    "# renameAxes(ax,renameDict1)\n",
    "# fig.suptitle('3T3aSFL on diverse substrates: Compressions')\n",
    "# fig.show()\n",
    "# # fig.savefig(todayFigDir + '//' + 'compressionsBSAvsFibro_woStats.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['medianThickness'] <= 1000)]\n",
    "co_order = makeOrder(['20um fibronectin discs'],['none','doxycyclin'])\n",
    "fig, ax = D1Plot(GlobalTable_ctField, CondCol=['substrate','drug'],Parameters=['medianThickness','fluctuAmpli'],\\\n",
    "                 Filters=Filters,stats=True,co_order=co_order,figSizeFactor=0.5)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on patterns: Constant Field')\n",
    "archiveFig(fig, ax, name='3T3aSFL_drug_medianThickness', figSubDir = figSubDir)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca_Py['validatedThickness'] == True), (GlobalTable_meca_Py['ctFieldThickness'] <= 1000)]\n",
    "co_order = makeOrder(['aSFL','aSFL-A8','aSFL-6FP'],['none','doxycyclin'])\n",
    "box_pairs=[('aSFL & none', 'aSFL & doxycyclin'),\n",
    " ('aSFL-A8 & none', 'aSFL-A8 & doxycyclin'),\n",
    " ('aSFL-6FP & none', 'aSFL-6FP & doxycyclin'),\n",
    " ('aSFL & none', 'aSFL-A8 & none'),\n",
    " ('aSFL & none', 'aSFL-6FP & none')]\n",
    "fig, ax = D1Plot(GlobalTable_meca_Py, CondCol=['cell subtype','drug'],Parameters=['ctFieldThickness','ctFieldFluctuAmpli'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,stats=True,co_order=co_order,box_pairs=box_pairs,figSizeFactor=1)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL on patterns: H and DH from meca expe')\n",
    "archiveFig(fig, ax, name='3T3aSFL_drug_medianThickness_fromMeca_PYTHONTABLE', figSubDir = figSubDir)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder(['aSFL','aSFL-6FP'],['none','doxycyclin'])\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order,stats=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL short vs long linker: Compressions')\n",
    "archiveFig(fig, ax, name='3T3aSFL_likerType&drug_SurroundingThickness&EChadwick', figSubDir = figSubDir)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder(['aSFL','aSFL-A8','aSFL-6FP'],['none','doxycyclin'])\n",
    "box_pairs=[('aSFL & none', 'aSFL & doxycyclin'),\n",
    " ('aSFL-A8 & none', 'aSFL-A8 & doxycyclin'),\n",
    " ('aSFL-6FP & none', 'aSFL-6FP & doxycyclin'),\n",
    " ('aSFL & none', 'aSFL-A8 & none'),\n",
    " ('aSFL & none', 'aSFL-6FP & none')]\n",
    "fig, ax = D1Plot(GlobalTable_meca, CondCol=['cell subtype','drug'],Parameters=['SurroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,cellID='CellName',co_order=co_order,box_pairs=box_pairs,stats=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL - All linker types: Compressions')\n",
    "archiveFig(fig, ax, name='3T3aSFL_likerType&drug_SurroundingThickness&EChadwick', figSubDir = figSubDir)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca_Py['validatedFit'] == True), (GlobalTable_meca_Py['validatedThickness'] == True), (GlobalTable_meca_Py['substrate'] == '20um fibronectin discs')]\n",
    "co_order = makeOrder(['aSFL','aSFL-A8','aSFL-6FP'],['none','doxycyclin'])\n",
    "box_pairs=[('aSFL & none', 'aSFL & doxycyclin'),\n",
    " ('aSFL-A8 & none', 'aSFL-A8 & doxycyclin'),\n",
    " ('aSFL-6FP & none', 'aSFL-6FP & doxycyclin'),\n",
    " ('aSFL & none', 'aSFL-A8 & none'),\n",
    " ('aSFL & none', 'aSFL-6FP & none')]\n",
    "fig, ax = D1Plot(GlobalTable_meca_Py, CondCol=['cell subtype','drug'],Parameters=['surroundingThickness','EChadwick'],\\\n",
    "                 Filters=Filters,AvgPerCell=True,cellID='cellID',co_order=co_order,box_pairs=box_pairs,stats=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('3T3aSFL - All linker types: Compressions')\n",
    "archiveFig(fig, ax, name='3T3aSFL_likerType&drug_SurroundingThickness&EChadwick_PYTHONTABLE', figSubDir = figSubDir)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = ['drug'], Filters=Filters, modelFit=True)\n",
    "fig.suptitle('3T3aSFL - Dh = f(H)')\n",
    "archiveFig(fig, ax, name='aSFL_Dh(h)_drug', figDir = todayFigDir, figSubDir='ThicknessPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above without the 2 thickest cells\n",
    "Filters = [(GlobalTable_ctField['validated'] == True), (GlobalTable_ctField['cell subtype'] == 'aSFL'), (GlobalTable_ctField['medianThickness'] <= 700)]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='medianThickness',YCol='fluctuAmpli',CondCol = ['drug'], Filters=Filters, modelFit=True)\n",
    "fig.suptitle('3T3aSFL - Dh = f(H)')\n",
    "archiveFig(fig, ax, name='aSFL_Dh(h)_drug_wo2LastPoints', figDir = todayFigDir, figSubDir='ThicknessPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='SurroundingThickness',YCol='EChadwick',CondCol = ['substrate','drug'],\\\n",
    "           Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=False, modelType='y=A*exp(kx)')\n",
    "fig.suptitle('3T3aSFL: E(h)')\n",
    "archiveFig(fig, ax, name='aSFL_E(h)_drug&substrate', figDir = todayFigDir, figSubDir='')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation with fluorescence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['drug'], \\\n",
    "                 Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL_iMC_E(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['drug'], \\\n",
    "                 Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL_iMC_E(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above without the lonely point\n",
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL'), (GlobalTable_meca['EChadwick'] <= 80000)]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['drug'], \\\n",
    "                 Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL_iMC_E(fluo)_woLastPoint', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_mecaBis['validatedFit'] == True), ((GlobalTable_mecaBis['cell subtype'] == 'aSFL') | (GlobalTable_mecaBis['cell subtype'] == 'aSFL-A8')), (GlobalTable_mecaBis['drug'] == 'doxycyclin')]\n",
    "fig, ax = D2Plot(GlobalTable_mecaBis, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['cell subtype'], \\\n",
    "                 Filters=Filters, cellID = 'cellID', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL & aSFL-A8 expressing linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL&A8_iMC_E(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above without the lonely point\n",
    "Filters = [(GlobalTable_mecaBis['validatedFit'] == True), (GlobalTable_mecaBis['drug'] == 'doxycyclin'), \\\n",
    "           ((GlobalTable_mecaBis['cell subtype'] == 'aSFL') | (GlobalTable_mecaBis['cell subtype'] == 'aSFL-A8')), \\\n",
    "           (GlobalTable_mecaBis['meanFluoPeakAmplitude'] <= 1200), (GlobalTable_mecaBis['EChadwick'] >= 2000)]\n",
    "fig, ax = D2Plot(GlobalTable_mecaBis, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['cell subtype'], \\\n",
    "                 Filters=Filters, cellID = 'cellID', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL & aSFL-A8 expressing linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL&A8_iMC_E(fluo)_fluo-1200_&_E+2000', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude',YCol='SurroundingThickness',CondCol = ['drug'], Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: H(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL_iMC_H(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_ctField['validated'] == True)]\n",
    "fig, ax = D2Plot(GlobalTable_ctField, XCol='meanFluoPeakAmplitude',YCol='medianThickness',CondCol = ['drug'],\\\n",
    "                 Filters=Filters, cellID = 'cellID', AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL expressing linker: medianH(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL_iMC_medianH(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['drug'], \\\n",
    "                 Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL-6FP expressing long linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL-6FP_iMC_E(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above without the lonely point\n",
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP'), (GlobalTable_meca['meanFluoPeakAmplitude'] <= 1000)]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='EChadwick', CondCol = ['drug'], \\\n",
    "                 Filters=Filters, cellID = 'CellName', AvgPerCell=True, modelFit=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL-6FP expressing long linker: E(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL-6FP_iMC_E(fluo)_woLastPoint', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Filters = [(GlobalTable_meca['Validated'] == True), (GlobalTable_meca['cell subtype'] == 'aSFL-6FP')]\n",
    "fig, ax = D2Plot(GlobalTable_meca, XCol='meanFluoPeakAmplitude', YCol='SurroundingThickness', CondCol = ['drug'],\\\n",
    "                 Filters=Filters, cellID = 'CellName',AvgPerCell=True)\n",
    "renameAxes(ax,renameDict1)\n",
    "fig.suptitle('aSFL-6FP expressing long linker: H(fluo)')\n",
    "archiveFig(fig, ax, name='aSFL-6FP_iMC_H(fluo)', figDir = todayFigDir, figSubDir='FluoPlots')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create a test table to test different fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Script !\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "testFileName = 'testFitCompression.txt'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "list_mecaFiles = [f for f in os.listdir(timeSeriesDataDir) \\\n",
    "                  if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".csv\") \\\n",
    "                  and ('R40' in f))] # Change to allow different formats in the future\n",
    "expDf = getExperimentalConditions()\n",
    "tableDictTest = {}\n",
    "\n",
    "for f in list_mecaFiles:\n",
    "    print(f)\n",
    "    tS_DataFilePath = os.path.join(timeSeriesDataDir, f)\n",
    "    tsDF = pd.read_csv(tS_DataFilePath, ';')\n",
    "    \n",
    "    split_f = f.split('_')\n",
    "    tsDF.dx, tsDF.dy, tsDF.dz, tsDF.D2, tsDF.D3 = tsDF.dx*1000, tsDF.dy*1000, tsDF.dz*1000, tsDF.D2*1000, tsDF.D3*1000\n",
    "    thisManipID = split_f[0] + '_' + split_f[1]\n",
    "    expDf['manipID'] = expDf['date'] + '_' + expDf['manip']\n",
    "    thisExpDf = expDf.loc[expDf['manipID'] == thisManipID]\n",
    "    DIAMETER = thisExpDf.at[thisExpDf.index.values[0], 'bead diameter']\n",
    "    thisCellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "    Ncomp = max(tsDF['idxCompression'])\n",
    "    \n",
    "    for i in range(1, Ncomp+1): #Ncomp+1):\n",
    "        thisCompHiD = thisCellID + '__' + str(i) + '__h'\n",
    "        thisCompFiD = thisCellID + '__' + str(i) + '__f'\n",
    "        print(thisCompHiD)\n",
    "        \n",
    "        thisCompDf = tsDF.loc[tsDF['idxCompression'] == i, :]\n",
    "        iStart = (findFirst(tsDF['idxCompression'], i))\n",
    "        iStop = iStart + thisCompDf.shape[0]\n",
    "        \n",
    "        # Delimit the start of the increase of B (typically the moment when the field decrease from 5 to 3)\n",
    "        # and the end of its decrease (typically when it goes back from 3 to 5)\n",
    "        \n",
    "        listB = thisCompDf.B.values\n",
    "        offsetStart, offsetStop = 0, 0\n",
    "        minB, maxB = min(listB), max(listB)\n",
    "        thresholdB = (maxB-minB)/50\n",
    "        k = 0\n",
    "        \n",
    "        while (listB[k] > minB+thresholdB) or (listB[-1-k] > minB+thresholdB):\n",
    "            offsetStart += int(listB[k] > minB+thresholdB)\n",
    "            k += 1\n",
    "        \n",
    "        jStart = offsetStart\n",
    "        jMax = np.argmax(thisCompDf.B)\n",
    "        \n",
    "        hCompr = (thisCompDf.D3.values[jStart:jMax+1] - DIAMETER)\n",
    "        fCompr = (thisCompDf.F.values[jStart:jMax+1])\n",
    "        \n",
    "        tableDictTest[thisCompHiD] = hCompr\n",
    "        tableDictTest[thisCompFiD] = fCompr\n",
    "        \n",
    "saveFile = open(testFilePath, 'w')\n",
    "for k in tableDictTest.keys():\n",
    "    saveFile.write(k)\n",
    "    for i in range(len(tableDictTest[k])):\n",
    "        saveFile.write(';')\n",
    "        saveFile.write(str(tableDictTest[k][i]))\n",
    "    saveFile.write('\\n')\n",
    "saveFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create a test table to try statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Script !\n",
    "\n",
    "# Create a test table to try statistical tests\n",
    "\n",
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "GlobalTable_meca = getGlobalTable_meca()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "table_fluo = getFluoData()\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_ExpConditions, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "GlobalTable_meca = pd.merge(GlobalTable_meca, table_fluo, how=\"left\", left_on='CellName', right_on='cellID'\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "print('Merged table has ' + str(GlobalTable_meca.shape[0]) + ' lines and ' + str(GlobalTable_meca.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Table 1\n",
    "\n",
    "testFileName = 'testStats01_allComp.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['BSA coated glass','20um fibronectin discs'],['none','doxycyclin']])\n",
    "data = GlobalTable_meca\n",
    "CondCol=['substrate','drug']\n",
    "Parameters=['SurroundingThickness','EChadwick']\n",
    "AvgPerCell=False\n",
    "cellID='CellName'\n",
    "\n",
    "data_filtered = data\n",
    "for fltr in Filters:\n",
    "    data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "NCond = len(CondCol)    \n",
    "if NCond == 1:\n",
    "    CondCol = CondCol[0]\n",
    "elif NCond > 1:\n",
    "    newColName = ''\n",
    "    for i in range(NCond):\n",
    "        newColName += CondCol[i]\n",
    "        newColName += ' & '\n",
    "    newColName = newColName[:-3]\n",
    "    data_filtered[newColName] = ''\n",
    "    for i in range(NCond):\n",
    "        data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "    data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "    CondCol = newColName\n",
    "    \n",
    "if AvgPerCell:\n",
    "    group = data_filtered.groupby(cellID)\n",
    "    dictAggMean = getDictAggMean(data_filtered)\n",
    "    data_filtered = group.agg(dictAggMean)\n",
    "\n",
    "data_filtered.sort_values(CondCol, axis=0, ascending=True, inplace=True)\n",
    "\n",
    "df_output = data_filtered[[cellID, 'CompNum', newColName] + Parameters]\n",
    "\n",
    "# df_output.to_csv(testFilePath)\n",
    "\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Table 2\n",
    "\n",
    "testFileName = 'testStats02_avgPerCell.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "Filters = [(GlobalTable_meca['Validated'] == 1), (GlobalTable_meca['cell subtype'] == 'aSFL')]\n",
    "co_order = makeOrder([['BSA coated glass','20um fibronectin discs'],['none','doxycyclin']])\n",
    "data = GlobalTable_meca\n",
    "CondCol=['substrate','drug']\n",
    "Parameters=['SurroundingThickness','EChadwick']\n",
    "AvgPerCell=True\n",
    "cellID='CellName'\n",
    "\n",
    "data_filtered = data\n",
    "for fltr in Filters:\n",
    "    data_filtered = data_filtered.loc[fltr]\n",
    "\n",
    "NCond = len(CondCol)    \n",
    "if NCond == 1:\n",
    "    CondCol = CondCol[0]\n",
    "elif NCond > 1:\n",
    "    newColName = ''\n",
    "    for i in range(NCond):\n",
    "        newColName += CondCol[i]\n",
    "        newColName += ' & '\n",
    "    newColName = newColName[:-3]\n",
    "    data_filtered[newColName] = ''\n",
    "    for i in range(NCond):\n",
    "        data_filtered[newColName] += data_filtered[CondCol[i]].astype(str)\n",
    "        data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x + ' & ')\n",
    "    data_filtered[newColName] = data_filtered[newColName].apply(lambda x : x[:-3])\n",
    "    CondCol = newColName\n",
    "    \n",
    "if AvgPerCell:\n",
    "    group = data_filtered.groupby(cellID)\n",
    "    dictAggMean = getDictAggMean(data_filtered)\n",
    "    data_filtered = group.agg(dictAggMean)\n",
    "\n",
    "data_filtered.sort_values(CondCol, axis=0, ascending=True, inplace=True)\n",
    "\n",
    "df_output = data_filtered[[cellID, 'CompNum', newColName] + Parameters]\n",
    "\n",
    "# df_output.to_csv(testFilePath)\n",
    "\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Table 3\n",
    "# Fake data\n",
    "\n",
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "\n",
    "testFileName = 'testStats03_FakeData.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "Npop = 10\n",
    "Npoints = 30\n",
    "minAvg = 100\n",
    "maxAvg = 600\n",
    "step = (maxAvg - minAvg)/(Npop-1)\n",
    "std = 250\n",
    "dictFakeData = {}\n",
    "np.random.seed(11)\n",
    "\n",
    "for i in [1, 2, 3, 10]:\n",
    "    dictFakeData['Distribution_' + str(i)] = np.random.normal(loc=minAvg + step*(i-1), scale=std, size=Npoints)\n",
    "\n",
    "dfFakeData = pd.DataFrame(dictFakeData)\n",
    "\n",
    "# dfFakeData.to_csv(testFilePath)\n",
    "\n",
    "dfFakeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Table 4\n",
    "# Fake data 2\n",
    "\n",
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "\n",
    "testFileName = 'testStats04_FakeDataLarge.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "Npop = 10\n",
    "Npoints = 300\n",
    "minAvg = 100\n",
    "maxAvg = 600\n",
    "step = (maxAvg - minAvg)/(Npop-1)\n",
    "std = 250\n",
    "dictFakeData = {}\n",
    "np.random.seed(11)\n",
    "\n",
    "for i in [1, 2, 3, 10]:\n",
    "    dictFakeData['Distribution_' + str(i)] = np.random.normal(loc=minAvg + step*(i-1), scale=std, size=Npoints)\n",
    "\n",
    "dfFakeData = pd.DataFrame(dictFakeData)\n",
    "\n",
    "# dfFakeData.to_csv(testFilePath)\n",
    "\n",
    "dfFakeData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Comparison of stat tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### With the fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "\n",
    "# Table 3\n",
    "# Fake data\n",
    "testFileName = 'testStats03_FakeData.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "dfFakeData = pd.read_csv(testFilePath)\n",
    "dfFakeData = dfFakeData.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "Ncol = len(dfFakeData.columns)\n",
    "\n",
    "refCol = dfFakeData[dfFakeData.columns[0]]\n",
    "boxPlotMatrix = []\n",
    "\n",
    "for i in range(0,Ncol):\n",
    "    boxPlotMatrix.append(dfFakeData[dfFakeData.columns[i]].values)\n",
    "    if i > 0:\n",
    "        print('Comparison between distribution 1 and ' + str(i+1))\n",
    "        tTest = st.ttest_ind(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "        print('tTest : ' + str(tTest.pvalue))\n",
    "        wilcox = st.wilcoxon(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "        print('wilcox : ' + str(wilcox.pvalue))\n",
    "        mannwhitneyu = st.mannwhitneyu(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "        print('mannwhitneyu : ' + str(mannwhitneyu.pvalue))\n",
    "        print('')\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.boxplot(boxPlotMatrix,labels=dfFakeData.columns.values)\n",
    "ax.tick_params(axis='x', labelrotation = 15, labelsize = 7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "\n",
    "# Table 4\n",
    "# Fake data Large\n",
    "testFileName = 'testStats04_FakeDataLarge.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "dfFakeData = pd.read_csv(testFilePath)\n",
    "dfFakeData = dfFakeData.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "Ncol = len(dfFakeData.columns)\n",
    "\n",
    "refCol = dfFakeData[dfFakeData.columns[0]]\n",
    "boxPlotMatrix = []\n",
    "\n",
    "for i in range(0,Ncol):\n",
    "    boxPlotMatrix.append(dfFakeData[dfFakeData.columns[i]].values)\n",
    "    if i > 0:\n",
    "        print('Comparison between distribution 1 and ' + str(i+1))\n",
    "        tTest = st.ttest_ind(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "        print('tTest : ' + str(tTest.pvalue))\n",
    "        wilcox = st.wilcoxon(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "        print('wilcox : ' + str(wilcox.pvalue))\n",
    "        mannwhitneyu = st.mannwhitneyu(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "        print('mannwhitneyu : ' + str(mannwhitneyu.pvalue))\n",
    "        print('')\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.boxplot(boxPlotMatrix,labels=dfFakeData.columns.values)\n",
    "ax.tick_params(axis='x', labelrotation = 15, labelsize = 7)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Summary of results: numpy.random.seed = 11\n",
    "    \n",
    "dictResultsFakeData = {}\n",
    "dictResultsFakeData['language'] = ['python'   , 'python', 'python'      , 'R'     , 'R'          , 'Matlab' , 'Matlab' ]\n",
    "dictResultsFakeData['test']     = ['ttest_ind', 'wilcox', 'mannwhitneyu', 't.test', 'wilcox.test', 'ttest2' , 'ranksum']\n",
    "dictResultsFakeData['1 vs 2']   = [0.2700     , 0.2711  , 0.1353        , 0.2701  , 0.2729       , 0.2700   , 0.2707   ]\n",
    "dictResultsFakeData['1 vs 3']   = [0.0714     , 0.0822  , 0.0452        , 0.0715  , 0.0906       , 0.0714   , 0.0905   ]\n",
    "dictResultsFakeData['1 vs 10']  = [1.33e-11   , 4.28e-06, 2.98e-09      , 1.44e-11, 5.799e-11    , 1.33e-11 , 5.96e-09 ]\n",
    "dfResultFakeData = pd.DataFrame(dictResultsFakeData)\n",
    "dfResultFakeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Summary of results: numpy.random.seed = 11\n",
    "\n",
    "dictResultsFakeDataLarge = {}\n",
    "dictResultsFakeDataLarge['language'] = ['python'   , 'python', 'python'      , 'R'     , 'R'          , 'Matlab' , 'Matlab' ]\n",
    "dictResultsFakeDataLarge['test']     = ['ttest_ind', 'wilcox', 'mannwhitneyu', 't.test', 'wilcox.test', 'ttest2' , 'ranksum']\n",
    "dictResultsFakeDataLarge['1 vs 2']   = [0.0082     , 0.0049  , 0.0038        , 0.0082  , 0.0077       , 0.0082   , 0.0077   ]\n",
    "dictResultsFakeDataLarge['1 vs 3']   = [1.26e-06   , 9.29e-06, 1.37e-06      , 1.27e-06, 2.75e-06     , 1.26e-06 , 2.74e-06 ]\n",
    "dictResultsFakeDataLarge['1 vs 10']  = [1.74e-98   , 5.64e-48, 6.39e-74      , 2.2e-16 , 2.2e-16      , 1.74e-98 , 1.27e-73 ]\n",
    "dictResultsFakeDataLarge = pd.DataFrame(dictResultsFakeDataLarge)\n",
    "dictResultsFakeDataLarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### With real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# testDir = os.path.join(dataDir, 'TestDataSet')\n",
    "\n",
    "# Table 1\n",
    "# Avg Per Cell\n",
    "testFileName = 'testStats01_AvgPerCell.csv'\n",
    "testFilePath = os.path.join(testDir, testFileName)\n",
    "\n",
    "dfAvgPerCell = pd.read_csv(testFilePath)\n",
    "# dfAvgPerCell = dfAvgPerCell.drop(columns=['Unnamed: 0'])\n",
    "dfAvgPerCell\n",
    "categories = list(dfAvgPerCell['substrate & drug'].unique())\n",
    "Ncat = len(categories)\n",
    "for i in range(Ncat):\n",
    "    for j in range(i,Ncat):\n",
    "        if j != i:\n",
    "            x = dfAvgPerCell.loc[dfAvgPerCell['substrate & drug'] == categories[i], 'EChadwick'].values\n",
    "            y = dfAvgPerCell.loc[dfAvgPerCell['substrate & drug'] == categories[j], 'EChadwick'].values\n",
    "            print('Comparison between ' + categories[i] + ' and ' + categories[j])\n",
    "            tTest = st.ttest_ind(x, y)\n",
    "            print('tTest : ' + str(tTest.pvalue))\n",
    "            mannwhitneyu = st.mannwhitneyu(x, y)\n",
    "            print('mannwhitneyu : ' + str(mannwhitneyu.pvalue))\n",
    "            print('')\n",
    "\n",
    "# refCol = dfFakeData[dfFakeData.columns[0]]\n",
    "# boxPlotMatrix = []\n",
    "\n",
    "# for i in range(0,Ncol):\n",
    "#     boxPlotMatrix.append(dfFakeData[dfFakeData.columns[i]].values)\n",
    "#     if i > 0:\n",
    "#         print('Comparison between distribution 1 and ' + str(i+1))\n",
    "#         tTest = st.ttest_ind(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "#         print('tTest : ' + str(tTest.pvalue))\n",
    "#         wilcox = st.wilcoxon(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "#         print('wilcox : ' + str(wilcox.pvalue))\n",
    "#         mannwhitneyu = st.mannwhitneyu(refCol.values, dfFakeData[dfFakeData.columns[i]].values)\n",
    "#         print('mannwhitneyu : ' + str(mannwhitneyu.pvalue))\n",
    "#         print('')\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "# ax.boxplot(boxPlotMatrix,labels=dfFakeData.columns.values)\n",
    "# ax.tick_params(axis='x', labelrotation = 15, labelsize = 7)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dictResultsAvgPerCell = {}\n",
    "order = ['language','test','c & NA vs iMC & A','c & NA vs c & A','iMC & NA vs iMC & A','c & NA vs iMC & NA','iMC & NA vs c & A','c & A vs iMC & A']\n",
    "dictResultsAvgPerCell['language']            = ['python_statAnot'   , 'python_statAnot' ,'python'   , 'python', 'R'      , 'R'          , 'Matlab' , 'Matlab' ]\n",
    "dictResultsAvgPerCell['test']                = ['ttest_ind', 'mannwhitneyu', 'ttest_ind', 'mannwhitneyu', 't.test' , 'wilcox.test', 'ttest2' , 'ranksum']\n",
    "dictResultsAvgPerCell['c & NA vs iMC & NA']  = [1.000e+00  , 5.669e-01     ,0.19226082553146542  , 0.047244130659518     , 0.1503   , 0.09502       , 0.1923       , 0.0945]\n",
    "dictResultsAvgPerCell['iMC & NA vs c & A']   = [9.673e-02  , 8.625e-03     ,0.016121864893694285  ,0.0007187494204219925     , 0.04082  , 0.0009726    , 0.0161     , 0.0014]\n",
    "dictResultsAvgPerCell['c & A vs iMC & A']    = [2.331e-02  , 1.376e-01     ,0.00388458593467288  , 0.01146586677766893     , 0.007326 , 0.02214      , 0.0039      , 0.0229]\n",
    "dictResultsAvgPerCell['c & NA vs c & A']     = [1.000e+00  , 1.000e+00     ,0.6977550928576132  , 0.2884535746840493     , 0.6948   , 0.5838       , 0.6978     , 0.5769]\n",
    "dictResultsAvgPerCell['iMC & NA vs iMC & A'] = [1.000e+00  , 1.000e+00     ,0.5573451346686198  , 0.41831870120029446,  0.5031  , 0.8387       , 0.5573      , 0.8366 ]\n",
    "dictResultsAvgPerCell['c & NA vs iMC & A']   = [9.726e-01  , 1.000e+00     ,0.16209530366557973  , 0.14893352365754048     , 0.04353  , 0.3043       , 0.1621       , 0.2979 ]\n",
    "dfResultsAvgPerCell = pd.DataFrame(dictResultsAvgPerCell)\n",
    "dfResultsAvgPerCell[order]\n",
    "\n",
    "# dfResultsAvgPerCell = dfResultsAvgPerCell.sort_values(by='test',ascending=False)\n",
    "# dfResultsAvgPerCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data formatting and filtering\n",
    "\n",
    "TBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mecaDataFile = 'Global_MecaData.csv'\n",
    "mecaDataFilePath = os.path.join(dataDir, mecaDataFile)\n",
    "mecaDF = pd.read_csv(mecaDataFilePath, sep=';')\n",
    "print('Extracted a table with ' + str(mecaDF.shape[0]) + ' lines and ' + str(mecaDF.shape[1]) + ' columns.')\n",
    "\n",
    "mecaDF = mecaDF.rename(columns={\"CellID\": \"CellName\", \"CellName\": \"CellID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mecaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "experimentalDataFile = 'ExperimentalConditions.csv'\n",
    "experimentalDataFilePath = os.path.join(experimentalDataDir, experimentalDataFile)\n",
    "expConditionsDF = pd.read_csv(experimentalDataFilePath, sep=';',header=0)\n",
    "print('Extracted a table with ' + str(expConditionsDF.shape[0]) + ' lines and ' + str(expConditionsDF.shape[1]) + ' columns.')\n",
    "\n",
    "# Cleaning the table\n",
    "try:\n",
    "    expConditionsDF = expConditionsDF.convert_dtypes()\n",
    "\n",
    "    listTextColumns = []\n",
    "    for col in expConditionsDF.columns:\n",
    "        if expConditionsDF[col].dtype == 'string':\n",
    "            listTextColumns.append(col)\n",
    "\n",
    "    expConditionsDF[listTextColumns] = expConditionsDF[listTextColumns].apply(lambda x: x.str.replace(',','.'))\n",
    "\n",
    "    expConditionsDF['scale pixel per um'] = expConditionsDF['scale pixel per um'].astype(float)\n",
    "    expConditionsDF['optical index correction'] = \\\n",
    "              expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[0]).astype(float) \\\n",
    "            / expConditionsDF['optical index correction'].apply(lambda x: x.split('/')[1]).astype(float)\n",
    "    expConditionsDF['magnetic field correction'] = expConditionsDF['magnetic field correction'].astype(float)\n",
    "    expConditionsDF['with fluo images'] = expConditionsDF['with fluo images'].astype(bool)\n",
    "\n",
    "    expConditionsDF['ramp field'] = \\\n",
    "    expConditionsDF['ramp field'].apply(lambda x: [x.split(';')[0], x.split(';')[1]] if not pd.isnull(x) else [])\n",
    "\n",
    "except:\n",
    "    print('Unexpected bug with the cleaning step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expConditionsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Unused for now\n",
    "cellDescriptionDataFile = 'CellDescription.csv'\n",
    "cellDescriptionDataFilePath = os.path.join(experimentalDataDir, cellDescriptionDataFile)\n",
    "cellDescriptionDF = pd.read_csv(cellDescriptionDataFilePath, ',')\n",
    "print('Extracted a table with ' + str(cellDescriptionDF.shape[0]) + ' lines and ' + str(cellDescriptionDF.shape[1]) + ' columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mecaDF['ManipID'] = mecaDF['ExpDay'] + '_' + mecaDF['CellName'].apply(lambda x: x.split('_')[0])\n",
    "expConditionsDF['ManipID'] = expConditionsDF['date'] + '_' + expConditionsDF['manip']\n",
    "\n",
    "mainMecaDF = pd.merge(\n",
    "    expConditionsDF,\n",
    "    mecaDF,\n",
    "    how=\"inner\",\n",
    "    on='ManipID',\n",
    "#     left_on=None,\n",
    "#     right_on=None,\n",
    "#     left_index=False,\n",
    "#     right_index=False,\n",
    "#     sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),\n",
    "#     copy=True,\n",
    "#     indicator=False,\n",
    "#     validate=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mainMecaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.reset_option('max_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### * Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "# mainMecaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.reset_option('max_columns')\n",
    "# mainMecaDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mainMecaDF_f = mainMecaDF.loc[(mainMecaDF[\"Validated\"] == 1)]\n",
    "# mainMecaDF_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "listCells = mainMecaDF_f['CellID'].drop_duplicates().astype('string').values\n",
    "timeSeriesDict = {}\n",
    "for cell in listCells:\n",
    "    currentCell_TimeSeriesData = getCellTimeSeriesData(cell)\n",
    "    timeSeriesDict[cell] = currentCell_TimeSeriesData\n",
    "start, stop = 80, 100\n",
    "fig, axes = plt.subplots((stop-start),1, figsize = (7,4*(stop-start)))\n",
    "fig.tight_layout()\n",
    "for k in range(start, stop):\n",
    "    if k < len(listCells):\n",
    "        currentCell_TimeSeriesData = timeSeriesDict[listCells[k]]\n",
    "        T = currentCell_TimeSeriesData['T'].values\n",
    "        idxCompression = currentCell_TimeSeriesData['idxCompression'].values\n",
    "        D3 = currentCell_TimeSeriesData['D3'].values\n",
    "        maskConstant = (idxCompression == 0)\n",
    "        maskCompression = (idxCompression > 0)\n",
    "        axes[k - start].plot(T, D3*1000-4503, 'k-', linewidth = 0.5)\n",
    "        axes[k - start].plot(T[maskCompression], D3[maskCompression]*1000-4503, 'ro', markersize=2)\n",
    "        axes[k - start].plot(T[maskConstant], D3[maskConstant]*1000-4503, 'co', markersize=2)\n",
    "        axes[k - start].set_title(listCells[k])\n",
    "        axes[k - start].set_xlabel('T (s)')\n",
    "        axes[k - start].set_ylabel('D3 (µm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "addExcludedCell('21-01-18_M1_P1_C2', 'passive')\n",
    "addExcludedCell('21-01-18_M1_P1_C3', 'passive')\n",
    "addExcludedCell('21-01-18_M1_P1_C4', 'passive')\n",
    "addExcludedCell('21-01-21_M3_P1_C4', 'passive')\n",
    "addExcludedCell('21-01-21_M3_P1_C5', 'passive')\n",
    "addExcludedCell('20-08-07_M1_P1_C6', 'too thick')\n",
    "addExcludedCell('20-08-07_M1_P1_C62', 'too thick')\n",
    "\n",
    "excludedCellsDict = getExcludedCells()\n",
    "# # excludedMask = (mainMecaDF_f[\"CellID\"].values not in excludedCellsDict.keys())\n",
    "# # mainMecaDF_f = mainMecaDF_f.loc[(mainMecaDF_f[\"CellID\"].values not in excludedCellsDict.keys())]\n",
    "# for i in range(len(excludedCellsDict)):\n",
    "#     print('a')\n",
    "# mainMecaDF_f[\"CellID\"].drop_duplicates().astype('string').values\n",
    "excludedCellsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# currentCell_TimeSeriesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mainMecaDF_GroupedPerCell = mainMecaDF_f.groupby('CellID')\n",
    "mainMecaDF_DataPerCell = mainMecaDF_GroupedPerCell.agg({\"EChadwick\": np.median, \"SurroundingThickness\": np.median, \"H0Chadwick\" : np.median})\n",
    "# mainMecaDF_GroupedPerCell.agg({\"EChadwick\": np.median, \"D\": lambda x: np.std(x, ddof=1)})\n",
    "cols = ['date', 'manip', 'experimentType', 'drug', 'substrate',\n",
    "       'objective magnification', 'scale pixel per um', 'objective immersion',\n",
    "       'optical index correction', 'magnetic field correction', 'cell type',\n",
    "       'cell subtype', 'bead type', 'bead diameter', 'normal field',\n",
    "       'ramp field', 'compression duration', 'with fluo images', 'comments',\n",
    "       'ManipID', 'ExpType', 'CellName', 'CellID']\n",
    "mainMecaDF_DataPerCell.dropna(inplace = True)\n",
    "mainMecaDF_DataPerCell = pd.merge(mainMecaDF_DataPerCell,\n",
    "                                  mainMecaDF_f[cols].drop_duplicates(subset=['CellID']),\n",
    "                                  how=\"inner\",\n",
    "                                  on='CellID',\n",
    "                                  #     left_on='CellID',\n",
    "                                  #     right_on='CellID',\n",
    "                                  #     left_index=False,\n",
    "                                  #     right_index=False,\n",
    "                                  #     sort=True,\n",
    "                                  #     suffixes=(\"_x\", \"_y\"),\n",
    "                                  #     copy=True,\n",
    "                                  #     indicator=False,\n",
    "                                  #     validate=None,\n",
    "                                  )\n",
    "# mainMecaDF_DataPerCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GlobalTable_meca_Count = GlobalTable_meca.groupby(['cell type', 'cell subtype', 'bead type', 'drug', 'substrate']).count()\n",
    "GlobalTable_meca_Count.loc[:, ['CellID']].rename(columns={'CellID' : 'Count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Other old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1,2, figsize = (8, 5))\n",
    "# axes[0].plot(np.ones(len(resDict['nodrug']['median'])), resDict['nodrug']['median'], 'co')\n",
    "# axes[0].plot(2*np.ones(len(resDict['doxy']['median'])), resDict['doxy']['median'], 'ro')\n",
    "# axes[0].set_xlim(0.5, 2.5)\n",
    "# axes[0].set_ylabel('Median Thickness (nm)')\n",
    "# axes[0].set_xticks([1,2])\n",
    "# axes[0].set_xticklabels(['Control','Doxycylin'])\n",
    "# axes[1].plot(np.ones(len(resDict['nodrug']['fluctu'])), resDict['nodrug']['fluctu'], 'co')\n",
    "# axes[1].plot(2*np.ones(len(resDict['doxy']['fluctu'])), resDict['doxy']['fluctu'], 'ro')\n",
    "# axes[1].set_xlim(0.5, 2.5)\n",
    "# axes[1].set_ylabel('Thickness Fluctuations (nm)')\n",
    "# axes[1].set_xticks([1,2])\n",
    "# axes[1].set_xticklabels(['Control','Doxycylin'])\n",
    "# fig.savefig(\"C://Users//JosephVermeil//Desktop//ActinCortexAnalysis//DataAnalysis//constantField.png\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Old code\n",
    "conditions = ['nodrug', 'doxy']\n",
    "correspondance = {conditions[0] : 'M1', conditions[1] : 'M2'}\n",
    "allTimeSeriesDataFiles = [f for f in os.listdir(timeSeriesDataDir) if (os.path.isfile(os.path.join(timeSeriesDataDir, f)) and f.endswith(\".txt\"))]\n",
    "dates = ['21-02-10']\n",
    "resDict = {conditions[0] : {}, conditions[1] : {}}\n",
    "for C in conditions:\n",
    "    resDict[C]['accepted'] = []\n",
    "    resDict[C]['rejected'] = []\n",
    "    resDict[C]['median'] = []\n",
    "    resDict[C]['fluctu'] = []\n",
    "    for D in dates:\n",
    "        for f in allTimeSeriesDataFiles:\n",
    "            if correspondance[C] in f and D in f:\n",
    "                split_f = f.split('_')\n",
    "                cellID = split_f[0] + '_' + split_f[1] + '_' + split_f[2] + '_' + split_f[3]\n",
    "                currentCellTS = getCellTimeSeriesData(cellID)\n",
    "                D3 = currentCellTS.D3.values\n",
    "                decile_1 = np.percentile(D3, 10)\n",
    "                median = np.median(D3)\n",
    "                decile_9 = np.percentile(D3, 90)\n",
    "                if decile_1 < 0:\n",
    "                    resDict[C]['rejected'].append(cellID)\n",
    "                else:\n",
    "                    resDict[C]['accepted'].append(cellID)\n",
    "                    resDict[C]['median'].append(median)\n",
    "                    resDict[C]['fluctu'].append(decile_9-decile_1)\n",
    "#resDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "GlobalTable_ctField = getGlobalTable_ctField()\n",
    "table_ExpConditions = getExperimentalConditions()\n",
    "GlobalTable_ctField = pd.merge(table_ExpConditions, GlobalTable_ctField, how=\"inner\", on='manipID',\n",
    "#     left_on=None,right_on=None,left_index=False,right_index=False,sort=True,\n",
    "#     suffixes=(\"_x\", \"_y\"),copy=True,indicator=False,validate=None,\n",
    ")\n",
    "\n",
    "# data=pd.DataFrame(dict(\n",
    "#             x=[1, 2, 3, 4, 5],\n",
    "#             y=[2, 5, 8, 2, 7],\n",
    "#             desc=['A', 'A', 'C', 'd', 'E'],\n",
    "#         ))\n",
    "\n",
    "data = GlobalTable_ctField[['medianThickness','fluctuAmpli','cellID']]\n",
    "\n",
    "source = ColumnDataSource(\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "# hover = HoverTool(\n",
    "#         tooltips=[\n",
    "#             (\"index\", \"$index\"),\n",
    "#             (\"(x,y)\", \"($x, $y)\"),\n",
    "#             (\"desc\", \"@desc\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"medianThickness\", \"@medianThickness\"),\n",
    "            (\"fluctuAmpli\", \"@fluctuAmpli\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "p = figure(plot_width=300, plot_height=300, tools=[hover], title=\"Mouse over the dots\")\n",
    "\n",
    "p.circle('medianThickness', 'fluctuAmpli', size=20, source=data)\n",
    "\n",
    "show(p)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(dict(\n",
    "            x=[1, 2, 3, 4, 5],\n",
    "            y=[2, 5, 8, 2, 7],\n",
    "            desc=['A', 'A', 'C', 'd', 'E'],\n",
    "        ))\n",
    "\n",
    "Conditions = list(data['desc'].unique())\n",
    "NCond = len(Conditions)\n",
    "data['X'] = 0\n",
    "for i in range(NCond):\n",
    "    mask = data['desc'] == Conditions[i]\n",
    "    data.loc[mask, ['X']] = i+1\n",
    "data.index = data.x\n",
    "data = data.drop(['x'], axis = 1)\n",
    "\n",
    "data.reset_index(level=0, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 765,
   "position": {
    "height": "40px",
    "left": "1610px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
